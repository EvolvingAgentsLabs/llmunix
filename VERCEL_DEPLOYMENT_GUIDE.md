# Vercel Deployment Guide for LLMos-Lite

Complete guide to deploying LLMos-Lite to Vercel with OpenRouter integration.

## üìã Prerequisites

1. **Vercel Account**: Sign up at https://vercel.com
2. **Vercel CLI**: Install globally
   ```bash
   npm install -g vercel
   ```
3. **GitHub Repository**: Push code to GitHub
4. **OpenRouter Account**: Optional - users can provide their own keys

## üöÄ Quick Deploy

### Option 1: Deploy via Vercel Dashboard

1. **Import Project**
   - Go to https://vercel.com/new
   - Import your GitHub repository
   - Vercel will auto-detect Next.js framework

2. **Configure Build Settings**
   - Framework Preset: `Next.js`
   - Root Directory: `llmos-lite`
   - Build Command: `cd ui && npm run build`
   - Output Directory: `ui/.next`

3. **Set Environment Variables**
   ```
   ANTHROPIC_API_KEY=sk-ant-... (optional, for server-side LLM calls)
   BLOB_READ_WRITE_TOKEN=(auto-generated by Vercel Blob)
   KV_REST_API_TOKEN=(auto-generated by Vercel KV)
   ```

4. **Deploy**
   - Click "Deploy"
   - Wait 2-3 minutes for build

### Option 2: Deploy via CLI

```bash
# Login to Vercel
vercel login

# Deploy to production
cd llmos-lite
vercel --prod

# Follow prompts to configure
```

## üì¶ Vercel Services Setup

### 1. Vercel Blob (File Storage)

Blob stores Git volumes (skills, workflows, traces).

```bash
# Enable Blob storage
vercel blob create llmos-volumes

# This generates BLOB_READ_WRITE_TOKEN automatically
```

**Structure**:
```
volumes/
‚îú‚îÄ‚îÄ system/
‚îÇ   ‚îî‚îÄ‚îÄ system/
‚îÇ       ‚îú‚îÄ‚îÄ skills/
‚îÇ       ‚îú‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ metadata.json
‚îú‚îÄ‚îÄ team/
‚îÇ   ‚îî‚îÄ‚îÄ {team_id}/
‚îÇ       ‚îú‚îÄ‚îÄ skills/
‚îÇ       ‚îú‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ metadata.json
‚îî‚îÄ‚îÄ user/
    ‚îî‚îÄ‚îÄ {user_id}/
        ‚îú‚îÄ‚îÄ skills/
        ‚îú‚îÄ‚îÄ workflows/
        ‚îî‚îÄ‚îÄ metadata.json
```

### 2. Vercel KV (Redis)

KV stores sessions and real-time data.

```bash
# Enable KV storage
vercel kv create llmos-sessions

# This generates KV_REST_API_TOKEN automatically
```

**Keys**:
```
session:{session_id} -> Session JSON
user:{user_id}:sessions -> Set[session_id]
team:{team_id}:sessions -> Set[session_id]
session:{session_id}:messages -> List[Message]
```

### 3. Vercel Cron Jobs

Cron jobs run evolution algorithms daily/weekly.

Configured in `vercel.json`:
```json
{
  "crons": [
    {
      "path": "/api/cron/evolution-user",
      "schedule": "0 0 * * *"
    },
    {
      "path": "/api/cron/evolution-team",
      "schedule": "0 0 * * 0"
    }
  ]
}
```

## üîë OpenRouter Integration

### User-Provided API Keys

LLMos-Lite uses **user-provided API keys** for privacy and cost control:

1. **First Visit**: User sees API key setup screen
2. **Provider Selection**: OpenRouter (recommended), Anthropic, or OpenAI
3. **API Key Entry**: User enters their own key
4. **Local Storage**: Key stored in browser localStorage (never sent to server)
5. **Direct Calls**: All LLM requests go directly to OpenRouter/provider

### Get OpenRouter API Key

Users can get a free OpenRouter key at:
- **URL**: https://openrouter.ai/keys
- **Free tier**: $0.10 credit to start
- **Free models**: Kimi K2, DeepSeek, etc.
- **Paid models**: Claude, GPT, etc.

### Supported Models

Configured in `lib/llm-client.ts`:

```typescript
export const AVAILABLE_MODELS = {
  // Anthropic Claude
  'claude-opus-4.5': {
    id: 'anthropic/claude-opus-4.5',
    inputCost: '$15/M tokens',
    outputCost: '$75/M tokens',
  },
  'claude-sonnet-4': {
    id: 'anthropic/claude-sonnet-4',
    inputCost: '$3/M tokens',
    outputCost: '$15/M tokens',
  },
  // OpenAI GPT
  'gpt-5.2-pro': {
    id: 'openai/gpt-5.2-pro',
    inputCost: '$20/M tokens',
    outputCost: '$100/M tokens',
  },
  // Free Models
  'kimi-k2-free': {
    id: 'moonshotai/kimi-k2:free',
    inputCost: '$0/M tokens',
    outputCost: '$0/M tokens',
  },
};
```

## üì° API Endpoints

### Chat Endpoint

**Path**: `/api/chat`

**Headers**:
```
X-API-Key: {user's OpenRouter key}
X-Model: {model ID}
Content-Type: application/json
```

**Request**:
```json
{
  "user_id": "user_alice",
  "team_id": "team_quantum",
  "message": "Create a quantum circuit with 3 qubits",
  "session_id": "sess_quantum_research",
  "include_skills": true,
  "max_skills": 5
}
```

**Response**:
```json
{
  "response": "I'll create a quantum circuit...",
  "skills_used": ["quantum_circuit_builder"],
  "trace_id": "trace_20251213_100000",
  "session_id": "sess_quantum_research",
  "model_used": "anthropic/claude-opus-4.5"
}
```

### Skills Endpoints

- `GET /api/skills` - List all skills
- `GET /api/skills/{skill_id}` - Get skill details
- `POST /api/skills` - Create new skill
- `PUT /api/skills/{skill_id}` - Update skill
- `DELETE /api/skills/{skill_id}` - Delete skill

### Sessions Endpoints

- `GET /api/sessions` - List sessions (filter by volume)
- `GET /api/sessions/{session_id}` - Get session details
- `POST /api/sessions` - Create new session
- `POST /api/sessions/{session_id}/messages` - Add message
- `PUT /api/sessions/{session_id}` - Update session
- `DELETE /api/sessions/{session_id}` - Delete session

### Workflows Endpoints

- `GET /api/workflows` - List workflows
- `GET /api/workflows/{workflow_id}` - Get workflow
- `POST /api/workflows` - Create workflow
- `PUT /api/workflows/{workflow_id}` - Update workflow
- `POST /api/workflows/{workflow_id}/execute` - Execute workflow
- `DELETE /api/workflows/{workflow_id}` - Delete workflow

### Cron Endpoints (Internal)

- `GET /api/cron/evolution-user` - Daily user evolution
- `GET /api/cron/evolution-team` - Weekly team evolution

## üß™ Testing Locally

### 1. Install Dependencies

```bash
# Python dependencies
pip install -r llmos-lite/requirements.txt

# UI dependencies
cd llmos-lite/ui
npm install
```

### 2. Set Environment Variables

Create `llmos-lite/.env.local`:
```bash
# Optional - for server-side calls
ANTHROPIC_API_KEY=sk-ant-...

# Vercel services (use mock locally)
BLOB_READ_WRITE_TOKEN=mock_token
KV_REST_API_TOKEN=mock_token
```

### 3. Run Development Server

```bash
# Start Next.js dev server
cd llmos-lite/ui
npm run dev

# Visit http://localhost:3000
```

### 4. Test API Endpoints

```bash
# Test chat endpoint (requires OpenRouter key)
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -H "X-API-Key: sk-or-v1-..." \
  -H "X-Model: anthropic/claude-opus-4.5" \
  -d '{
    "user_id": "test_user",
    "team_id": "test_team",
    "message": "Hello, create a quantum circuit"
  }'

# Test skills endpoint
curl http://localhost:3000/api/skills

# Test sessions endpoint
curl http://localhost:3000/api/sessions?volume=user
```

## üìä Production Deployment

### 1. Deploy to Production

```bash
vercel --prod
```

### 2. Configure Domain

```bash
# Add custom domain
vercel domains add llmos.yourdomain.com
```

### 3. Monitor Deployment

- **Dashboard**: https://vercel.com/dashboard
- **Logs**: Click on deployment ‚Üí "Logs" tab
- **Analytics**: Click on deployment ‚Üí "Analytics" tab
- **Cron Logs**: "Settings" ‚Üí "Cron Jobs"

### 4. Verify Services

```bash
# Check Blob storage
vercel blob ls

# Check KV storage
vercel kv get session:test

# Check cron jobs
vercel cron ls
```

## üîí Security Best Practices

### 1. API Key Security

- ‚úÖ **DO**: Store user keys in localStorage only
- ‚úÖ **DO**: Send keys via headers (X-API-Key)
- ‚úÖ **DO**: Validate key format before sending
- ‚ùå **DON'T**: Store keys in cookies
- ‚ùå **DON'T**: Log keys in server logs
- ‚ùå **DON'T**: Send keys in URL params

### 2. CORS Configuration

Vercel handles CORS automatically, but you can customize in `next.config.js`:

```javascript
module.exports = {
  async headers() {
    return [
      {
        source: '/api/:path*',
        headers: [
          { key: 'Access-Control-Allow-Origin', value: 'https://yourdomain.com' },
          { key: 'Access-Control-Allow-Methods', value: 'GET,POST,PUT,DELETE' },
        ],
      },
    ];
  },
};
```

### 3. Rate Limiting

Add rate limiting to API endpoints:

```python
from fastapi import HTTPException
import time

# Simple rate limiter
rate_limits = {}

def check_rate_limit(user_id: str, limit: int = 100, window: int = 3600):
    """Allow {limit} requests per {window} seconds"""
    now = time.time()
    key = f"rate:{user_id}"

    if key not in rate_limits:
        rate_limits[key] = []

    # Remove old timestamps
    rate_limits[key] = [ts for ts in rate_limits[key] if now - ts < window]

    if len(rate_limits[key]) >= limit:
        raise HTTPException(status_code=429, detail="Rate limit exceeded")

    rate_limits[key].append(now)
```

## üí∞ Cost Estimation

### Vercel Costs

| Service | Free Tier | Pro Plan ($20/mo) |
|---------|-----------|-------------------|
| **Bandwidth** | 100 GB | 1 TB |
| **Function Execution** | 100 GB-hours | 1,000 GB-hours |
| **Blob Storage** | 500 MB | 100 GB |
| **KV Storage** | 30 MB | 256 MB |
| **Cron Jobs** | ‚úÖ Included | ‚úÖ Included |

### LLM Costs (User-Paid)

Users pay for LLM usage directly through OpenRouter:

| Model | Input | Output | Example Cost* |
|-------|-------|--------|---------------|
| **Kimi K2 (Free)** | $0/M | $0/M | $0 |
| **Claude Sonnet 4** | $3/M | $15/M | $0.18 per session |
| **Claude Opus 4.5** | $15/M | $75/M | $0.90 per session |
| **GPT-5.2 Pro** | $20/M | $100/M | $1.20 per session |

*Example: 10K input + 2K output tokens

### Total Monthly Cost (Estimated)

- **Hosting**: $20/mo (Vercel Pro)
- **LLM**: $0 (user-paid via OpenRouter)
- **Total**: **$20/mo**

## üêõ Troubleshooting

### Build Failures

**Error**: "Module not found: Can't resolve '@/lib/llm-client'"

**Solution**:
```bash
# Check tsconfig.json has correct paths
{
  "compilerOptions": {
    "paths": {
      "@/*": ["./*"]
    }
  }
}
```

### API Errors

**Error**: "Missing X-API-Key header"

**Solution**: User needs to configure API key in setup screen.

**Error**: "OpenRouter API error: 401 Unauthorized"

**Solution**: User's API key is invalid or expired.

### Cron Job Issues

**Error**: Cron job not running

**Solution**:
1. Check vercel.json has correct cron configuration
2. Verify endpoint exists at specified path
3. Check Vercel dashboard ‚Üí "Settings" ‚Üí "Cron Jobs"

## üìö Next Steps

1. **Deploy to Vercel**: Follow "Quick Deploy" steps above
2. **Configure Storage**: Enable Blob and KV services
3. **Test UI**: Visit deployed URL and configure API key
4. **Create Skills**: Use the workflow editor to create first skill
5. **Monitor**: Check Vercel dashboard for usage and logs

## üîó Resources

- **Vercel Docs**: https://vercel.com/docs
- **OpenRouter Docs**: https://openrouter.ai/docs
- **Next.js Docs**: https://nextjs.org/docs
- **FastAPI Docs**: https://fastapi.tiangolo.com

## üìû Support

If you encounter issues:

1. Check Vercel deployment logs
2. Review browser console for client errors
3. Test API endpoints with curl
4. Check OpenRouter dashboard for API usage
5. Open GitHub issue with error details

---

**üéâ Happy Deploying!**
