# LLMunix: Dual-Mode Hybrid Intelligence Platform

> **Learn once with powerful models. Execute repeatedly with efficient models. Deploy anywhere.**

## ðŸš€ What is LLMunix?

LLMunix is a **Pure Markdown Operating System** that enables AI agents to:

1. **Learn** from experience through powerful cloud models (Claude Sonnet 4.5)
2. **Compile** successful workflows into efficient execution traces
3. **Deploy** traces to edge devices for fast, cheap, offline execution
4. **Evolve** continuously based on real-world performance data

### Key Innovation: Learner-Follower Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEARNER MODE (Claude Sonnet 4.5)             â”‚
â”‚  â€¢ Creative problem solving                    â”‚
â”‚  â€¢ Multi-agent orchestration                   â”‚
â”‚  â€¢ Generates execution traces                  â”‚
â”‚  â€¢ Cost: $0.50-$2.00 per task                 â”‚
â”‚  â€¢ Use: Novel problems                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ Creates trace
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Execution Trace (YAML)                        â”‚
â”‚  â€¢ Deterministic instruction set               â”‚
â”‚  â€¢ Validated workflow                          â”‚
â”‚  â€¢ Confidence score                            â”‚
â”‚  â€¢ Version controlled                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ Executes
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FOLLOWER MODE (Granite Nano / Edge)          â”‚
â”‚  â€¢ Fast deterministic execution                â”‚
â”‚  â€¢ No reasoning required                       â”‚
â”‚  â€¢ Offline capable                             â”‚
â”‚  â€¢ Cost: $0.001-$0.05 per task                â”‚
â”‚  â€¢ Use: Proven workflows                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸŽ¯ Use Cases

### Perfect For
âœ… Research & analysis workflows
âœ… Data processing pipelines (ETL)
âœ… Report generation
âœ… Testing & validation
âœ… Deployment automation
âœ… Industrial control systems (edge)
âœ… Field operations (offline)

### Benefits
- **20-80x cost reduction** for repetitive tasks
- **3-10x speed improvement**
- **Offline edge deployment** capability
- **Continuous learning** and improvement
- **Markdown-first** philosophy (human-readable)

## ðŸ“Š Performance

### Cost Comparison

| Mode | Model | Cost | Speed |
|------|-------|------|-------|
| Learner (Cloud) | Claude Sonnet 4.5 | $0.50-$2.00 | Baseline |
| Follower (Cloud) | Granite Nano 4B | $0.025-$0.05 | 3-10x faster |
| Follower (Edge) | Local Granite | ~$0.001 | 3-10x faster |

### Real Example: Research Task (10 steps)

**First Execution (Learning)**:
```
Mode: LEARNER
Cost: $1.20
Time: 240 seconds
Output: âœ… Research report
Result: Created execution_trace_research_v1.0.yaml
```

**Second Execution (Following)**:
```
Mode: FOLLOWER
Cost: $0.05 (24x cheaper)
Time: 45 seconds (5x faster)
Output: âœ… Same quality report
Result: Updated trace confidence to 0.95
```

**Edge Execution (Deployed)**:
```
Mode: FOLLOWER (Edge)
Cost: $0.001 (1200x cheaper)
Time: 50 seconds (5x faster)
Output: âœ… Same quality report
Network: Not required (offline)
```

## ðŸ—ï¸ Architecture

### Three Core Components

#### 1. Pure Markdown Architecture
- **Markdown files**: Single source of truth, 100% human-readable
- **YAML frontmatter**: Structured metadata for efficient queries
- **Native tools**: Glob, Grep, Read for fast file-based queries
- **Git-versionable**: Full version control without database sync

```bash
# Query memory using native tools
grep -A 50 "task_type: legal" system/memory_log.md | grep "outcome: success"

# Find traces matching criteria
find projects -name "execution_trace_*.md" -exec grep -l "confidence: 0.9" {} \;

# Agents use Claude Code tools:
# - Glob: Pattern matching for file discovery
# - Grep: Content search and filtering
# - Read: Parse YAML frontmatter
```

#### 2. Execution Trace Format
- **YAML specification**: Complete workflow definition
- **Deterministic steps**: Exact tool calls and parameters
- **Validation rules**: Pre/post conditions and checks
- **Error recovery**: Retry logic and fallback strategies
- **Version control**: Semantic versioning for evolution

```yaml
# Example trace
trace_id: research-ai-trends-v1.2
confidence: 0.95
steps:
  - step: 1
    tool_call:
      tool: WebFetch
      parameters:
        url: "https://techcrunch.com/ai"
    validation:
      - type: content_not_empty
```

#### 3. Dual-Mode Execution
- **SystemAgent**: Intelligent mode selector
- **GraniteFollowerAgent**: Trace executor
- **Edge Runtime**: Standalone Python executor for devices

```python
# Automatic mode selection
if high_confidence_trace_exists(goal, confidence >= 0.9):
    execute_follower_mode()  # 20-80x cheaper
else:
    execute_learner_mode()   # Learn and create trace
```

## ðŸš¦ Quick Start

### 1. Installation

```bash
git clone <repository>
cd llmunix

# Install minimal dependencies
pip install -r requirements.txt

# LLMunix is ready! No database initialization needed.
# Memory is stored in markdown files with YAML frontmatter.
```

### 2. Run Your First Task

```bash
# LLMunix automatically uses Learner mode (first time)
llmunix execute: "Research quantum computing trends from ArXiv, summarize key findings"

# Creates:
# - Research report
# - Execution trace (execution_trace_quantum_v1.0.yaml)
# - Memory entry with metadata
```

### 3. Run the Same Task Again

```bash
# LLMunix automatically uses Follower mode (has trace)
llmunix execute: "Research quantum computing trends from ArXiv, summarize key findings"

# Result:
# - 20-80x cheaper
# - 3-10x faster
# - Same quality output
```

### 4. Deploy to Edge (Optional)

```bash
# On Raspberry Pi or edge device
cd llmunix/edge_runtime
pip install -r requirements.txt

# Transfer trace
scp projects/Project_quantum/memory/long_term/execution_trace_quantum_v1.0.yaml \
    edge-device:/home/user/llmunix/edge_runtime/traces/

# Execute offline
./run_follower.py --trace traces/execution_trace_quantum_v1.0.yaml
# â†’ Cost: ~$0.001, No internet needed
```

## ðŸ“š Documentation

### Getting Started
- **[Quick Start Guide](QUICKSTART_DUAL_MODE.md)**: 5-minute setup
- **[Deployment Guide](doc/DUAL_MODE_DEPLOYMENT_GUIDE.md)**: Complete deployment manual
- **[Implementation Summary](IMPLEMENTATION_SUMMARY.md)**: Technical details

### Reference
- **[Execution Trace Schema](system/infrastructure/execution_trace_schema.md)**: Trace format specification
- **[CLAUDE.md](CLAUDE.md)**: LLMunix architecture and philosophy

### Development
- **[Memory Indexer](system/infrastructure/memory_indexer.py)**: Hybrid memory system implementation
- **[Edge Runtime](edge_runtime/run_follower.py)**: Standalone execution engine

## ðŸ”§ Advanced Features

### 1. File-Based Memory System

```bash
# Query memory using Grep patterns
grep -A 50 "outcome: success" system/memory_log.md | \
  grep -B 50 "task_type: research" | \
  grep "confidence_score: 0.[89]"

# Find execution traces
find projects -name "execution_trace_*.md" | \
  xargs grep -l "goal_signature.*research AI trends"

# Agents use QueryMemoryTool which leverages:
# - Glob for file pattern matching
# - Grep for content filtering
# - Read for YAML parsing
```

### 2. Trace Evolution

Traces improve automatically with usage:

```
Initial:     confidence = 0.75, usage_count = 0
After 10x:   confidence = 0.89, usage_count = 10
After 20x:   confidence = 0.95, usage_count = 20
After failure: confidence = 0.67 â†’ System re-learns
```

### 3. Edge Deployment

```bash
# Minimum requirements
- RAM: 2GB
- Storage: 5GB
- CPU: 4-core ARM/x86
- Network: Optional (offline capable)

# Compatible models
- IBM Granite Nano 4B
- Llama 3.1 8B (quantized)
- Mistral 7B (quantized)
- Phi-3 Mini
```

### 4. Memory Relationships (In YAML)

```yaml
# Experience entries can include relationship metadata
---
experience_id: exp_20250103_143022_ai_research
memory_links:
  causal_parent: exp_20241228_103022_quarterly_planning
  temporal_previous: exp_20250102_121543_data_collection
  conceptual_similar:
    - exp_20241201_143022_legal_research
---

# Enables queries like:
# - "What led to this experience?" (causal_parent)
# - "What happened before/after?" (temporal links)
# - "Find similar approaches" (conceptual_similar)
```

## ðŸ›£ï¸ Roadmap

### âœ… Completed (Current Release)
- [x] Pure Markdown Architecture (Zero database dependencies)
- [x] Execution trace format and schema
- [x] Granite Follower Agent for trace execution
- [x] SystemAgent Learner-Follower dispatch logic
- [x] Standalone edge runtime (offline capable)
- [x] File-based memory queries with Glob/Grep/Read
- [x] Comprehensive documentation

### â³ In Progress
- [ ] Enhanced QueryMemoryTool for pattern-based queries
- [ ] MemoryConsolidationAgent trace generation
- [ ] Automated relationship detection in YAML frontmatter
- [ ] Temporal causal reasoning implementation

### ðŸ”® Future Enhancements
- [ ] Automatic trace optimization
- [ ] Distributed trace repository
- [ ] Real-time trace sync to edge devices
- [ ] Container-based execution sandboxing
- [ ] Trace composition (combine multiple traces)
- [ ] Visual trace editor (GUI)
- [ ] Performance profiling tools
- [ ] Multi-model support expansion

## ðŸ§ª Testing

### Test Memory Queries

```bash
# Create test experience in memory log
cat >> system/memory_log.md <<EOF

## Experience: exp_test_001

---
experience_id: exp_test_001
outcome: success
task_type: research
confidence_score: 0.9
---

### Summary
Test experience for validation
EOF

# Query with Grep
grep -A 20 "outcome: success" system/memory_log.md | grep "task_type: research"
```

### Test Edge Runtime

```bash
# Create test trace
cat > test_trace.yaml <<EOF
trace_id: test-v1.0
steps:
  - step: 1
    tool_call:
      tool: Write
      parameters:
        file_path: output/test.txt
        content: "Hello from Edge"
EOF

# Execute
cd edge_runtime
./run_follower.py --trace test_trace.yaml --output report.json
cat output/test.txt  # â†’ "Hello from Edge"
```

## ðŸ“– How It Works

### Execution Flow

```
1. User submits goal â†’ SystemAgent

2. SystemAgent queries memory indexer:
   "Do we have a high-confidence trace for this goal?"

3a. IF trace found (confidence >= 0.9):
    â†’ FOLLOWER MODE
    â†’ Invoke GraniteFollowerAgent
    â†’ Execute trace deterministically
    â†’ Update trace metadata
    â†’ 20-80x cheaper, 3-10x faster

3b. IF no trace found:
    â†’ LEARNER MODE
    â†’ Multi-agent orchestration (Claude Sonnet 4.5)
    â†’ Creative problem solving
    â†’ Generate execution trace from success
    â†’ Index trace for future use

4. Trace deployment (optional):
   â†’ Transfer trace to edge device
   â†’ Execute offline with local runtime
   â†’ Cost: ~$0.001 per execution
```

### Memory System Flow

```
Markdown Files (Single Source of Truth)
    â†“ Direct queries using native tools
    â”œâ”€â†’ Glob: Pattern matching (~50ms)
    â”‚   â””â”€â†’ Find files matching patterns
    â”œâ”€â†’ Grep: Content search (~100-500ms)
    â”‚   â””â”€â†’ Filter by structured fields and keywords
    â””â”€â†’ Read: Parse YAML (~10-50ms per file)
        â””â”€â†’ Extract metadata and structured data

All queries â†’ Direct file access (no sync, no index)
Performance: Acceptable for typical usage (<500 experiences)
```

## ðŸŒŸ Key Benefits

### For Developers
- **Learn once, execute many**: No need to re-solve the same problem
- **Cost optimization**: System automatically selects cheapest execution mode
- **Transparent learning**: All reasoning captured in human-readable markdown
- **Version controlled**: Traces and memory in Git

### For Operations
- **Edge deployment**: Run AI workflows on resource-constrained devices
- **Offline capable**: No internet required after learning phase
- **Deterministic**: Follower mode eliminates LLM hallucinations
- **Monitored**: Execution reports track success/failure

### For Organizations
- **80x cost reduction**: After initial learning, operations are cheap
- **10x faster**: Deterministic execution is faster than reasoning
- **Knowledge preservation**: Organizational learning captured in traces
- **Portable**: Deploy anywhere (cloud, edge, air-gapped)

## ðŸ¤ Contributing

This is a research project exploring:
- Hybrid human-AI memory systems
- Learning-to-execute architectures
- Edge AI deployment patterns
- Continuous improvement loops

Contributions welcome! Focus areas:
- Trace optimization algorithms
- Additional tool implementations
- Edge model integrations
- Real-world use case examples

## ðŸ“œ License

[Add your license here]

## ðŸ™ Acknowledgments

Built on:
- **Claude Sonnet 4.5** (Anthropic) - Learner mode intelligence
- **IBM Granite Nano** - Edge execution model
- **Pure Markdown** - Human-readable, Git-versionable storage
- **YAML Frontmatter** - Structured metadata for queries

## ðŸ“ž Support

- **Quick Start**: [QUICKSTART_DUAL_MODE.md](QUICKSTART_DUAL_MODE.md)
- **Full Guide**: [doc/DUAL_MODE_DEPLOYMENT_GUIDE.md](doc/DUAL_MODE_DEPLOYMENT_GUIDE.md)
- **Issues**: GitHub Issues for bug reports
- **Discussions**: GitHub Discussions for questions

---

**LLMunix: Where AI learns once and executes everywhere.**

Learn with powerful models in the cloud.
Execute with efficient models on the edge.
Improve continuously with every execution.

*The future of AI is hybrid.*
