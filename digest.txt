Directory structure:
└── llmunix/
    ├── README.md
    ├── CLAUDE.md
    ├── EXAMPLES.md
    ├── INTERACTIVE_SESSION_README.md
    ├── LICENSE
    ├── LLM-OS-BLUEPRINT.md
    ├── LLM_INTERPRETER_README.md
    ├── RUNTIME_COMPARISON.md
    ├── example.env
    ├── llm_interpreter.py
    ├── llmunix-llm
    ├── requirements_llm.txt
    ├── test_dynamic.py
    ├── test_dynamic_creation.md
    ├── test_llm_interpreter.py
    ├── components/
    │   ├── .DS_Store
    │   ├── agents/
    │   │   ├── MemoryAnalysisAgent.md
    │   │   ├── RealSummarizationAgent.md
    │   │   ├── SimulatedFineTunedAgent.md
    │   │   ├── SummarizationAgent.md
    │   │   └── SummarizationAgent_v2.md
    │   └── tools/
    │       ├── FileWriterTool.md
    │       ├── LLMInterpreterWebFetchTool.md
    │       ├── QueryMemoryTool.md
    │       ├── RealFileSystemTool.md
    │       ├── RealWebFetchTool.md
    │       ├── TranslationTool.md
    │       └── WebFetcherTool.md
    ├── scenarios/
    │   ├── Evolve_summarization.md
    │   ├── FineTuned_LLM_Simulation.md
    │   ├── NewsAnalysis_InterpreterTest.md
    │   ├── RealWorld_Research_Task.md
    │   ├── Summarize_website.md
    │   └── Translate_website.md
    ├── system/
    │   ├── ClaudeCodeToolMap.md
    │   ├── ExecutionStateTemplate.md
    │   ├── FineTunedLLMSimulator.md
    │   ├── LLMunixCommandInterpreter.md
    │   ├── LLMunixFileSystem.md
    │   ├── LLMunixKernel.md
    │   ├── LLMunixMemoryManager.md
    │   ├── LLMunixProcessManager.md
    │   ├── LLMunixShell.md
    │   ├── SmartLibrary.md
    │   ├── SmartMemory.md
    │   ├── StateDirectoryTemplate.md
    │   ├── SystemAgent.md
    │   └── memory_log.md
    ├── workspace/
    │   └── state/
    ├── .claude/
    │   └── settings.local.json
    └── .opencode/
        └── commands/

================================================
File: README.md
================================================
# LLMunix

A Pure Markdown Operating System where everything is either an agent or tool defined in markdown documents. The LLM interpreter reads and sends full markdown specifications to LLM for interpretation and execution of any task.

## Quick Start

### Prerequisites
First, start Claude Code in your terminal:
```bash
claude
```

### Boot LLMunix
Once you're in the Claude Code console, boot the LLMunix operating system:
```bash
boot llmunix
```

![LLMunix boot demo](./llmunix.gif)

*Watch LLMunix boot demonstration*

### Example Commands

**Constraint-aware intelligence gathering:**
```bash
llmunix execute: "Monitor 5 tech news sources, extract trending topics, and generate intelligence briefing"
# System adapts constraints based on API limitations, maintains intelligence value through graceful degradation
```

**Memory-driven task execution:**
```bash
llmunix execute: "Research AI safety papers - query memory for past research patterns and apply successful approaches"
# QueryMemoryTool consults past experiences, MemoryAnalysisAgent recommends optimal strategy
```

**Adaptive execution with sentiment tracking:**
```bash
llmunix execute: "Urgent: analyze this legal document for risks in 10 minutes"
# System detects urgency, adapts constraints: priority='speed_and_clarity', persona='concise_assistant'
```

## What is LLMunix?

LLMunix is an AI-powered operating system that implements **Adaptive Behavior Management** - where system behavior dynamically adapts through evolving behavioral constraints:

- **Pure Markdown Architecture**: All system components are markdown files. LLM interpreter reads and sends specifications to LLM for interpretation and execution
- **Adaptive State Management**: Behavioral constraints (user sentiment, priorities, error tolerance) evolve during execution
- **Intelligent Memory System**: Structured, queryable experience database with pattern recognition and adaptive learning
- **Modular State Architecture**: Specialized state files (plan.md, context.md, constraints.md) for atomic updates
- **Real Tool Integration**: Maps to Claude Code's native tools with graceful degradation and error recovery
- **Adaptive Execution**: Dynamic constraint evolution based on user feedback, errors, and execution context

### Sentient State Architecture

```
llmunix/
├── system/
│   ├── SystemAgent.md              # Sentient state machine orchestrator
│   ├── memory_log.md               # Structured, queryable experience database
│   ├── StateDirectoryTemplate.md   # Modular state architecture template
│   └── ClaudeCodeToolMap.md        # Real tool mappings with error handling
├── components/
│   ├── tools/
│   │   ├── RealWebFetchTool.md     # Live web content with constraint-aware execution
│   │   ├── QueryMemoryTool.md      # Intelligent memory consultation
│   │   └── RealFileSystemTool.md   # File operations with behavioral adaptation
│   └── agents/
│       ├── RealSummarizationAgent.md  # Content analysis with confidence scoring
│       └── MemoryAnalysisAgent.md     # Pattern recognition across experiences
├── scenarios/           # Real-world task workflows with adaptive execution
└── workspace/
    ├── state/           # Modular execution state
    │   ├── plan.md      # Execution steps and metadata
    │   ├── context.md   # Knowledge accumulation
    │   ├── variables.json # Structured data passing
    │   ├── history.md   # Execution log
    │   └── constraints.md # Behavioral modifiers (sentient state)
    └── [Output files]   # Task results and artifacts
```

### Advanced Features

#### Adaptive State Management
- **Behavioral Constraints**: Dynamic user sentiment tracking, priority adaptation, error tolerance management
- **Constraint Evolution**: Real-time behavioral modification based on execution events and user feedback
- **Adaptive Personas**: Communication style switching (concise assistant, detailed analyst, proactive collaborator)
- **Memory-Driven Initialization**: Past experiences inform initial constraint settings for similar tasks

#### Intelligent Memory System
- **Structured Experience Database**: YAML frontmatter with qualitative insights for intelligent querying
- **QueryMemoryTool**: Natural language interface to historical experience patterns
- **MemoryAnalysisAgent**: Advanced pattern recognition and recommendation engine
- **Behavioral Learning**: User sentiment evolution and constraint preference tracking

#### Modular State Architecture
- **Atomic State Transitions**: Independent updates to plan, context, variables, history, and constraints
- **Resumable Execution**: Full context preservation with mid-task pause/resume capabilities
- **Constraint-Aware Planning**: Historical patterns guide component selection and execution strategy

### Operating Modes

1. **EXECUTION MODE**: Real operations with adaptive constraint management and intelligent error recovery
2. **SIMULATION MODE**: Training data generation with behavioral pattern simulation for agent fine-tuning

### Adaptive Behavior Principles

LLMunix operates on **Adaptive Behavior Management**: system state encompasses not just data and decisions, but **evolving behavioral constraints** that actively modify decision-making processes for intelligent adaptive behavior.

#### Key Behavioral Modifiers:
- **user_sentiment**: Detected emotional state influencing interaction style (neutral, pleased, frustrated, stressed)
- **priority**: Execution focus (speed_and_clarity, comprehensiveness, cost_efficiency, quality)
- **active_persona**: Communication style (concise_assistant, detailed_analyst, proactive_collaborator)
- **error_tolerance**: Risk acceptance level (strict, moderate, flexible)
- **human_review_trigger_level**: Threshold for seeking guidance (low, medium, high)

#### Constraint Adaptation Examples:
- **User Frustration Detected** → priority="speed_and_clarity", human_review_trigger_level="low"
- **Positive Feedback Received** → user_sentiment="pleased", active_persona="proactive_collaborator"
- **Repeated Failures** → error_tolerance="strict", memory consultation for recovery strategies
- **Cost Exceeding Budget** → priority="cost_efficiency", prefer lower-cost tool alternatives

## Advanced Capabilities

### Real Tool Integration with Adaptive Behavior
- **RealWebFetchTool**: Live content retrieval with constraint-aware error handling and graceful degradation
- **RealFileSystemTool**: File operations with behavioral adaptation based on user preferences
- **RealSummarizationAgent**: Content analysis with confidence scoring and memory-recommended approaches
- **QueryMemoryTool**: Intelligent consultation of historical experiences for decision-making

### Sentient State Features
- **Dynamic Constraint Evolution**: Behavioral modifiers adapt based on user sentiment, errors, and context
- **Intelligent Error Recovery**: Memory-guided recovery strategies from past failure patterns
- **Adaptive Execution Style**: Priority shifting (speed vs comprehensiveness) based on user needs
- **Human-in-the-Loop Integration**: Context-aware escalation based on confidence and constraint settings

### Advanced Learning Pipeline
- **Structured Memory Log**: Complete execution traces with behavioral context and learning insights
- **Pattern Recognition**: Cross-execution analysis for workflow optimization and error prevention
- **Training Data Generation**: Real execution experiences converted to fine-tuning datasets
- **Performance Optimization**: Cost tracking, latency analysis, and success rate monitoring

## Advanced Capabilities: Intelligent Adaptive Systems

### Adaptive State Management
- **Emotional Intelligence**: Real-time user sentiment detection with adaptive response strategies
- **Behavioral Evolution**: Constraints dynamically modify based on execution events and user feedback
- **Memory-Driven Adaptation**: Historical patterns inform current behavioral settings and decision-making
- **Context-Aware Personas**: Communication and execution style adapts to optimize user experience

### Intelligent Memory Architecture
- **Structured Experience Database**: YAML frontmatter enables complex querying of past executions
- **Pattern Recognition Engine**: Cross-execution analysis identifies successful approaches and failure patterns
- **Adaptive Recommendations**: Memory provides actionable insights for current task optimization
- **Behavioral Learning Database**: User preference evolution and constraint effectiveness tracking

### Modular Execution Framework
- **Atomic State Transitions**: Independent file updates (plan.md, context.md, constraints.md) for precision
- **Resumable Workflows**: Full context preservation enables pause/resume at any execution point
- **Constraint-Aware Planning**: Behavioral modifiers influence component selection and execution strategy
- **Graceful Degradation**: Intelligent fallback strategies maintain value when external dependencies fail

### Advanced Error Intelligence
- **Memory-Guided Recovery**: QueryMemoryTool provides historical error recovery strategies
- **Predictive Failure Prevention**: Pattern analysis prevents repeated failure scenarios
- **Adaptive Error Tolerance**: Risk acceptance adjusts based on task criticality and user preferences
- **Human-in-the-Loop Optimization**: Context-aware escalation based on confidence and constraint settings

### Real-World Integration
- **Production-Ready Execution**: Real Claude Code tool integration with enterprise-grade error handling
- **Cost-Aware Operations**: Intelligent tool selection balancing performance, cost, and quality constraints
- **Training Data Generation**: Complete execution traces become fine-tuning datasets for autonomous agents
- **Security & Compliance**: Complete audit trails with behavioral context for enterprise deployment

## Quick Start Guide

### 1. Boot LLMunix
```bash
boot llmunix
```

### 2. Execute with Constraint Awareness
```bash
llmunix execute: "[your goal here]"
# System automatically:
# - Initializes modular state architecture (workspace/state/)
# - Consults memory for similar task patterns
# - Adapts behavioral constraints based on context
# - Executes with real tools and adaptive error recovery
# - Records complete experience for future learning
```

### 3. Monitor Adaptive Behavior
Watch as LLMunix:
- Detects your sentiment and adapts communication style
- Evolves constraints based on execution events
- Learns from memory to optimize current task approach
- Maintains intelligence value despite external tool limitations

---

## Implementation Status: Production Ready

✅ **Sentient State Architecture**: Fully implemented with behavioral constraint evolution  
✅ **Modular State Management**: Complete workspace/state/ directory structure  
✅ **Intelligent Memory System**: Structured experience database with QueryMemoryTool  
✅ **Pure Markdown Component System**: Robust LLM-based parsing without hardcoded rules  
✅ **On-Demand Component Analysis**: Intelligent interpretation when components are called  
✅ **Real Tool Integration**: Production Claude Code tool mappings with error recovery  
✅ **Adaptive Execution**: Dynamic constraint modification based on user feedback and events  
✅ **Training Data Pipeline**: Automatic generation from real execution experiences  

---

## Acknowledgements

*   Original Concept Contributors: [Matias Molinas](https://github.com/matiasmolinas) and [Ismael Faro](https://github.com/ismaelfaro).

*LLMunix: Where Adaptive Behavior meets Intelligent Memory, creating the foundation for truly intelligent autonomous AI.*



================================================
File: CLAUDE.md
================================================
# LLMunix: Pure Markdown Operating System Framework

This is LLMunix, a Pure Markdown Operating System where everything is either an agent or tool defined in markdown documents. Claude Code serves as the runtime engine interpreting these markdown specifications.

## Framework Philosophy: Pure Markdown

**CRITICAL: LLMunix is a PURE MARKDOWN framework. Everything is either an agent or tool defined in markdown documents.**

### Core Principles:
- **Markdown-Driven Execution**: LLM interpreter reads and sends full markdown specifications to LLM for interpretation and execution
- **No Code Generation**: System behavior emerges from LLM interpreting markdown documents sent at runtime
- **Agent/Tool Duality**: Every component is either an agent (decision maker) or tool (executor) defined in markdown
- **Real Tool Integration**: Markdown components map to actual tool execution via TOOL_CALL format
- **Sentient State Architecture**: Behavioral constraints evolve dynamically to enable adaptive decision-making
- **Memory-Driven Learning**: Historical experiences become actionable intelligence for continuous improvement
- **Dynamic Creation**: New tools/agents are created as markdown specifications during runtime
- **LLM as Interpreter**: LLM receives and interprets markdown system definitions to achieve any goal

### Operating Modes:
1. **EXECUTION MODE**: Real operations using Claude Code's native tools mapped through markdown specs
2. **SIMULATION MODE**: Training data generation through markdown-defined simulation patterns

The OS "boots" when Claude reads the markdown system files and begins interpreting them as a functional operating system.

## How to Boot LLMunix

### Boot Command
```
boot llmunix
```

This simple command activates the LLMunix kernel by having Claude read and interpret the markdown system files as a functional operating system. **Boot automatically cleans the workspace directory to ensure a fresh execution environment.**

### Boot Welcome Message
When LLMunix boots, display ASCII art welcome and example commands in this format:

```
██╗     ██╗     ███╗   ███╗██╗   ██╗███╗   ██╗██╗██╗  ██╗
██║     ██║     ████╗ ████║██║   ██║████╗  ██║██║╚██╗██╔╝
██║     ██║     ██╔████╔██║██║   ██║██╔██╗ ██║██║ ╚███╔╝ 
██║     ██║     ██║╚██╔╝██║██║   ██║██║╚██╗██║██║ ██╔██╗ 
███████╗███████╗██║ ╚═╝ ██║╚██████╔╝██║ ╚████║██║██╔╝ ██╗
╚══════╝╚══════╝╚═╝     ╚═╝ ╚═════╝ ╚═╝  ╚═══╝╚═╝╚═╝  ╚═╝
                Pure Markdown Operating System v1.0
```

Examples:
```bash
llmunix execute: "Monitor 5 tech news sources (TechCrunch, Ars Technica, Hacker News, MIT Tech Review, Wired), extract trending topics, identify patterns, and generate a weekly intelligence briefing"

llmunix execute: "Get live content from https://huggingface.co/blog and create a research summary"

llmunix simulate: "Research task workflow for fine-tuning dataset"
```

### Running the Real-World Research Scenario

1. **Execute the scenario** by asking Claude to:
   - Act as the SystemAgent defined in `system/SystemAgent.md`
   - Execute the goal from `scenarios/RealWorld_Research_Task.md`
   - Use EXECUTION MODE for real tool calls

2. **Expected behavior:**
   - Claude creates modular `workspace/state/` directory with specialized files
   - Initializes `constraints.md` with behavioral modifiers based on task context
   - Uses QueryMemoryTool for intelligent memory consultation during planning
   - Adapts execution style based on user sentiment and historical patterns
   - State machine execution:
     - **State 1→2**: RealWebFetchTool fetches live content with constraint-aware error handling
     - **State 2→3**: RealSummarizationAgent analyzes content using memory-recommended approaches
     - **State 3→4**: RealFileSystemTool saves structured outputs with behavioral adaptations
   - Updates modular state files after each step with real results and constraint evolution
   - Records complete experience in structured memory log with sentiment and adaptation insights
   - Generates training data from real execution trace including behavioral learning patterns

## Key Capabilities

### Real Tool Integration
- **WebFetch**: Live web content retrieval with error handling
- **FileSystem**: Real file operations (Read/Write/Search/List)
- **Bash**: System command execution for complex tasks
- **Task**: Parallel sub-task execution for complex workflows

### Sentient State Management
- **Modular State Architecture**: Specialized files for plan, context, variables, history, and constraints
- **Dynamic Behavioral Adaptation**: Constraints evolve based on user sentiment and execution events
- **Memory-Driven Planning**: Historical experiences influence current decision-making
- **Intelligent Error Recovery**: Past failure patterns guide recovery strategies
- **Atomic State Transitions**: Each step updates relevant state components
- **Resumable Execution**: Can pause and resume at any step with full context preservation
- **Cost Tracking**: Real-time monitoring with budget-aware constraint adaptation

### Advanced Learning Pipeline
- **Structured Memory Log**: YAML frontmatter with qualitative insights for intelligent querying
- **Behavioral Pattern Extraction**: User sentiment evolution and constraint adaptation tracking
- **Execution Traces**: Complete tool call sequences with real results and behavioral context
- **Performance Metrics**: Actual costs, timing, success rates, and adaptation effectiveness
- **Error Scenarios**: Real error handling examples with sentiment-aware recovery strategies
- **Quality Assessments**: Output quality scoring with behavioral and contextual metadata

### File Structure

```
llm-os/
├── system/
│   ├── SystemAgent.md              # Sentient state machine orchestrator with adaptive behavior
│   ├── SmartLibrary.md             # Component registry with real tools and memory components
│   ├── memory_log.md               # Structured, queryable experience database
│   ├── StateDirectoryTemplate.md   # Modular state architecture template
│   ├── ClaudeCodeToolMap.md        # Tool mapping and metadata
│   └── ExecutionStateTemplate.md   # Legacy template (deprecated)
├── components/
│   ├── tools/
│   │   ├── RealWebFetchTool.md     # [REAL] Live web content
│   │   ├── RealFileSystemTool.md   # [REAL] File operations
│   │   ├── QueryMemoryTool.md      # [REAL] Memory consultation interface
│   │   └── [Legacy simulation tools]
│   └── agents/
│       ├── RealSummarizationAgent.md  # [REAL] Content analysis
│       ├── MemoryAnalysisAgent.md     # [REAL] Intelligent memory querying
│       └── [Legacy simulation agents]
├── scenarios/
│   ├── RealWorld_Research_Task.md  # Live web research demo
│   └── [Legacy simulation scenarios]
├── workspace/                      # Active execution environment
│   ├── state/                     # Modular execution state
│   │   ├── plan.md                # Execution steps and metadata
│   │   ├── context.md             # Knowledge accumulation
│   │   ├── variables.json         # Structured data passing
│   │   ├── history.md             # Execution log
│   │   └── constraints.md         # Behavioral modifiers (sentient state)
│   └── [Output files from tasks]
├── LLM-OS-BLUEPRINT.md            # Architecture documentation
└── CLAUDE.md                      # This configuration file
```

### Execution Commands

**Interactive Session (Claude Code style):**
```
./llmunix-llm interactive
```

**Execute with Interactive Mode:**
```
./llmunix-llm execute: "Create a Python calculator" -i
```

**Real Task Execution:**
```
"Act as SystemAgent and execute the RealWorld_Research_Task scenario in EXECUTION MODE"
```

**Training Data Generation:**
```  
"Act as SystemAgent and simulate the research task scenario in SIMULATION MODE for training data"
```

**Custom Real Task:**
```
"Act as SystemAgent and execute: [your goal] using real tools"
```

### Interactive Session Features

The interactive session provides a Claude Code-like experience:

**Available Commands:**
- `refine` - Refine and re-execute the last goal with improvements
- `status` - Show current workspace and execution status  
- `history` - Display execution history
- `clear` - Clear workspace for fresh start (with confirmation)
- `help` - Show available commands and examples
- `exit`/`quit` - Exit interactive session

**Goal Execution:**
Simply type any goal to execute it:
```
🎯 llmunix> Create a web scraper for news articles
🎯 llmunix> Build a REST API with FastAPI
🎯 llmunix> Analyze the data in my workspace
```

**Goal Refinement:**
After executing a goal, use `refine` to improve it:
```
🎯 llmunix> refine
Previous goal: Create a web scraper for news articles
How would you like to refine this goal?
🔄 refinement> Add error handling and save to JSON format
```

**Session Management:**
- Docker containers persist across multiple executions within a session
- Workspace state is maintained between commands
- Full execution history and context available throughout session
- Clean exit with proper resource cleanup

## Development

### Adding New Real Components:
1. Create component `.md` file in `components/` with Claude tool mapping
2. Register in `system/SmartLibrary.md` with [REAL] tag and metadata
3. Test execution and validate training data generation

### Extending Tool Mappings:
1. Add new mappings to `system/ClaudeCodeToolMap.md`
2. Include cost, latency, and error mode specifications
3. Update component definitions to reference new tools

## Advanced Features

### Sentient State Management:
- **Modular State Architecture**: Specialized files in `workspace/state/` for focused updates
- **Behavioral Constraints**: `constraints.md` enables dynamic adaptation based on user sentiment and context
- **Memory-Driven Initialization**: Past experiences inform initial constraint settings
- **Real-time Adaptation**: Constraints evolve during execution based on user feedback and events
- **Atomic State Transitions**: Each component can be updated independently
- **Full Context Preservation**: Complete behavioral and execution context maintained
- **Resumable Execution**: Can pause and resume with full sentient state restoration

### Cost Optimization:
- Real-time cost tracking for all tool calls
- Intelligent tool selection based on cost/performance
- Budget management and cost reporting

### Intelligent Error Resilience:
- **Memory-Guided Recovery**: QueryMemoryTool provides historical error recovery strategies
- **Sentiment-Aware Adaptation**: Error handling adapts based on user frustration levels
- **Constraint Evolution**: Failed attempts trigger behavioral modifications for future prevention
- **Real Error Learning**: Actual tool failures become training data for improved resilience
- **Adaptive Planning**: Execution strategy adjusts based on historical success patterns
- **Context-Aware Human Escalation**: Human-in-the-loop triggered based on confidence and constraint settings

### Training Pipeline:
- Automatic training data collection from real executions
- Structured datasets for fine-tuning autonomous agents
- Quality metrics and performance benchmarking

## Clean Restart

To reset LLM-OS:
1. Clear `workspace/` directory including `workspace/state/` (preserves execution artifacts)
2. Reset `system/memory_log.md` to empty state (clears learning history and behavioral patterns)
3. Archive any valuable execution traces and behavioral learning data for training
4. Ready for fresh scenario execution with clean sentient state

## New Memory and Learning Features

### Intelligent Memory Consultation
- **QueryMemoryTool**: Standardized interface for memory-driven decision making
- **MemoryAnalysisAgent**: Advanced pattern recognition across historical executions
- **Behavioral Learning**: User sentiment patterns and constraint preferences captured
- **Adaptive Recommendations**: Memory provides actionable insights for current tasks

### Sentient State Architecture
- **Dynamic Constraints**: Behavioral modifiers that evolve based on context and feedback
- **User Sentiment Tracking**: Emotional state detection and adaptive response strategies
- **Priority Adaptation**: Execution focus adjusts based on user needs and historical patterns
- **Persona Switching**: Communication style adapts to optimize user experience
- **Error Tolerance Management**: Risk acceptance levels adjust based on task criticality and user preferences

### Advanced Execution Patterns
- **Memory-Informed Planning**: Historical success patterns guide component selection and strategy
- **Constraint-Aware Execution**: Every action considers current behavioral modifiers
- **Real-time Adaptation**: Behavioral constraints update during execution based on events
- **Sentiment-Driven Recovery**: Error handling strategies adapt to user emotional state
- **Learning Integration**: Every execution contributes to behavioral pattern database


================================================
File: EXAMPLES.md
================================================
# LLMunix Examples: Adaptive Behavior & Intelligent Memory

This document demonstrates LLMunix's **Adaptive Behavior Management** with behavioral constraint evolution, intelligent memory consultation, adaptive execution patterns, and **robust LLM-based component parsing**.

## 🎯 Real-World Validation Experiment

### Research Paper Analysis: Traditional vs Adaptive Approach

**Scenario**: Analyze 10 recent AI research papers, extract key insights, identify trends, and create a comprehensive review document.

**Why This Scenario**: 
- Easily reproducible and measurable
- Clear quality indicators (completeness, accuracy, insight depth)
- Real external dependencies (web access, API limitations)
- Observable behavior adaptation opportunities

---

#### ❌ Traditional AI Agent Approach:
```bash
execute: "Analyze 10 AI research papers and create a review document"
```

**Observable Limitations:**
- **Static Processing**: Same analysis depth for all papers regardless of complexity
- **No Error Recovery**: When arXiv API fails, entire process stops
- **Fixed Communication**: Detailed technical output even when user needs quick summary
- **No Learning**: Doesn't improve approach based on previous analysis quality
- **Binary Failure**: Single point of failure halts entire workflow

---

#### ✅ LLMunix Adaptive Behavior Management:
```bash
llmunix execute: "Analyze 10 AI research papers with adaptive behavior and memory consultation"
```

### **Step-by-Step Validation Process:**

#### **Step 1: Initial Memory Consultation** *(Validatable)*
```bash
# Query past research patterns
QueryMemoryTool: "How were research analysis tasks handled successfully in previous executions?"
```
**Expected Behavior**: System consults workspace/memory_log.md for research patterns
**Validation**: Check if constraints.md initializes with memory-recommended settings
**Advantage**: Starting with proven successful patterns vs trial-and-error

#### **Step 2: Adaptive Processing Detection** *(Validatable)*
```bash
# Monitor constraint evolution during execution
watch workspace/state/constraints.md
```
**Expected Behavior**: Constraints adapt when encountering complex papers
- Simple papers → priority='speed_and_clarity'
- Complex papers → priority='comprehensiveness', active_persona='detailed_analyst'
**Validation**: Observe constraint changes in real-time during different paper types
**Advantage**: Processing efficiency adapts to content complexity

#### **Step 3: Graceful Degradation Testing** *(Validatable)*
```bash
# Simulate API failure during execution
# Block arXiv access midway through analysis
```
**Expected LLMunix Behavior**:
- Detects API failure in history.md
- Constraints adapt: error_tolerance='flexible'
- Memory provides alternative sources (Google Scholar, direct PDFs)
- Continues analysis with fallback methods
**Validation**: Compare completion rate with/without API access
**Advantage**: Maintains progress vs complete failure

#### **Step 4: User Interaction Adaptation** *(Validatable)*
```bash
# Provide feedback during execution:
"This is taking too long, I need faster results"
```
**Expected Behavior**: 
- System detects urgency cues
- Constraints evolve: user_sentiment='impatient', priority='speed_and_clarity'
- Output format shifts to executive summaries
**Validation**: Compare response style before/after feedback
**Advantage**: Adapts to user needs in real-time vs fixed behavior

#### **Step 5: Memory Learning Validation** *(Validatable)*
```bash
# After completion, run similar task:
llmunix execute: "Analyze 5 ML papers for trend analysis"
```
**Expected Behavior**: 
- QueryMemoryTool applies lessons from previous research task
- Initial constraints set based on successful patterns
- Faster setup and optimization
**Validation**: Compare setup time and approach quality vs first execution
**Advantage**: Continuous improvement vs starting from scratch

### **🔬 Measurable Validation Criteria:**

#### **Behavioral Adaptation Evidence:**
- **constraints.md changes**: Document real-time constraint evolution
- **history.md entries**: Track adaptation triggers and responses
- **Output quality variation**: Different analysis depth based on paper complexity
- **Recovery patterns**: Successful continuation after simulated failures

#### **Memory Integration Evidence:**
- **memory_log.md updates**: Complete experience recording
- **Subsequent task optimization**: Improved performance on similar tasks
- **Pattern application**: Evidence of successful strategy reuse

#### **System Resilience Evidence:**
- **API failure handling**: Completion rate with/without external dependencies
- **Fallback activation**: Alternative method engagement when primary fails
- **Service continuity**: Maintained progress vs binary failure

### **📋 Validation Protocol:**

1. **Setup Baseline**: Run traditional approach, document outcomes
2. **Execute LLMunix**: Follow step-by-step process above
3. **Monitor State Files**: Track real-time changes in workspace/state/
4. **Introduce Challenges**: API failures, user feedback, complexity variations
5. **Compare Outcomes**: Quality, completeness, adaptation evidence
6. **Repeat Execution**: Validate memory learning with similar follow-up task

### **🎯 Expected Demonstrable Advantages:**

- **Adaptive Processing**: Observable constraint changes based on content complexity
- **Error Resilience**: Continued operation during simulated API failures
- **User Responsiveness**: Real-time adaptation to feedback and changing requirements
- **Memory Application**: Improved performance on subsequent similar tasks
- **Complete Traceability**: Full behavioral context preserved in modular state files

**Why These Advantages Matter**: Unlike static AI systems, LLMunix creates an observable, validatable record of intelligent adaptation that can be measured and reproduced.

## 🚀 System Boot & Adaptive State Initialization

### Basic Boot with State Architecture
```
boot llmunix
```
*Automatically initializes modular state architecture in workspace/state/ with behavioral constraints*

### Boot with Memory Consultation
```
boot llmunix and query memory for patterns from previous executions to optimize initial constraints
```
*System consults memory_log.md for behavioral preferences and successful patterns*

### Boot with Constraint Customization
```
boot llmunix with priority='speed_and_clarity' and active_persona='concise_assistant'
```
*Initializes with specific behavioral modifiers for immediate constraint-aware execution*

## Adaptive Behavior & Execution Examples

### Constraint-Aware Intelligence Gathering with Graceful Degradation
```
llmunix execute: "Monitor 5 tech news sources (TechCrunch, Ars Technica, Hacker News, MIT Tech Review, Wired), extract trending topics, identify patterns, and generate a weekly intelligence briefing"
```
**Adaptive Behavior Demonstrated:**
- WebFetch API limitations detected → constraints adapt: error_tolerance='flexible', priority='adaptability'
- System gracefully degrades to intelligence framework generation
- Maintains 85% confidence through adaptive constraint evolution
- Complete behavioral learning recorded in memory_log.md

### Memory-Driven Research with Historical Pattern Application
```
llmunix execute: "Research AI safety papers - query memory for past research patterns and apply successful approaches"
```
**QueryMemoryTool Consultation:**
- Queries: "How were research tasks handled successfully in the past?"
- Memory provides: Previous summarization strategies, quality metrics, user satisfaction patterns
- Constraints initialized based on historical success: priority='comprehensiveness', active_persona='detailed_analyst'

### Sentiment-Adaptive Urgent Task Processing
```
llmunix execute: "URGENT: Analyze this legal document for risks in 10 minutes - deadline critical!"
```
**Real-Time Constraint Evolution:**
- Detects user stress → user_sentiment='stressed'
- Adapts: priority='speed_and_clarity', active_persona='concise_assistant', human_review_trigger_level='low'
- Execution style optimizes for time while maintaining accuracy
- Post-completion: user_sentiment evolves to 'relieved'

## Memory-Driven Tool Evolution Examples

### Adaptive API Integration with Memory Learning
```
llmunix execute: "Create a Slack analysis tool, then use it for team productivity insights"
```
**Memory Integration Pattern:**
- QueryMemoryTool: "What were successful patterns for API integration tools?"
- Memory recommends: Error handling strategies, rate limiting approaches, confidence scoring
- Tool creation incorporates past learnings for improved reliability
- Experience logged: Tool performance, user satisfaction, optimization opportunities

### Constraint-Aware Specialized Tool Creation
```
llmunix execute: "Create a scientific paper processor for quantum computing analysis"
```
**Behavioral Constraint Application:**
- Memory consultation reveals: Past technical tools benefited from detailed_analyst persona
- Constraints set: active_persona='detailed_analyst', priority='quality', error_tolerance='strict'
- Tool creation adapts to constraints: Comprehensive validation, detailed output formatting
- Memory update: Technical domain patterns, constraint effectiveness

### Error-Resilient Pipeline with Memory Recovery
```
llmunix execute: "Generate stock data analysis pipeline with automated trading signals"
```
**Memory-Guided Error Recovery:**
- QueryMemoryTool: "How were financial data failures handled in past executions?"
- Memory provides: Fallback data sources, validation strategies, confidence thresholds
- Pipeline incorporates memory-recommended resilience patterns
- Real-time constraint adaptation based on data quality and market volatility

## Advanced Multi-Agent Workflows

### Autonomous Research Team
```
Act as SystemAgent and orchestrate: "Deploy 3 specialized research agents - one for data collection, one for analysis, one for synthesis. Have them collaborate on investigating the impact of remote work on software development productivity"
```

### Content Creation Factory
```
llmunix execute: "Set up a content pipeline with agents for topic research, outline creation, writing, editing, and SEO optimization. Generate a complete blog post about emerging AI trends"
```

### Quality Assurance Swarm
```
Act as SystemAgent and deploy: "Create a QA team of agents to test a web application - one for functionality testing, one for performance analysis, one for security assessment, and one for user experience evaluation"
```

## Advanced State Management & Memory Intelligence

### Modular State with Atomic Constraint Evolution
```
llmunix execute: "Comprehensive electric vehicle market research - enable modular state management"
```
**Modular State Architecture:**
- workspace/state/plan.md: Research phases with adaptive milestones
- workspace/state/constraints.md: Evolving behavioral modifiers based on research complexity
- workspace/state/context.md: Accumulating market insights and pattern recognition
- workspace/state/history.md: Complete audit trail with constraint adaptation events
- Mid-execution constraint evolution: As data complexity increases, priority shifts to 'comprehensiveness'

### Memory-Driven Learning with Behavioral Pattern Analysis
```
llmunix execute: "Social media sentiment analysis with memory-driven methodology improvement"
```
**Intelligent Memory Learning:**
- MemoryAnalysisAgent queries: Historical sentiment analysis approaches, accuracy patterns, user feedback
- Identifies: Previous constraint combinations that led to highest user satisfaction
- Applies: Proven behavioral patterns while adapting to current context
- Records: New insights about constraint-accuracy relationships for future optimization

### Constraint-Aware Workflow Evolution
```
llmunix execute: "Optimize data processing workflow using memory patterns and constraint adaptation"
```
**Sentient State Optimization:**
- Memory consultation: "What constraint patterns led to successful workflow optimizations?"
- Constraint initialization: Based on historical optimization success patterns
- Real-time adaptation: Constraints evolve as bottlenecks are identified
- Performance feedback loop: Constraint effectiveness metrics update memory database

## Behavioral Learning & Training Data Examples

### Constraint-Aware Customer Service Training
```
llmunix simulate: "Generate customer service training data with behavioral constraint adaptation patterns"
```
**Behavioral Pattern Generation:**
- Simulates constraint evolution: user_sentiment changes from 'frustrated' to 'satisfied'
- Training data includes: Constraint adaptation triggers, response style modifications, escalation patterns
- Memory integration: How constraint changes correlate with resolution success rates
- Output: Complete behavioral adaptation sequences for fine-tuning autonomous agents

### Memory-Driven Error Recovery Training
```
llmunix simulate: "API failure scenarios with memory-guided recovery pattern generation"
```
**Intelligent Recovery Simulation:**
- QueryMemoryTool integration: Historical recovery strategies and their success rates
- Constraint adaptation: How error_tolerance and priority shift during failure scenarios
- Training patterns: Memory consultation → strategy selection → constraint evolution → execution adaptation
- Learning data: Complete error-to-recovery workflows with behavioral context

### Sentient State Decision-Making Datasets
```
llmunix simulate: "Project management scenarios with dynamic constraint evolution training data"
```
**Advanced Behavioral Simulation:**
- Constraint evolution patterns: How priority, user_sentiment, and active_persona change with project phases
- Memory consultation simulation: How historical project patterns influence current decision-making
- Training complexity: Multi-dimensional constraint interactions and their outcomes
- Dataset richness: Complete sentient state transitions with performance correlation data

## Complex Integration Examples

### Enterprise Workflow Automation
```
llmunix execute: "Integrate with enterprise systems (CRM, ERP, email) to automate lead qualification, proposal generation, and follow-up sequences"
```

### IoT Data Processing Hub
```
Act as SystemAgent and create: "A real-time IoT data processing system that ingests sensor data, applies machine learning models, and triggers automated responses"
```

### Multi-Platform Content Syndication
```
llmunix execute: "Create content once, then automatically adapt and distribute it across multiple platforms (blog, social media, newsletter, documentation) with platform-specific optimizations"
```

## Emergency Response & Problem Solving

### System Diagnostics & Repair
```
llmunix execute: "Diagnose a failing web application by analyzing logs, monitoring metrics, identifying root causes, and proposing automated fixes"
```

### Crisis Management Coordination
```
Act as SystemAgent and coordinate: "Manage a data breach response by orchestrating security assessment, stakeholder communication, remediation planning, and compliance reporting"
```

### Resource Optimization Emergency
```
llmunix execute: "Rapidly optimize cloud infrastructure costs by analyzing usage patterns, identifying waste, and implementing automated cost reduction measures"
```

## Research & Discovery Examples

### Scientific Literature Mining
```
llmunix execute: "Mine scientific databases for research on [topic], identify knowledge gaps, generate research hypotheses, and create a comprehensive literature review"
```

### Patent Landscape Analysis
```
Act as SystemAgent and analyze: "The patent landscape for quantum computing technologies, identify key players, technology trends, and potential white spaces for innovation"
```

### Market Opportunity Discovery
```
llmunix execute: "Discover emerging market opportunities by analyzing startup funding patterns, technology trends, and consumer behavior shifts"
```

## Creative & Content Examples

### Adaptive Storytelling Engine
```
llmunix execute: "Create an interactive story that adapts based on reader choices, maintains narrative consistency, and generates multiple branching storylines"
```

### Personalized Learning Content
```
Act as SystemAgent and generate: "Personalized learning materials that adapt to individual learning styles, pace, and knowledge gaps for [subject]"
```

### Dynamic Presentation Builder
```
llmunix execute: "Generate a presentation on [topic] that automatically adapts content depth, visual style, and examples based on audience profile"
```

## Performance & Optimization Examples

### Code Performance Analysis
```
llmunix execute: "Analyze codebase performance, identify bottlenecks, suggest optimizations, and generate automated refactoring recommendations"
```

### Website Optimization Suite
```
Act as SystemAgent and optimize: "A complete website performance audit including speed, SEO, accessibility, and user experience improvements"
```

### Database Query Optimization
```
llmunix execute: "Analyze database query patterns, identify slow queries, generate optimized alternatives, and create automated monitoring"
```

## Meta-System Examples

### Self-Improving Agent Creation
```
llmunix execute: "Create an agent that analyzes its own performance, identifies improvement opportunities, and evolves its capabilities over time"
```

### System Architecture Evolution
```
Act as SystemAgent and evolve: "The LLMunix system architecture by analyzing usage patterns, identifying inefficiencies, and proposing structural improvements"
```

### Automated Tool Discovery
```
llmunix execute: "Automatically discover and integrate new tools/APIs that could enhance system capabilities based on current workflow analysis"
```

## Power User Commands

### Complete Project Automation
```
llmunix execute: "Take project requirements and automatically generate: architecture design, implementation plan, code structure, testing strategy, deployment pipeline, and monitoring setup"
```

### Intelligent Business Operations
```
Act as SystemAgent and automate: "Complete business operations including lead generation, qualification, proposal creation, negotiation support, and customer onboarding"
```

### Autonomous Development Team
```
llmunix execute: "Simulate a complete software development team with product owner, developers, testers, and DevOps engineers working on a real project"
```

## Sentient State Mastery Tips

### Constraint Optimization
1. **Initialize with Memory**: Always query memory for similar task patterns before execution
2. **Monitor Sentiment Evolution**: Watch how your feedback influences system behavioral adaptation
3. **Leverage Constraint Inheritance**: Successful constraint patterns are applied to similar future tasks
4. **Embrace Adaptive Execution**: Allow system to evolve constraints based on execution events

### Memory Intelligence Utilization
5. **Pattern Recognition**: Use MemoryAnalysisAgent for complex queries about historical success patterns
6. **Behavioral Learning**: System learns your preferences and adapts communication style accordingly
7. **Error Recovery Wisdom**: Memory provides sophisticated recovery strategies from past failure patterns
8. **Confidence Calibration**: Historical accuracy helps system calibrate confidence and escalation thresholds

### Modular State Mastery
9. **Atomic Updates**: Leverage modular state for precise execution tracking and resumability
10. **Constraint Awareness**: Monitor workspace/state/constraints.md to understand current behavioral context
11. **Context Accumulation**: Use workspace/state/context.md for knowledge building across execution steps
12. **Full Traceability**: Complete execution history enables sophisticated debugging and optimization

## Advanced Command Patterns

### Sentient State Commands
- `llmunix execute:` - Constraint-aware execution with adaptive behavioral modification
- `llmunix execute with priority='speed_and_clarity':` - Explicit constraint initialization
- `llmunix execute and query memory for [pattern]:` - Memory-driven execution strategy
- `llmunix execute with sentiment tracking:` - Enhanced user sentiment detection and adaptation

### Memory Intelligence Commands  
- `query memory for: "How were [task type] handled successfully?"` - Pattern consultation
- `analyze memory patterns for constraint optimization` - Behavioral learning analysis
- `apply memory-recommended constraints for [task type]` - Historical pattern application
- `update memory with current execution insights` - Learning integration

### Adaptive Execution Commands
- `boot llmunix with adaptive constraints` - Dynamic behavioral initialization
- `resume execution from workspace/state/ with constraint evolution` - Stateful resumption
- `execute with graceful degradation for external tool failures` - Resilient execution
- `monitor constraint evolution during execution` - Real-time behavioral adaptation tracking

### Training & Simulation Commands
- `llmunix simulate: [scenario] with behavioral pattern generation` - Training data with constraint context
- `generate training data from memory experiences` - Historical execution learning datasets
- `simulate constraint adaptation patterns for [scenario type]` - Behavioral adaptation training

## Pure Markdown Component System Examples

### Automatic Component Discovery and Analysis
```bash
# LLM Interpreter automatically discovers and analyzes components
🔍 Analyzing component: QueryMemoryTool.md
✅ Found component: Query Memory Tool (TOOL)
🔍 Analyzing component: RealSummarizationAgent.md  
✅ Found component: RealSummarizationAgent (AGENT)
🔧 Components loaded: 26 tools/agents
```

### On-Demand Component Execution
```bash
llmunix execute: "Use QueryMemoryTool to find patterns from past research tasks"

# System automatically:
# 1. Recognizes QueryMemoryTool as a markdown component
# 2. Analyzes its specification on-demand
# 3. Interprets execution logic using LLM
# 4. Maps to real tools (cat, grep, echo) as needed
# 5. Returns structured results matching component outputs
```

### Dynamic Component Creation
```bash
llmunix execute: "Create a new specialized tool for analyzing code quality metrics"

# LLM can:
# - Create new markdown component specifications
# - Define inputs, outputs, and execution patterns
# - Implement using real tools automatically
# - Register for future use
```

### Flexible Component Recognition
The system recognizes components through multiple patterns:
- **Exact matches**: `QueryMemoryTool`, `RealWebFetchTool`
- **Pattern matching**: `query memory`, `web fetch`, `summarization`
- **File stem matching**: Any markdown file can become a component
- **Intelligent inference**: LLM determines component type and capabilities

## The Sentient State Advantage

LLMunix's revolutionary approach combines:
- **Pure Markdown Component System**: Zero hardcoded parsing rules, LLM understands any markdown component
- **On-Demand Analysis**: Components analyzed only when needed for optimal performance
- **Behavioral Constraint Evolution**: System adapts its behavior based on context, user feedback, and execution events
- **Intelligent Memory Integration**: Historical experiences actively inform current decision-making
- **Modular State Architecture**: Atomic state transitions enable precise control and resumability
- **Adaptive Error Recovery**: Memory-guided strategies maintain intelligence value despite external failures

Every execution contributes to the system's behavioral intelligence, creating a continuously improving autonomous agent foundation.


================================================
File: INTERACTIVE_SESSION_README.md
================================================
# LLMunix Interactive Session

The LLMunix LLM Interpreter now supports an interactive session mode similar to Claude Code, allowing you to refine goals and execute new tasks without terminating the session.

## Getting Started

### Start Interactive Session
```bash
./llmunix-llm interactive
```

### Execute Goal and Enter Interactive Mode
```bash
./llmunix-llm execute: "Create a Python calculator" -i
```

## Interactive Commands

Once in the interactive session, you can use these commands:

### Goal Execution
Simply type any goal to execute it:
```
🎯 llmunix> Create a web scraper for news articles
🎯 llmunix> Build a REST API with FastAPI  
🎯 llmunix> Analyze the data in my workspace
```

### Goal Refinement
Use the `refine` command to improve the last executed goal:
```
🎯 llmunix> refine
Previous goal: Create a web scraper for news articles
How would you like to refine this goal?
🔄 refinement> Add error handling and save to JSON format
```

### Status and Monitoring
- `status` - Show workspace files, state directory, and recent files
- `history` - Display execution history from the current session
- `clear` - Clear workspace with confirmation prompt

### Session Management
- `help` - Show available commands and examples
- `exit` or `quit` - Exit interactive session with cleanup

## Key Features

### Pure Markdown Component System
- **Robust LLM-based parsing**: Any markdown component can be understood and executed
- **On-demand analysis**: Components analyzed only when needed for optimal performance
- **Intelligent recognition**: Flexible pattern matching for component names and types
- **Zero hardcoded rules**: LLM interprets component specifications dynamically

### Persistent Environment
- Docker containers persist across multiple executions within a session
- Workspace state is maintained between commands
- Full execution history and context available throughout session
- Component analysis cache preserved across commands

### Session Context
- Last executed goal is remembered for refinement
- Execution state persists across commands
- Component registry maintained throughout session
- Clean resource management on session exit

### Error Handling
- Graceful handling of Ctrl+C interruption
- Error recovery with helpful guidance
- Safe workspace clearing with confirmation

## Examples

### Complete Workflow
```bash
# Start interactive session
./llmunix-llm interactive

# Execute initial goal
🎯 llmunix> Create a simple web scraper

# Check status
🎯 llmunix> status

# Refine the goal
🎯 llmunix> refine
🔄 refinement> Add beautiful soup parsing and error handling

# Execute new task
🎯 llmunix> Create unit tests for the scraper

# View history
🎯 llmunix> history

# Exit session
🎯 llmunix> exit
```

### One-Shot with Interactive
```bash
# Execute goal and enter interactive mode
./llmunix-llm execute: "Build a FastAPI server" -i

# Now in interactive mode - can refine or add new goals
🎯 llmunix> refine
🔄 refinement> Add database integration with SQLAlchemy

🎯 llmunix> Create documentation for the API

🎯 llmunix> exit
```

## Technical Details

### State Management
- Execution context preserved across commands
- Modular state files updated continuously
- Docker container lifecycle managed efficiently

### Resource Cleanup
- Containers cleaned up only on session exit
- Workspace preserved during session
- Graceful shutdown on interruption

### Integration with LLMunix
- Full compatibility with existing LLMunix features
- SystemAgent delegation for all executions
- Pure markdown component system integration
- Real tool integration maintained
- On-demand component analysis for any markdown file


================================================
File: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
File: LLM-OS-BLUEPRINT.md
================================================
# LLM-OS: Claude Code Runtime Architecture

## Overview

LLM-OS uses an LLM interpreter that reads markdown specifications and sends them to LLM for interpretation. Instead of simulation, it performs real operations by having LLM interpret markdown agent/tool definitions and execute via TOOL_CALL format.

## Architecture

### Core Concept: Dual-Mode Operation

LLM-OS operates in two modes seamlessly:

1. **Training Data Generation Mode**: Pure document simulation for creating fine-tuning datasets
2. **Execution Mode**: Real tool execution through LLM interpretation of markdown specifications with TOOL_CALL format

### Key Components

#### 1. State Machine Engine (`workspace/execution_state.md`)
- Tracks current execution state, step, and variables
- Records tool call results and state transitions  
- Enables pausable, resumable execution
- Provides atomic state transitions

#### 2. Enhanced SystemAgent (`system/SystemAgent.md`)
- Orchestrates execution through LLM interpretation of markdown tools
- Manages state transitions via modular state files
- Handles error recovery and adaptation through constraint evolution
- Supports both simulation and real execution modes via TOOL_CALL format

#### 3. Real Tool Mapping (`system/ClaudeCodeToolMap.md`)
- Maps abstract framework tools to real CLI tools and commands
- Defines cost, latency, and side-effect metadata for LLM execution
- Enables intelligent tool selection

#### 4. Training Data Generator (`system/TrainingDataCollector.md`)
- Captures execution traces for fine-tuning
- Records state transitions and tool results
- Generates structured datasets

## Tool Integration Strategy

### Native Claude Code Tools Available:
- **WebFetch**: Real web content retrieval
- **Read/Write**: File system operations  
- **Bash**: System command execution
- **Grep/Glob**: Code search and file discovery
- **Task**: Automated sub-task execution

### Framework Tool Mapping:
```markdown
WebFetcherTool → WebFetch tool
FileWriterTool → Write tool  
FileReaderTool → Read tool
SearchTool → Grep/Glob tools
SystemTool → Bash tool
SubTaskTool → Task tool
```

## Execution Flow

1. **Goal Input**: User provides high-level goal
2. **State Initialization**: Create `workspace/execution_state.md` 
3. **Planning Phase**: SystemAgent reads memory/library, creates execution plan
4. **State Machine Loop**:
   - Read current state from `execution_state.md`
   - Execute next planned step using real Claude Code tools
   - Update state with results
   - Continue until goal achieved
5. **Memory Update**: Record experience in SmartMemory
6. **Training Data Export**: Save execution trace for fine-tuning

## Key Innovations

### 1. Grounded Tool Metadata
Components now include real-world characteristics:
- `cost`: Token/API cost estimates
- `latency`: Expected execution time
- `side_effects`: System state changes
- `error_modes`: Common failure patterns

### 2. Error Recovery & Adaptation
- Real error handling from actual tool failures
- Network timeouts, file system errors, API limits
- Dynamic plan adjustment based on actual results

### 3. Human-in-the-Loop Integration
- Pause execution for human input/approval
- Capture human feedback for training data
- Interactive debugging and guidance

### 4. Multi-Modal Capability  
- Image processing through Claude Code's vision
- File type detection and appropriate handling
- Structured data manipulation (JSON, CSV, etc.)

## Training Data Generation

Every execution generates a complete training example:

```json
{
  "execution_id": "exec_001",
  "goal": "Get weather for London and save to file",
  "initial_state": { ... },
  "transitions": [
    {
      "step": 1,
      "tool": "WebFetch",
      "inputs": { "url": "weather-api.com/london" },
      "outputs": { "content": "London: 15°C, cloudy" },
      "state_after": { ... }
    }
  ],
  "final_outcome": "success",
  "total_cost": "$0.0023",
  "execution_time": "4.2s"
}
```

## Benefits Over Previous Approach

1. **Real Execution**: Actual tool calls with real results
2. **Cost Awareness**: Learns actual costs and trade-offs
3. **Error Resilience**: Handles real-world failures
4. **Performance Optimization**: Optimizes for actual latency/cost
5. **Human Collaboration**: Natural human-AI interaction
6. **Rich Training Data**: Captures real execution complexity

## Implementation Plan

1. Create execution state management system
2. Update SystemAgent for Claude Code tool integration  
3. Build tool mapping and metadata system
4. Implement training data collection
5. Create demonstration scenarios
6. Test end-to-end execution and data generation

This architecture transforms AGI-DAF from a simulation into a practical, production-ready agent framework that generates its own training data through real-world execution.


================================================
File: LLM_INTERPRETER_README.md
================================================
# LLMunix LLM Interpreter

The **LLM Interpreter** is a standalone runtime engine for LLMunix that reads markdown specifications and sends them to LLM for interpretation and execution of any task using OpenAI's GPT models.

## Overview

The LLM Interpreter serves as an alternative runtime to Claude Code, offering:

- **Zero Hardcoded Logic**: All decisions made by LLM interpreting markdown specifications
- **Pure Markdown Component Parsing**: Interpreter reads and sends full markdown specs to LLM for interpretation
- **Autonomous Execution**: Complete goal achievement without user intervention  
- **On-Demand Component Analysis**: Intelligent interpretation of markdown tools/agents when needed
- **Environment Detection**: Intelligent adaptation to execution environments
- **Docker Sandbox**: Secure tool execution in isolated containers
- **Modular State Management**: Complete execution tracking and resumability
- **Real Tool Integration**: Actual command execution with error recovery

## Quick Start

### 1. Installation

```bash
# Install Python dependencies
pip install -r requirements_llm.txt

# Configure environment
cp example.env .env
# Edit .env with your OpenAI API key

# Make CLI executable
chmod +x llmunix-llm
```

### 2. Basic Usage

```bash
# Boot LLMunix with LLM Interpreter
./llmunix-llm boot

# Execute a goal
./llmunix-llm execute: "Create a Python calculator application"
```

### 3. Test Installation

```bash
# Run comprehensive test suite
python3 test_llm_interpreter.py
```

## Architecture

### Core Components

#### LLMunixInterpreter Class
- **Goal Analysis**: LLM breaks down objectives into executable plans
- **Pure Markdown Parsing**: Reads and sends full markdown specifications to LLM for interpretation
- **On-Demand Component Interpretation**: Loads markdown tools/agents and sends to LLM when needed
- **Environment Adaptation**: Detects available tools and adjusts execution  
- **Autonomous Execution**: Iterative LLM-driven action selection and execution
- **State Management**: Maintains modular execution state across iterations

#### Environment Detection System
- **Tool Discovery**: Scans execution environment for available utilities
- **Package Manager Detection**: Identifies OS package managers (apk, apt-get, yum, etc.)
- **OS Identification**: Determines Linux distribution and version
- **Documentation Generation**: Creates environment reference for LLM planning

#### Docker Integration
- **Isolated Execution**: Runs tools in secure container environments
- **Environment Consistency**: Predictable tool availability across systems
- **Resource Management**: Automatic container lifecycle management
- **Cross-Platform Support**: Works with various Docker configurations

### Execution Flow

1. **Goal Reception**: User provides natural language objective
2. **Environment Detection**: System scans for available tools and capabilities
3. **Component Discovery**: Lightweight discovery of all markdown files in the system
4. **LLM Planning**: GPT model analyzes goal and creates execution strategy
5. **Iterative Execution**: LLM decides next actions and executes them
6. **On-Demand Component Analysis**: When markdown components are called, interpreter reads and sends their specifications to LLM
7. **State Tracking**: Complete execution history maintained in modular files
8. **Error Recovery**: LLM adapts to failures and continues toward goal
9. **Completion Summary**: Final results and artifacts documented

### State Management

The interpreter creates a modular state architecture in `workspace/state/`:

```
workspace/state/
├── plan.md                    # LLM-generated execution plan
├── context.md                 # Accumulated knowledge and insights
├── variables.json             # Runtime data and structured information  
├── history.md                 # Complete execution log with decisions
├── constraints.md             # Behavioral modifiers and adaptation rules
├── container_environment.md   # Environment detection results
└── execution_plan.json        # Structured plan data
```

## Environment Detection

### Automatic Tool Discovery

The interpreter automatically detects:

- **Available Commands**: python3, pip, curl, bash, git, etc.
- **Package Managers**: apk (Alpine), apt-get (Ubuntu/Debian), yum (CentOS), etc.
- **OS Information**: Distribution, version, architecture
- **Environment Characteristics**: Container vs host, tool versions

### Adaptive Command Selection

Based on detection results, the LLM automatically:

- Uses correct package manager for the detected OS
- Selects available tools instead of assuming standard ones
- Adapts commands to environment capabilities
- Provides fallback strategies for missing tools

### Example Detection Output

```markdown
# Container Environment Detection

## Available Tools
python3, pip, curl, bash, sh, cat, grep, sed, awk

## Package Managers  
apk

## OS Information  
Linux 5.15.0 x86_64

## Distribution
Alpine Linux v3.22

## Recommendations for LLM
- Use these available tools: python3, pip, curl, bash, sh
- For package installation, use: apk
- This is an Alpine Linux system
```

## Configuration

### Environment Variables (.env file)

```bash
# Required: OpenAI API access
OPENAI_API_KEY=your-api-key-here
OPENAI_MODEL=gpt-4o

# Optional: Docker settings
DOCKER_IMAGE=alpine:latest

# Optional: Execution parameters
MAX_ITERATIONS=10
EXECUTION_TIMEOUT=300
```

### Docker Requirements

Docker is optional but recommended for:
- **Real Tool Execution**: Actual command execution vs simulation
- **Environment Isolation**: Secure execution sandbox
- **Consistent Results**: Predictable tool availability

**Without Docker**: Interpreter runs in simulation mode, providing execution plans and analysis without actual tool execution.

## CLI Usage

### Boot Command
```bash
./llmunix-llm boot
```
Initializes LLMunix operating system with ASCII art welcome and environment setup.

### Execute Command
```bash
# Standard syntax
./llmunix-llm execute: "Your goal description"

# Alternative syntax  
./llmunix-llm execute "Your goal description"
```

### Example Commands

```bash
# Development tasks
./llmunix-llm execute: "Create a REST API using Flask"
./llmunix-llm execute: "Build a web scraper for news websites"

# Research and analysis
./llmunix-llm execute: "Fetch content from https://example.com and create summary"
./llmunix-llm execute: "Research current AI trends and generate report"

# System operations
./llmunix-llm execute: "Set up Python development environment"
./llmunix-llm execute: "Create automated backup script"
```

## Testing

### Test Suite

Run the comprehensive test suite to validate installation:

```bash
python3 test_llm_interpreter.py
```

**Test Coverage:**
- Environment setup and API key validation
- Interpreter initialization and configuration
- Docker availability and container management
- Environment detection capabilities
- Simple goal execution and state management

### Manual Testing

1. **Environment Test**: `./llmunix-llm boot`
2. **Simple Goal**: `./llmunix-llm execute: "Create hello.txt with greeting"`
3. **Web Request**: `./llmunix-llm execute: "Fetch content from https://httpbin.org/json"`
4. **Development Task**: `./llmunix-llm execute: "Create Python script to calculate fibonacci"`

## Error Handling

### Common Issues

**API Key Problems:**
```bash
❌ OpenAI API key not found
💡 Solution: Set OPENAI_API_KEY in .env file
```

**Docker Issues:**
```bash
⚠️  Docker not available - running in simulation mode
💡 Install Docker for real tool execution
```

**Permission Errors:**
```bash
❌ Permission denied: ./llmunix-llm
💡 Solution: chmod +x llmunix-llm
```

### Error Recovery

The interpreter includes intelligent error recovery:

- **API Failures**: Automatic retry with exponential backoff
- **Tool Errors**: LLM analyzes failures and adapts approach
- **Environment Issues**: Graceful degradation to available tools
- **Timeout Handling**: Configurable execution timeouts

## Advanced Features

### Pure Markdown Component System
The interpreter features **markdown-driven execution** where LLM interprets any markdown component:

- **Zero Hardcoded Rules**: Interpreter reads and sends markdown files to LLM for interpretation
- **On-Demand Analysis**: Markdown specifications sent to LLM only when components are called
- **Intelligent Recognition**: Flexible pattern matching for component names and capabilities
- **Automatic Discovery**: Finds all markdown components across the system

```bash
# Components are discovered and analyzed automatically
🔍 Analyzing component: QueryMemoryTool.md
✅ Found component: Query Memory Tool
🔧 Executing markdown component: QueryMemoryTool
```

### Custom Model Selection
```bash
# Use different OpenAI model
OPENAI_MODEL=gpt-4o python3 llm_interpreter.py execute "Your goal"
```

### Docker Customization
```bash
# Use custom Docker image
DOCKER_IMAGE=ubuntu:latest ./llmunix-llm execute: "Your goal"
```

### State Inspection
```bash
# Monitor execution state in real-time
watch -n 1 cat workspace/state/history.md
```

### Debugging Mode
```bash
# Enable verbose logging
DEBUG=1 ./llmunix-llm execute: "Your goal"
```

## Integration

### Programmatic Usage

```python
from llm_interpreter import LLMunixInterpreter

# Initialize interpreter
interpreter = LLMunixInterpreter(model="gpt-4o")

# Execute goal
interpreter.execute("Create data analysis script")

# Access results
summary_path = interpreter.workspace_dir / "execution_summary.md"
```

### CI/CD Integration

```yaml
# GitHub Actions example
- name: Run LLMunix Task
  env:
    OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  run: |
    ./llmunix-llm execute: "Generate test report from coverage data"
```

## Comparison with Claude Code Runtime

| Feature | Claude Code Runtime | LLM Interpreter |
|---------|-------------------|-----------------|
| **Execution Style** | Interactive dialogue | Autonomous |
| **User Interaction** | Real-time feedback | Hands-off |
| **Tool Execution** | Claude's native tools | Docker + real commands |
| **Environment Awareness** | Implicit context | Explicit detection |
| **State Management** | Full modular system | Full modular system |
| **Error Recovery** | Interactive problem-solving | Autonomous adaptation |
| **API Dependency** | Claude API | OpenAI API |
| **Setup Requirements** | Claude Code CLI | Python + API key |
| **Docker Requirement** | No | Optional |

Both runtimes execute identical markdown specifications and produce equivalent results.

## Security Considerations

### API Key Management
- Store API keys in `.env` files (not version controlled)
- Use environment variables in production
- Rotate keys regularly for security

### Docker Security
- Containers run with limited privileges
- No host filesystem access by default  
- Automatic cleanup of temporary containers

### Code Execution
- All tool execution happens in isolated containers
- LLM decisions are logged for audit trails
- No arbitrary code execution on host system

## Performance Optimization

### API Usage
- Efficient prompt design to minimize token usage
- Streaming responses for long operations
- Automatic retry logic for reliability

### Docker Performance  
- Image caching for faster container startup
- Resource limits to prevent runaway processes
- Cleanup automation to manage disk usage

### State Management
- Modular file updates for minimal I/O
- JSON for structured data, Markdown for human-readable logs
- Compression for large execution histories

## Troubleshooting

### Debug Information
```bash
# Enable debug mode
DEBUG=1 ./llmunix-llm execute: "Your goal"

# Check Docker status
docker ps -a | grep llmunix

# Inspect state files
ls -la workspace/state/
```

### Log Analysis
```bash
# View execution history
cat workspace/state/history.md

# Check LLM decisions
grep -A 5 -B 5 "LLM Decision" workspace/state/history.md

# Monitor real-time execution
tail -f workspace/state/history.md
```

### Common Solutions

1. **Slow Execution**: Reduce MAX_ITERATIONS or use faster model
2. **High API Costs**: Switch to gpt-4o-mini for simpler tasks
3. **Docker Issues**: Verify Docker daemon is running
4. **Memory Usage**: Monitor container resource usage

## Contributing

The LLM Interpreter is part of the LLMunix framework. Contributions welcome:

1. **Environment Detection**: Add support for new OS distributions
2. **Tool Integration**: Expand available tool mappings
3. **Error Recovery**: Improve autonomous failure handling
4. **Performance**: Optimize LLM prompt efficiency

## License

Same as LLMunix framework - open source with attribution to original contributors.

---

*LLM Interpreter: Autonomous execution of the Pure Markdown Operating System through LLM-driven decision making.*


================================================
File: RUNTIME_COMPARISON.md
================================================
# LLMunix Runtime Comparison: Claude Code vs LLM Interpreter

LLMunix provides **two powerful runtime engines** that execute identical markdown specifications through different approaches. This document provides a comprehensive comparison to help you choose the right runtime for your needs.

## Quick Comparison

| Aspect | Claude Code Runtime | LLM Interpreter |
|--------|-------------------|-----------------|
| **Execution Style** | Interactive dialogue | Autonomous |
| **Decision Making** | Claude's native intelligence | GPT-4 with zero hardcoded logic |
| **Component Parsing** | Claude's built-in understanding | **Interpreter reads and sends markdown to LLM** |
| **User Interaction** | Real-time feedback and clarification | Hands-off execution |
| **Tool Execution** | Claude Code's native tools | Docker sandbox + real commands |
| **Environment Detection** | Implicit context awareness | **Explicit environment scanning** |
| **Setup Requirements** | Claude Code CLI | Python + OpenAI API key |
| **Docker Requirement** | No | Optional (enables real execution) |
| **Cost Model** | Claude API usage | OpenAI API usage |

**Both runtimes execute the same markdown specifications and produce equivalent results.**

## Detailed Feature Comparison

### Execution Approach

#### Claude Code Runtime
- **Interactive Execution**: Dialogue-based with real-time user feedback
- **Native Intelligence**: Leverages Claude's built-in reasoning capabilities
- **Contextual Awareness**: Implicit understanding of environment and tools
- **User Guidance**: Can ask clarifying questions during execution
- **Immediate Access**: No setup required beyond Claude Code installation

#### LLM Interpreter
- **Autonomous Execution**: Complete goal achievement without user intervention
- **LLM Decision Making**: All choices made through GPT model reasoning
- **Pure Markdown Component Parsing**: Interpreter reads and sends markdown specs to LLM for interpretation
- **On-Demand Component Analysis**: Markdown specifications sent to LLM when components are called
- **Environment Detection**: Explicit scanning and documentation of available tools
- **Standalone Operation**: Designed for programmatic and batch execution
- **Zero Hardcoded Logic**: Pure LLM-driven decision making

### Tool Execution Capabilities

#### Claude Code Runtime
```bash
# Uses Claude Code's native tool ecosystem
- Read/Write files directly
- Bash command execution
- Web content fetching
- Multi-tool coordination
- Error handling through Claude's logic
```

#### LLM Interpreter
```bash
# Real command execution in Docker containers
- Docker-based tool isolation
- Actual package manager usage (apk, apt-get, yum)
- Real programming language execution
- Network requests and data processing
- LLM-driven error analysis and recovery
```

### State Management

**Both runtimes use identical modular state architecture:**

```
workspace/state/
├── plan.md              # Execution plan and strategy
├── context.md           # Knowledge accumulation
├── variables.json       # Runtime data and metadata
├── history.md           # Complete execution log
├── constraints.md       # Behavioral modifiers
└── [Runtime-specific files]
```

**LLM Interpreter additions:**
- `container_environment.md` - Environment detection results
- `execution_plan.json` - Structured LLM-generated plans

### Environment Awareness

#### Claude Code Runtime
- **Implicit Context**: Claude inherently understands common environments
- **Adaptive Responses**: Automatically adjusts to available Claude Code tools
- **Interactive Discovery**: Can explore environment through dialogue
- **Context Integration**: Seamlessly incorporates environmental factors

#### LLM Interpreter
- **Explicit Detection**: Systematic scanning of execution environment
- **Tool Inventory**: Documents all available commands and utilities
- **OS-Specific Adaptation**: Automatically detects package managers and distributions
- **Environment Documentation**: Creates reference files for LLM planning

**Example Environment Detection:**
```markdown
# Container Environment Detection Results

## Available Tools
python3, pip, curl, bash, sh, cat, grep, sed, awk, git

## Package Managers
apk (Alpine Linux)

## OS Information
Alpine Linux v3.22, Linux 5.15.0 x86_64

## LLM Recommendations
- Use 'apk add' for package installation (not apt-get)
- Available programming languages: Python 3.11
- Network tools: curl, wget available
```

## Use Case Comparison

### When to Choose Claude Code Runtime

**Best for:**
- **Interactive Development**: Projects requiring real-time feedback and iteration
- **Exploratory Tasks**: When you're not sure exactly what you want
- **Complex Problem Solving**: Tasks that benefit from Claude's reasoning
- **Learning and Experimentation**: Understanding how LLMunix works
- **Quick Prototyping**: Rapid iteration with immediate feedback

**Example Scenarios:**
```bash
# Interactive data analysis with user feedback
llmunix execute: "Analyze this dataset and help me understand the trends"

# Creative content generation with refinement
llmunix execute: "Create a marketing campaign for our product"

# Complex debugging with Claude's insight
llmunix execute: "Help diagnose and fix this performance issue"
```

### When to Choose LLM Interpreter

**Best for:**
- **Autonomous Execution**: Tasks that should run without user intervention
- **Batch Processing**: Multiple similar tasks or scheduled operations
- **CI/CD Integration**: Automated workflows and deployment pipelines
- **Production Systems**: Reliable, repeatable execution patterns
- **API Integration**: Programmatic access to LLMunix capabilities

**Example Scenarios:**
```bash
# Automated report generation
./llmunix-llm execute: "Generate weekly performance report from metrics"

# Scheduled data processing
./llmunix-llm execute: "Process overnight batch data and update database"

# CI/CD automation
./llmunix-llm execute: "Run test suite and generate deployment artifacts"
```

## Performance Characteristics

### Execution Speed

#### Claude Code Runtime
- **Interactive Speed**: Limited by dialogue exchange rate
- **Decision Quality**: High quality through Claude's native reasoning
- **Context Efficiency**: Efficient due to Claude's built-in understanding
- **User Control**: Can guide execution for optimal paths

#### LLM Interpreter
- **Autonomous Speed**: Maximum API rate limits
- **Parallel Potential**: Can run multiple instances simultaneously
- **Optimization**: LLM learns to minimize iterations
- **Consistent Performance**: Predictable execution patterns

### Resource Usage

#### Claude Code Runtime
- **API Costs**: Based on Claude conversation length
- **Memory**: Minimal local resource requirements
- **Storage**: Only for workspace state files
- **Network**: Primarily Claude API calls

#### LLM Interpreter
- **API Costs**: OpenAI API usage for LLM decisions
- **Memory**: Python process + Docker containers
- **Storage**: Container images and execution artifacts
- **Network**: OpenAI API + tool execution (Docker networking)

### Cost Analysis

| Factor | Claude Code Runtime | LLM Interpreter |
|--------|-------------------|-----------------|
| **API Costs** | Claude per-token pricing | OpenAI per-token pricing |
| **Infrastructure** | None (Claude Code handles) | Docker infrastructure |
| **Development Time** | Faster for exploration | Faster for automation |
| **Maintenance** | Minimal | Container management |

## Error Handling and Recovery

### Claude Code Runtime
- **Interactive Recovery**: User can guide error resolution
- **Claude's Intelligence**: Sophisticated built-in error analysis
- **Real-time Adaptation**: Immediate strategy adjustment
- **Human-in-the-Loop**: Natural error escalation to user

### LLM Interpreter
- **Autonomous Recovery**: LLM analyzes errors and adapts approach
- **Pattern Learning**: Improves error handling through experience
- **Systematic Debugging**: Structured error analysis and resolution
- **Fallback Strategies**: Multiple recovery approaches per error type

## Integration and Deployment

### Claude Code Runtime

**Setup:**
```bash
# Simple setup
claude  # Start Claude Code
boot llmunix  # Boot operating system
```

**Integration:**
- Direct integration with Claude Code ecosystem
- Natural language interaction model
- Immediate availability
- No additional infrastructure

### LLM Interpreter

**Setup:**
```bash
# Environment setup
pip install -r requirements_llm.txt
cp example.env .env  # Configure API key
chmod +x llmunix-llm

# Execution
./llmunix-llm boot
./llmunix-llm execute: "Your goal"
```

**Integration:**
- Programmatic API for automation
- CI/CD pipeline integration
- Docker-based execution environment
- Configurable model selection

## Security Considerations

### Claude Code Runtime
- **Security**: Relies on Claude Code's security model
- **Isolation**: Native Claude Code tool isolation
- **Audit**: Claude Code's built-in logging
- **Access Control**: Claude Code permissions

### LLM Interpreter
- **Sandbox Execution**: Docker container isolation
- **API Security**: OpenAI API key management
- **Audit Trail**: Complete execution logging
- **Container Security**: Isolated environment with cleanup

## Advanced Capabilities

### Both Runtimes Support:
- **Modular State Management**: Identical state architecture
- **Adaptive Behavior**: Dynamic constraint evolution
- **Memory Integration**: Structured experience database
- **Error Recovery**: Intelligent failure handling
- **Tool Integration**: Real tool execution capabilities

### LLM Interpreter Unique Features:
- **Pure Markdown Component System**: Robust LLM-based analysis of any markdown component without hardcoded rules
- **On-Demand Component Analysis**: Intelligent interpretation when components are called, avoiding startup delays
- **Environment Detection**: Automatic tool and OS detection
- **Programmatic Access**: Python API for integration
- **Batch Execution**: Multiple goal processing
- **Custom Model Selection**: Choice of OpenAI models

### Claude Code Runtime Unique Features:
- **Interactive Refinement**: Real-time goal clarification
- **Native Claude Intelligence**: Built-in reasoning capabilities
- **Seamless Experience**: Integrated Claude Code workflow
- **Zero Setup**: Immediate availability

## Migration Between Runtimes

### Runtime Switching
Both runtimes work with identical markdown specifications, enabling easy switching:

```bash
# Same goal, different runtimes
# Claude Code Runtime:
llmunix execute: "Create Python web scraper"

# LLM Interpreter:
./llmunix-llm execute: "Create Python web scraper"
```

### State Compatibility
- **Workspace Structure**: Identical across runtimes
- **State Files**: Compatible markdown and JSON formats
- **Output Artifacts**: Same file structures and naming
- **Execution History**: Comparable logging formats

## Hybrid Usage Patterns

### Development + Production
```bash
# Develop with Claude Code (interactive)
claude
boot llmunix
llmunix execute: "Create data processing pipeline"

# Deploy with LLM Interpreter (autonomous)
./llmunix-llm execute: "Run data processing pipeline on schedule"
```

### Prototype + Scale
```bash
# Prototype with Claude Code
llmunix execute: "Experiment with machine learning model"

# Scale with LLM Interpreter
./llmunix-llm execute: "Train ML model on production dataset"
```

## Conclusion

**Choose Claude Code Runtime for:**
- Interactive development and exploration
- Complex problem-solving requiring human insight
- Learning and experimentation
- Tasks requiring real-time feedback

**Choose LLM Interpreter for:**
- Autonomous execution and automation
- CI/CD and production workflows
- Batch processing and scheduled tasks
- Programmatic integration needs

**Both runtimes:**
- Execute identical LLMunix specifications
- Provide equivalent capabilities
- Support the same advanced features
- Generate comparable results

The choice depends on your execution style preference and use case requirements. Many organizations use both runtimes for different scenarios within the same LLMunix deployment.

---

*LLMunix: One framework, two runtimes, infinite possibilities.*


================================================
File: example.env
================================================
# LLMunix LLM Interpreter Configuration
# Copy this file to .env and configure your settings

# OpenAI API Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o

# Optional: Docker Configuration  
# DOCKER_IMAGE=alpine:latest

# Optional: LLM Interpreter Settings
# MAX_ITERATIONS=10
# EXECUTION_TIMEOUT=300


================================================
File: llm_interpreter.py
================================================
#!/usr/bin/env python3

"""
LLMunix LLM Interpreter - Lightweight Runtime for Pure Markdown OS

This is a minimal runtime that delegates all logic to markdown-defined agents
and tools. The interpreter serves as a bridge between the command line and
the LLMunix markdown specifications.

Key Features:
- No hardcoded logic - everything delegated to markdown agents/tools
- Docker/CLI tool detection and mapping
- SystemAgent.md orchestrates all execution
- Pure markdown-driven decision making
- Lightweight bridge to host system tools

The runtime reads markdown specifications and lets the SystemAgent make all decisions.
"""

import os
import sys
import json
import time
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime

# Environment setup
from dotenv import load_dotenv
load_dotenv()

# OpenAI client
from openai import OpenAI

@dataclass
class ExecutionContext:
    """Maintains execution state and configuration"""
    goal: str
    workspace_dir: Path
    state_dir: Path
    container_name: Optional[str] = None
    environment_info: Dict[str, Any] = None
    execution_history: List[Dict] = None
    
    def __post_init__(self):
        if self.execution_history is None:
            self.execution_history = []
        if self.environment_info is None:
            self.environment_info = {}

class LLMunixInterpreter:
    """
    LLMunix LLM Interpreter - Lightweight Runtime
    
    Minimal runtime that delegates all logic to markdown-defined SystemAgent.
    This class only handles basic setup and environment detection.
    """
    
    def __init__(self, model: str = None):
        """Initialize the LLM interpreter"""
        
        # Validate OpenAI API key
        self.api_key = os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable.")
        
        # Initialize OpenAI client
        self.client = OpenAI(api_key=self.api_key)
        self.model = model or os.getenv('OPENAI_MODEL', 'gpt-4o')
        
        # Setup paths
        self.llmunix_root = Path(__file__).parent
        self.workspace_dir = self.llmunix_root / "workspace"
        self.state_dir = self.workspace_dir / "state"
        
        # Execution context
        self.context: Optional[ExecutionContext] = None
        
        # Load markdown components registry (lightweight discovery)
        self.component_registry = self._discover_component_files()
        
        print(f"🤖 LLMunix LLM Interpreter initialized")
        print(f"📁 LLMunix root: {self.llmunix_root}")
        print(f"🏗️  Workspace: {self.workspace_dir}")
        print(f"🧠 Model: {self.model}")
        print(f"🔧 Components loaded: {len(self.component_registry)} tools/agents")
        
        # Show discovered components for debugging
        if self.component_registry:
            print("📋 Discovered components:")
            for key, comp in self.component_registry.items():
                comp_type = comp.get('type', 'UNKNOWN')
                comp_name = comp.get('name', key)
                print(f"   • {comp_name} ({comp_type})")
    
    def boot(self):
        """Boot LLMunix operating system"""
        print("\n" + "="*60)
        print("██╗     ██╗     ███╗   ███╗██╗   ██╗███╗   ██╗██╗██╗  ██╗")
        print("██║     ██║     ████╗ ████║██║   ██║████╗  ██║██║╚██╗██╔╝")
        print("██║     ██║     ██╔████╔██║██║   ██║██╔██╗ ██║██║ ╚███╔╝ ")
        print("██║     ██║     ██║╚██╔╝██║██║   ██║██║╚██╗██║██║ ██╔██╗ ")
        print("███████╗███████╗██║ ╚═╝ ██║╚██████╔╝██║ ╚████║██║██╔╝ ██╗")
        print("╚══════╝╚══════╝╚═╝     ╚═╝ ╚═════╝ ╚═╝  ╚═══╝╚═╝╚═╝  ╚═╝")
        print("                Pure Markdown Operating System v1.0")
        print("                    LLM Interpreter Runtime")
        print("="*60)
        
        # Clean workspace
        self._clean_workspace()
        
        # Setup Docker environment if available
        self._setup_docker()
        
        print("\n🎯 LLMunix is ready for autonomous execution!")
        print("\nExample commands:")
        print('  ./llmunix-llm interactive                      # Start interactive session')
        print('  ./llmunix-llm execute: "Create a Python calculator"')
        print('  ./llmunix-llm execute: "Research AI trends" -i   # Execute and enter interactive mode')
        print('  ./llmunix-llm execute: "Fetch data from URL and analyze"')
        print("\nInteractive session features:")
        print('  🔄 Goal refinement with "refine" command')
        print('  📊 Workspace status with "status" command') 
        print('  📜 Execution history with "history" command')
        print('  🧹 Workspace management with "clear" command')
        print("\n" + "="*60 + "\n")
    
    def execute(self, goal: str, cleanup_after=True):
        """Execute a goal by delegating to SystemAgent.md"""
        
        print(f"🎯 Executing goal: {goal}")
        print("="*60)
        
        # Initialize execution context
        self.context = ExecutionContext(
            goal=goal,
            workspace_dir=self.workspace_dir,
            state_dir=self.state_dir,
            container_name=getattr(self, 'container_name', None)
        )
        
        # Setup workspace and state
        self._setup_workspace()
        
        # Detect environment if Docker is available
        if self.context.container_name:
            self._detect_container_environment()
        
        # Delegate execution to SystemAgent
        try:
            self._delegate_to_system_agent()
        except Exception as e:
            print(f"❌ Execution failed: {e}")
            self._log_error(str(e))
        finally:
            if cleanup_after:
                self._cleanup_execution()
    
    def _setup_workspace(self):
        """Setup modular workspace structure"""
        print("🏗️  Setting up modular workspace...")
        
        # Create directories
        self.workspace_dir.mkdir(exist_ok=True)
        self.state_dir.mkdir(exist_ok=True)
        
        # Initialize state files
        state_files = {
            'plan.md': f"# Execution Plan\n\n**Goal:** {self.context.goal}\n\n## Status\nInitializing...\n",
            'context.md': "# Execution Context\n\n## Knowledge Accumulation\n\n",
            'history.md': f"# Execution History\n\n**Started:** {datetime.now().isoformat()}\n\n",
            'constraints.md': "# Behavioral Constraints\n\n## Initial Settings\n- priority: balanced\n- error_tolerance: moderate\n",
            'variables.json': json.dumps({"goal": self.context.goal, "start_time": datetime.now().isoformat()}, indent=2)
        }
        
        for filename, content in state_files.items():
            file_path = self.state_dir / filename
            file_path.write_text(content, encoding='utf-8')
        
        print(f"✅ Workspace setup complete: {self.workspace_dir}")
    
    def _setup_docker(self):
        """Setup Docker environment for tool execution"""
        try:
            # Check if Docker is available
            result = subprocess.run(['docker', '--version'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode != 0:
                print("⚠️  Docker not available - running in simulation mode")
                return
            
            print("🐳 Docker detected - setting up execution environment...")
            
            # Create and start container
            container_name = f"llmunix-{int(time.time())}"
            
            # Mount workspace directory for file access from outside container
            workspace_mount = f"{self.workspace_dir}:/workspace"
            
            # Use Alpine Linux for lightweight container with package manager detection
            run_command = [
                'docker', 'run', '-d', '--name', container_name,
                '-v', workspace_mount,
                '-w', '/workspace',
                'alpine:latest',
                'sh', '-c', 'apk add --no-cache python3 py3-pip curl bash && pip install requests beautifulsoup4 && sleep 3600'
            ]
            
            result = subprocess.run(run_command, capture_output=True, text=True, timeout=60)
            if result.returncode == 0:
                self.container_name = container_name
                print(f"✅ Container '{container_name}' created and running")
                time.sleep(5)  # Allow time for package installation
            else:
                print(f"⚠️  Failed to create container: {result.stderr}")
                
        except Exception as e:
            print(f"⚠️  Docker setup failed: {e}")
    
    def _detect_container_environment(self):
        """Detect available tools and environment in the container"""
        if not hasattr(self, 'container_name') or not self.container_name:
            return
            
        print("🔍 Detecting container environment...")
        
        # Common tools to check
        tools_to_check = [
            'python3', 'python', 'pip', 'pip3', 'curl', 'wget', 'bash', 'sh',
            'git', 'nano', 'vim', 'cat', 'grep', 'sed', 'awk', 'jq'
        ]
        
        # Package managers to check
        package_managers = ['apt-get', 'yum', 'apk', 'dnf', 'zypper']
        
        available_tools = []
        available_package_managers = []
        
        # Check tools
        for tool in tools_to_check:
            try:
                result = subprocess.run([
                    'docker', 'exec', self.container_name, 'which', tool
                ], capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    available_tools.append(tool)
            except:
                pass
        
        # Check package managers
        for pm in package_managers:
            try:
                result = subprocess.run([
                    'docker', 'exec', self.container_name, 'which', pm
                ], capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    available_package_managers.append(pm)
            except:
                pass
        
        # Get OS information
        os_info = self._get_os_info()
        distro_info = self._detect_distro()
        
        # Store environment information
        self.context.environment_info = {
            'available_tools': available_tools,
            'package_managers': available_package_managers,
            'os_info': os_info,
            'distro': distro_info,
            'container_name': self.container_name
        }
        
        # Create environment documentation
        self._create_environment_doc()
        
        print(f"✅ Environment detected: {len(available_tools)} tools, {len(available_package_managers)} package managers")
        print(f"🐧 OS: {distro_info}")
    
    def _get_os_info(self):
        """Get OS information from container"""
        try:
            result = subprocess.run([
                'docker', 'exec', self.container_name, 'uname', '-a'
            ], capture_output=True, text=True, timeout=5)
            return result.stdout.strip() if result.returncode == 0 else "Unknown"
        except:
            return "Unknown"
    
    def _detect_distro(self):
        """Detect Linux distribution"""
        distro_files = [
            ('/etc/alpine-release', 'Alpine Linux'),
            ('/etc/os-release', 'os-release'),
            ('/etc/redhat-release', 'Red Hat based'),
            ('/etc/debian_version', 'Debian based')
        ]
        
        for file_path, distro_type in distro_files:
            try:
                result = subprocess.run([
                    'docker', 'exec', self.container_name, 'cat', file_path
                ], capture_output=True, text=True, timeout=5)
                if result.returncode == 0:
                    if file_path == '/etc/alpine-release':
                        return f"Alpine Linux v{result.stdout.strip()}"
                    elif file_path == '/etc/os-release':
                        # Parse os-release for detailed info
                        for line in result.stdout.split('\n'):
                            if line.startswith('PRETTY_NAME='):
                                return line.split('=')[1].strip('\"')
                    else:
                        return f"{distro_type}: {result.stdout.strip()}"
            except:
                pass
        
        return "Unknown Linux distribution"
    
    def _create_environment_doc(self):
        """Create environment documentation for LLM reference"""
        env_info = self.context.environment_info
        
        doc_content = f"""# Container Environment Detection

## Available Tools
{', '.join(env_info.get('available_tools', []))}

## Package Managers  
{', '.join(env_info.get('package_managers', []))}

## OS Information
{env_info.get('os_info', 'Unknown')}

## Distribution
{env_info.get('distro', 'Unknown')}

## Recommendations for SystemAgent
- Use these available tools: {', '.join(env_info.get('available_tools', [])[:10])}
- For package installation, use: {env_info.get('package_managers', ['unknown'])[0] if env_info.get('package_managers') else 'unknown'}
- This is a {env_info.get('distro', 'unknown')} system

## Notes
- Always check tool availability before using
- Use appropriate package manager for this system
- Consider OS-specific command variations
"""
        
        env_file = self.state_dir / "container_environment.md"
        env_file.write_text(doc_content, encoding='utf-8')
        print(f"📋 Environment documentation created: {env_file}")
    
    def _delegate_to_system_agent(self):
        """Delegate execution to SystemAgent.md with real tool execution loop"""
        print("🧠 Delegating to SystemAgent.md...")
        
        # Read SystemAgent specification
        system_agent_path = self.llmunix_root / "system" / "SystemAgent.md"
        if not system_agent_path.exists():
            raise FileNotFoundError(f"SystemAgent.md not found at {system_agent_path}")
        
        system_agent_spec = system_agent_path.read_text(encoding='utf-8')
        
        # Initialize execution context
        execution_context = self._build_full_execution_context()
        
        # Start agentic execution loop
        max_iterations = int(os.getenv('MAX_ITERATIONS', 10))
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            print(f"🔄 Execution iteration {iteration}")
            
            # Build current prompt with context
            prompt = self._build_iteration_prompt(system_agent_spec, execution_context, iteration)
            
            # Get LLM response
            response = self._call_llm(prompt, max_tokens=2000)
            
            # Log the response
            self._log_execution_step(f"SystemAgent Iteration {iteration}", response)
            
            # Parse and execute any tool calls from the response
            tool_results = self._extract_and_execute_tools(response)
            
            # Update execution context with results
            execution_context = self._update_execution_context(execution_context, response, tool_results)
            
            # Let SystemAgent decide if execution is complete via its response
            if "EXECUTION_COMPLETE" in response or "TASK_COMPLETE" in response:
                print("✅ Execution completed successfully")
                break
            
            # Update state files
            self._update_state_with_context(execution_context)
        
        if iteration >= max_iterations:
            print("⚠️ Maximum iterations reached")
        
        print("✅ Execution delegated to SystemAgent")
    
    def _get_available_cli_tools(self) -> str:
        """Get list of available CLI tools"""
        if self.context.environment_info:
            tools = self.context.environment_info.get('available_tools', [])
            return f"Container tools: {', '.join(tools)}"
        else:
            # Host system tools
            common_tools = ['python3', 'curl', 'cat', 'echo', 'mkdir', 'ls', 'grep', 'sed']
            return f"Host system tools: {', '.join(common_tools)} (detection needed)"
    
    def _list_workspace_files(self) -> str:
        """List current workspace files"""
        files = []
        for file_path in self.workspace_dir.rglob('*'):
            if file_path.is_file():
                rel_path = file_path.relative_to(self.workspace_dir)
                files.append(str(rel_path))
        return "\n".join(files) if files else "No files in workspace"
    
    def _build_full_execution_context(self) -> Dict[str, Any]:
        """Build complete execution context for agentic loop"""
        return {
            'goal': self.context.goal,
            'workspace_dir': str(self.workspace_dir),
            'state_dir': str(self.state_dir),
            'container_name': self.context.container_name,
            'environment_info': self.context.environment_info or {},
            'execution_history': [],
            'tool_results': [],
            'current_state': 'initialized'
        }
    
    def _build_iteration_prompt(self, system_agent_spec: str, execution_context: Dict, iteration: int) -> str:
        """Build prompt for current iteration with full context"""
        
        # Get current workspace state
        workspace_files = self._list_workspace_files()
        available_tools = self._get_available_cli_tools()
        
        prompt = f"""You are acting as the SystemAgent from this specification:

{system_agent_spec}

EXECUTION CONTEXT (Iteration {iteration}):
- Goal: {execution_context['goal']}
- Workspace: {execution_context['workspace_dir']}
- State directory: {execution_context['state_dir']}
- Container: {execution_context['container_name'] or "None (host system)"}
- Current state: {execution_context['current_state']}

ENVIRONMENT:
{self._format_environment_info(execution_context['environment_info'])}

AVAILABLE CLI TOOLS:
{available_tools}

WORKSPACE STATE FILES:
{workspace_files}

EXECUTION HISTORY:
{self._format_execution_history(execution_context['execution_history'])}

TOOL RESULTS FROM PREVIOUS STEPS:
{self._format_tool_results(execution_context['tool_results'])}

INSTRUCTIONS:
You can execute real tools and commands. When you need to use a tool, format your response like this:

TOOL_CALL: command_name
PARAMETERS: parameter1=value1, parameter2=value2
REASONING: Why you need this tool

Available tool patterns:
- TOOL_CALL: curl
  PARAMETERS: url=https://example.com, output_file=/workspace/content.html
  REASONING: Fetch web content

- TOOL_CALL: python3
  PARAMETERS: script=/workspace/process.py, args=input.txt output.txt
  REASONING: Process downloaded content

- TOOL_CALL: cat
  PARAMETERS: file=/workspace/file.txt
  REASONING: Read file contents

Continue execution according to the SystemAgent specification. If you need to execute tools, use the TOOL_CALL format above."""

        return prompt
    
    def _extract_and_execute_tools(self, response: str) -> List[Dict[str, Any]]:
        """Extract and execute tool calls from LLM response"""
        tool_results = []
        
        # Parse tool calls from response
        lines = response.split('\n')
        i = 0
        
        while i < len(lines):
            if lines[i].startswith('TOOL_CALL:'):
                tool_call = self._parse_tool_call(lines, i)
                if tool_call:
                    print(f"🔧 Executing tool: {tool_call['command']}")
                    result = self._execute_tool_call(tool_call)
                    tool_results.append(result)
                    i += tool_call.get('lines_consumed', 1)
                else:
                    i += 1
            else:
                i += 1
        
        return tool_results
    
    def _parse_tool_call(self, lines: List[str], start_index: int) -> Dict[str, Any]:
        """Parse a tool call from response lines"""
        if start_index >= len(lines):
            return None
            
        tool_call = {
            'command': lines[start_index].replace('TOOL_CALL:', '').strip(),
            'parameters': {},
            'reasoning': '',
            'lines_consumed': 1
        }
        
        # Parse parameters and reasoning
        i = start_index + 1
        while i < len(lines) and i < start_index + 10:  # Look ahead max 10 lines
            line = lines[i].strip()
            if line.startswith('PARAMETERS:'):
                param_str = line.replace('PARAMETERS:', '').strip()
                tool_call['parameters'] = self._parse_parameters(param_str)
                tool_call['lines_consumed'] += 1
            elif line.startswith('REASONING:'):
                tool_call['reasoning'] = line.replace('REASONING:', '').strip()
                tool_call['lines_consumed'] += 1
            elif line == '' or not line.startswith(('PARAMETERS:', 'REASONING:')):
                break
            i += 1
        
        return tool_call
    
    def _parse_parameters(self, param_str: str) -> Dict[str, str]:
        """Parse parameters string into dictionary"""
        params = {}
        if not param_str:
            return params
            
        # Split by comma and parse key=value pairs
        for pair in param_str.split(','):
            if '=' in pair:
                key, value = pair.split('=', 1)
                params[key.strip()] = value.strip()
        
        return params
    
    def _execute_tool_call(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a tool call - either CLI command or markdown component"""
        command = tool_call['command']
        params = tool_call['parameters']
        
        result = {
            'tool': command,
            'parameters': params,
            'reasoning': tool_call['reasoning'],
            'success': False,
            'output': '',
            'error': '',
            'timestamp': datetime.now().isoformat()
        }
        
        try:
            # First, check if this is a markdown component
            if self._is_markdown_component(command):
                print(f"🔧 Executing markdown component: {command}")
                markdown_result = self._execute_markdown_component(command, params)
                
                # Convert markdown result to tool result format
                result['success'] = markdown_result.get('success', False)
                result['output'] = markdown_result.get('output', '')
                result['error'] = markdown_result.get('error', '')
                result['metadata'] = markdown_result.get('metadata', {})
                
                if result['success']:
                    print(f"✅ Markdown component '{command}' executed successfully")
                else:
                    print(f"❌ Markdown component '{command}' failed: {result['error']}")
                
                return result
            
            # Otherwise, execute as CLI command
            if command == 'curl':
                result = self._execute_curl(params, result)
            elif command == 'python3':
                result = self._execute_python(params, result)
            elif command in ['cat', 'ls', 'grep', 'echo', 'mkdir']:
                result = self._execute_shell_command(command, params, result)
            else:
                result = self._execute_generic_command(command, params, result)
                
        except Exception as e:
            result['error'] = str(e)
            result['success'] = False
            print(f"❌ Tool execution failed: {e}")
        
        return result
    
    def _is_markdown_component(self, command: str) -> bool:
        """Check if a command refers to a markdown component with on-demand analysis"""
        
        # First check against known component patterns
        common_component_patterns = [
            'QueryMemoryTool', 'RealWebFetchTool', 'RealFileSystemTool', 'RealSummarizationAgent',
            'MemoryAnalysisAgent', 'WebFetcherTool', 'SummarizationAgent', 'FileWriterTool',
            'SystemAgent', 'LLMInterpreterWebFetchTool'
        ]
        
        if command in common_component_patterns:
            return True
        
        # Check against file stems (component names)
        for comp_data in self.component_registry.values():
            file_stem = comp_data.get('file_stem', '')
            
            # Direct matches
            if (command == file_stem or 
                command.lower() == file_stem.lower()):
                return True
            
            # Partial matches for common component naming patterns
            if (any(keyword in command.lower() for keyword in ['tool', 'agent']) and
                any(keyword in file_stem.lower() for keyword in ['tool', 'agent']) and
                (command.lower() in file_stem.lower() or file_stem.lower() in command.lower())):
                return True
        
        return False
    
    def _execute_curl(self, params: Dict[str, str], result: Dict[str, Any]) -> Dict[str, Any]:
        """Execute curl command to fetch web content"""
        url = params.get('url', '')
        output_file = params.get('output_file', '/workspace/fetched_content.html')
        
        if not url:
            result['error'] = "URL parameter required for curl"
            return result
        
        if self.context.container_name:
            # Execute in container
            cmd = ['docker', 'exec', self.context.container_name, 'curl', '-s', '-L', url, '-o', output_file]
        else:
            # Execute on host
            cmd = ['curl', '-s', '-L', url, '-o', output_file]
        
        process_result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        result['success'] = process_result.returncode == 0
        result['output'] = process_result.stdout
        result['error'] = process_result.stderr
        
        if result['success']:
            print(f"✅ Successfully fetched {url} to {output_file}")
        
        return result
    
    def _execute_python(self, params: Dict[str, str], result: Dict[str, Any]) -> Dict[str, Any]:
        """Execute Python script"""
        script = params.get('script', '')
        args = params.get('args', '')
        
        if not script:
            result['error'] = "Script parameter required for python3"
            return result
        
        if self.context.container_name:
            cmd = ['docker', 'exec', self.context.container_name, 'python3', script] + args.split() if args else ['docker', 'exec', self.context.container_name, 'python3', script]
        else:
            cmd = ['python3', script] + args.split() if args else ['python3', script]
        
        process_result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        result['success'] = process_result.returncode == 0
        result['output'] = process_result.stdout
        result['error'] = process_result.stderr
        
        return result
    
    def _execute_shell_command(self, command: str, params: Dict[str, str], result: Dict[str, Any]) -> Dict[str, Any]:
        """Execute basic shell commands"""
        args = []
        
        # Build command arguments from parameters
        if command == 'cat':
            file_path = params.get('file', '')
            if file_path:
                args = [file_path]
        elif command == 'ls':
            path = params.get('path', '/workspace')
            args = [path]
        elif command == 'mkdir':
            path = params.get('path', '')
            if path:
                args = ['-p', path]
        elif command == 'echo':
            text = params.get('text', '')
            if text:
                args = [text]
        
        if self.context.container_name:
            cmd = ['docker', 'exec', self.context.container_name, command] + args
        else:
            cmd = [command] + args
        
        process_result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        result['success'] = process_result.returncode == 0
        result['output'] = process_result.stdout
        result['error'] = process_result.stderr
        
        return result
    
    def _execute_generic_command(self, command: str, params: Dict[str, str], result: Dict[str, Any]) -> Dict[str, Any]:
        """Execute generic command with parameters"""
        args = []
        for key, value in params.items():
            if key != 'command':
                args.extend([f'--{key}', value])
        
        if self.context.container_name:
            cmd = ['docker', 'exec', self.context.container_name, command] + args
        else:
            cmd = [command] + args
        
        process_result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        result['success'] = process_result.returncode == 0
        result['output'] = process_result.stdout
        result['error'] = process_result.stderr
        
        return result
    
    def _update_execution_context(self, context: Dict[str, Any], response: str, tool_results: List[Dict]) -> Dict[str, Any]:
        """Update execution context with latest results"""
        context['execution_history'].append({
            'response': response,
            'timestamp': datetime.now().isoformat()
        })
        context['tool_results'].extend(tool_results)
        
        # Update current state based on results
        if tool_results:
            if any(not r['success'] for r in tool_results):
                context['current_state'] = 'error_recovery'
            else:
                context['current_state'] = 'progressing'
        
        return context
    
    
    def _update_state_with_context(self, context: Dict[str, Any]):
        """Update state files with current context"""
        # Update plan with current state
        plan_content = f"# Execution Plan\n\n**Goal:** {context['goal']}\n\n## Current State\n{context['current_state']}\n\n## Progress\n{len(context['execution_history'])} iterations completed\n"
        self._update_state_file('plan.md', plan_content)
        
        # Update variables with latest data
        variables = {
            'goal': context['goal'],
            'current_state': context['current_state'],
            'iterations': len(context['execution_history']),
            'tool_calls': len(context['tool_results'])
        }
        self._update_state_file('variables.json', json.dumps(variables, indent=2))
    
    def _format_environment_info(self, env_info: Dict) -> str:
        """Format environment info for prompt"""
        if not env_info:
            return "Environment: Not detected"
        
        return f"""
        Available tools: {', '.join(env_info.get('available_tools', [])[:10])}
        Package managers: {', '.join(env_info.get('package_managers', []))}
        OS: {env_info.get('distro', 'Unknown')}
        Container: {env_info.get('container_name', 'None')}
        """
    
    def _format_execution_history(self, history: List[Dict]) -> str:
        """Format execution history for prompt"""
        if not history:
            return "No previous execution steps"
        
        formatted = []
        for i, step in enumerate(history[-3:], 1):  # Show last 3 steps
            response = step.get('response', '')
            if isinstance(response, str):
                formatted.append(f"Step {i}: {response[:200]}...")
            else:
                formatted.append(f"Step {i}: {str(response)[:200]}...")
        
        return '\n'.join(formatted)
    
    def _format_tool_results(self, results: List[Dict]) -> str:
        """Format tool results for prompt"""
        if not results:
            return "No tool executions yet"
        
        formatted = []
        for result in results[-5:]:  # Show last 5 results
            status = "✅" if result.get('success') else "❌"
            tool_name = result.get('tool', 'unknown')
            output = result.get('output', '')
            if isinstance(output, str):
                output_preview = output[:100]
            else:
                output_preview = str(output)[:100]
            formatted.append(f"{status} {tool_name}: {output_preview}...")
        
        return '\n'.join(formatted)

    def _discover_component_files(self) -> Dict[str, Dict]:
        """Lightweight discovery of markdown files without LLM analysis"""
        registry = {}
        
        # Discover all markdown files
        md_files = []
        
        # Components directory (most likely to contain components)
        components_dir = self.llmunix_root / "components"
        if components_dir.exists():
            md_files.extend(components_dir.rglob("*.md"))
        
        # System directory
        system_dir = self.llmunix_root / "system"
        if system_dir.exists():
            md_files.extend(system_dir.glob("*.md"))
        
        # Create basic registry entries for all markdown files
        for md_file in md_files:
            file_key = md_file.stem
            registry[file_key] = {
                'file_stem': file_key,
                'full_path': str(md_file),
                'relative_path': str(md_file.relative_to(self.llmunix_root)),
                'analyzed': False  # Will be analyzed on-demand
            }
        
        return registry
    
    def _load_component_registry(self) -> Dict[str, Dict]:
        """Load all markdown components using LLM analysis"""
        registry = {}
        
        # Discover all markdown files in the system
        md_files = []
        
        # System directory
        system_dir = self.llmunix_root / "system"
        if system_dir.exists():
            md_files.extend(system_dir.glob("*.md"))
        
        # Components directory (recursive)
        components_dir = self.llmunix_root / "components"
        if components_dir.exists():
            md_files.extend(components_dir.rglob("*.md"))
        
        # Scenarios directory
        scenarios_dir = self.llmunix_root / "scenarios"
        if scenarios_dir.exists():
            md_files.extend(scenarios_dir.glob("*.md"))
        
        # Prioritize likely component files for analysis
        component_files = []
        other_files = []
        
        for md_file in md_files:
            # Prioritize files in components/ directory and with component-like names
            if ('components' in str(md_file) or 
                any(keyword in md_file.stem.lower() for keyword in ['tool', 'agent', 'query', 'real', 'fetch', 'memory'])):
                component_files.append(md_file)
            else:
                other_files.append(md_file)
        
        # Analyze component files first (likely to be actual components)
        for md_file in component_files[:8]:  # Limit to first 8 to avoid timeout
            try:
                print(f"🔍 Analyzing component: {md_file.name}")
                component_info = self._analyze_markdown_file(md_file)
                if component_info and component_info.get('is_component'):
                    file_key = md_file.stem
                    registry[file_key] = component_info
                    print(f"✅ Found component: {component_info.get('name', file_key)}")
            except Exception as e:
                print(f"⚠️  Error analyzing {md_file}: {e}")
        
        # Analyze a few other files if we have capacity
        for md_file in other_files[:3]:  # Limit to 3 others
            try:
                component_info = self._analyze_markdown_file(md_file)
                if component_info and component_info.get('is_component'):
                    file_key = md_file.stem
                    registry[file_key] = component_info
            except Exception as e:
                print(f"⚠️  Error analyzing {md_file}: {e}")
        
        return registry
    
    def _analyze_markdown_file(self, md_file: Path) -> Dict[str, Any]:
        """Use LLM to analyze a markdown file and determine if it's a component"""
        
        try:
            content = md_file.read_text(encoding='utf-8')
            
            # Skip very large files to avoid token limits
            if len(content) > 10000:
                content = content[:10000] + "\n...[truncated]"
            
            analysis_prompt = f"""Analyze this markdown file to determine if it defines a LLMunix component (tool or agent).

FILE PATH: {md_file}
FILE CONTENT:
{content}

ANALYSIS INSTRUCTIONS:
1. Determine if this file defines a reusable component (tool or agent)
2. Extract key information about the component
3. Identify what type of component it is
4. Look for execution patterns, inputs, outputs, and capabilities

Respond with JSON in this exact format:
```json
{{
  "is_component": true/false,
  "name": "component name",
  "type": "TOOL|AGENT|SYSTEM|SCENARIO|OTHER",
  "description": "brief description",
  "inputs": ["list", "of", "input", "parameters"],
  "outputs": ["list", "of", "output", "parameters"],
  "capabilities": ["list", "of", "capabilities"],
  "execution_pattern": "how this component works",
  "real_tools_used": ["list", "of", "real", "tools", "it", "uses"],
  "file_path": "{md_file}",
  "callable_names": ["possible", "names", "to", "call", "this", "component"]
}}
```

Be strict about is_component - only return true for files that define reusable, executable components."""
            
            response = self._call_llm(analysis_prompt, max_tokens=1000)
            
            # Extract JSON from response
            json_start = response.find('```json')
            if json_start != -1:
                json_start += 7
                json_end = response.find('```', json_start)
                if json_end != -1:
                    json_str = response[json_start:json_end].strip()
                    result = json.loads(json_str)
                    
                    # Add file metadata
                    result['full_path'] = str(md_file)
                    result['relative_path'] = str(md_file.relative_to(self.llmunix_root))
                    result['file_stem'] = md_file.stem
                    
                    return result
            
            # Fallback if JSON parsing fails
            return {
                'is_component': False,
                'error': 'Failed to parse LLM analysis',
                'full_path': str(md_file)
            }
            
        except Exception as e:
            return {
                'is_component': False,
                'error': str(e),
                'full_path': str(md_file)
            }
    
    def _execute_markdown_component(self, component_name: str, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a markdown-defined component using LLM interpretation"""
        
        # Find component file by name matching
        component = None
        for comp_data in self.component_registry.values():
            file_stem = comp_data.get('file_stem', '')
            
            # Check various matching patterns
            if (component_name == file_stem or 
                component_name.lower() == file_stem.lower() or
                (any(keyword in component_name.lower() for keyword in ['tool', 'agent']) and
                 any(keyword in file_stem.lower() for keyword in ['tool', 'agent']) and
                 (component_name.lower() in file_stem.lower() or file_stem.lower() in component_name.lower()))):
                component = comp_data
                break
        
        if not component:
            return {
                'success': False,
                'error': f"Component '{component_name}' not found. Available: {list(self.component_registry.keys())}",
                'output': ''
            }
        
        # Load component specification
        try:
            component_path = Path(component['full_path'])
            
            if not component_path.exists():
                return {
                    'success': False,
                    'error': f"Component file not found: {component_path}",
                    'output': ''
                }
            
            component_spec = component_path.read_text(encoding='utf-8')
            
            # Analyze component on-demand if not already analyzed
            if not component.get('analyzed', False):
                print(f"🔍 On-demand analysis of: {component_name}")
                analyzed_data = self._analyze_markdown_file(component_path)
                if analyzed_data:
                    component.update(analyzed_data)
                    component['analyzed'] = True
            
            # Execute component using LLM interpretation
            return self._interpret_and_execute_component(component_spec, component, inputs)
            
        except Exception as e:
            return {
                'success': False,
                'error': f"Error loading component: {e}",
                'output': ''
            }
    
    def _interpret_and_execute_component(self, spec: str, component: Dict, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Use LLM to interpret markdown specification and execute with real tools"""
        
        # Build execution prompt using LLM-analyzed metadata
        prompt = f"""You are executing a LLMunix component. Your task is to interpret the markdown specification and execute it using real tools.

COMPONENT SPECIFICATION:
{spec}

LLM-ANALYZED COMPONENT METADATA:
- Name: {component.get('name', 'Unknown')}
- Type: {component.get('type', 'Unknown')}
- Description: {component.get('description', 'No description')}
- Capabilities: {', '.join(component.get('capabilities', []))}
- Expected Inputs: {', '.join(component.get('inputs', []))}
- Expected Outputs: {', '.join(component.get('outputs', []))}
- Execution Pattern: {component.get('execution_pattern', 'Not specified')}
- Real Tools Used: {', '.join(component.get('real_tools_used', []))}

INPUT PARAMETERS:
{json.dumps(inputs, indent=2)}

EXECUTION CONTEXT:
- Workspace: {self.workspace_dir}
- State Directory: {self.state_dir}
- Container: {self.context.container_name if self.context else 'None'}

AVAILABLE REAL TOOLS:
{self._get_available_cli_tools()}

INSTRUCTIONS:
1. Read and understand the component specification and its analyzed metadata
2. Follow the execution pattern described in the metadata
3. Use the expected inputs and produce the expected outputs
4. Execute the required steps using real tools (TOOL_CALL format)
5. Leverage the real tools identified in the metadata analysis

You can use these real tools:
- curl: For web requests
- python3: For data processing
- cat: For reading files
- echo: For creating simple content
- grep: For searching content
- mkdir: For creating directories
- ls: For listing files

Use the TOOL_CALL format when you need to execute tools:

TOOL_CALL: command_name
PARAMETERS: param1=value1, param2=value2
REASONING: Why you need this tool

After all tool executions, provide a final JSON result matching the component's output specification:
```json
{{
  "success": true,
  "output": "main result content based on component outputs: {', '.join(component.get('outputs', []))}",
  "metadata": {{"execution_time": "...", "tools_used": [...], "component_type": "{component.get('type', 'Unknown')}"}},
  "error": null
}}
```"""

        # Get LLM response for component execution
        response = self._call_llm(prompt, max_tokens=3000)
        
        # Extract and execute any tool calls
        tool_results = self._extract_and_execute_tools(response)
        
        # Parse final JSON result from response
        final_result = self._extract_final_result(response, tool_results)
        
        return final_result
    
    def _extract_final_result(self, response: str, tool_results: List[Dict]) -> Dict[str, Any]:
        """Extract the final JSON result from LLM response"""
        
        # Look for JSON block in response
        try:
            # Find JSON block
            json_start = response.find('```json')
            if json_start != -1:
                json_start += 7  # Skip ```json
                json_end = response.find('```', json_start)
                if json_end != -1:
                    json_str = response[json_start:json_end].strip()
                    result = json.loads(json_str)
                    
                    # Add tool execution metadata
                    if 'metadata' not in result:
                        result['metadata'] = {}
                    result['metadata']['tool_executions'] = len(tool_results)
                    result['metadata']['successful_tools'] = sum(1 for r in tool_results if r.get('success'))
                    
                    return result
        except Exception as e:
            print(f"⚠️  Error parsing JSON result: {e}")
        
        # Fallback result if JSON parsing fails
        success = len(tool_results) == 0 or any(r.get('success') for r in tool_results)
        output = response if not tool_results else tool_results[-1].get('output', response)
        
        return {
            'success': success,
            'output': output,
            'metadata': {
                'tool_executions': len(tool_results),
                'successful_tools': sum(1 for r in tool_results if r.get('success')),
                'fallback_parsing': True
            },
            'error': None if success else 'Component execution may have failed'
        }

    # Utility methods
    
    def _build_system_context(self) -> str:
        """Build system context for LLM"""
        return f"""
        - LLMunix Pure Markdown Operating System
        - Runtime: LLM Interpreter (lightweight)
        - Workspace: {self.workspace_dir}
        - State management: {self.state_dir}
        - Docker available: {hasattr(self, 'container_name')}
        """
    
    def _build_environment_context(self) -> str:
        """Build environment context for LLM"""
        if not self.context.environment_info:
            return "Environment: Not detected (simulation mode)"
        
        env = self.context.environment_info
        return f"""
        Available tools: {', '.join(env.get('available_tools', [])[:10])}
        Package managers: {', '.join(env.get('package_managers', []))}
        OS: {env.get('distro', 'Unknown')}
        Container: {env.get('container_name', 'None')}
        """
    
    def _call_llm(self, prompt: str, max_tokens: int = 1000) -> str:
        """Call OpenAI LLM with prompt"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            error_msg = f"LLM call failed: {e}"
            print(f"❌ {error_msg}")
            return f"ERROR: {error_msg}"
    
    def _update_state_file(self, filename: str, content: str):
        """Update a state file"""
        file_path = self.state_dir / filename
        file_path.write_text(content, encoding='utf-8')
    
    def _read_state_file(self, filename: str) -> str:
        """Read a state file"""
        file_path = self.state_dir / filename
        return file_path.read_text(encoding='utf-8') if file_path.exists() else ""
    
    def _log_execution_step(self, step: str, content: str):
        """Log an execution step"""
        timestamp = datetime.now().isoformat()
        log_entry = f"\n## {step} - {timestamp}\n\n{content}\n"
        
        history_file = self.state_dir / "history.md"
        current_history = history_file.read_text(encoding='utf-8') if history_file.exists() else ""
        updated_history = current_history + log_entry
        
        history_file.write_text(updated_history, encoding='utf-8')
    
    def _update_context(self, content: str):
        """Update execution context"""
        timestamp = datetime.now().isoformat()
        context_entry = f"\n**{timestamp}:** {content}\n"
        
        context_file = self.state_dir / "context.md"
        current_context = context_file.read_text(encoding='utf-8') if context_file.exists() else ""
        updated_context = current_context + context_entry
        
        context_file.write_text(updated_context, encoding='utf-8')
    
    def _log_error(self, error: str):
        """Log an error"""
        self._log_execution_step("ERROR", error)
    
    def _clean_workspace(self):
        """Clean workspace for fresh execution"""
        import shutil
        
        if self.workspace_dir.exists():
            shutil.rmtree(self.workspace_dir)
        
        self.workspace_dir.mkdir(exist_ok=True)
        print("🧹 Workspace cleaned")
    
    def _cleanup_execution(self):
        """Cleanup after execution"""
        if hasattr(self, 'container_name') and self.container_name:
            try:
                subprocess.run(['docker', 'stop', self.container_name], 
                             capture_output=True, timeout=30)
                subprocess.run(['docker', 'rm', self.container_name], 
                             capture_output=True, timeout=30)
                print(f"🧹 Container {self.container_name} cleaned up")
            except:
                pass
    
    def interactive_session(self):
        """Start an interactive session for goal refinement and new tasks"""
        print("\n" + "="*60)
        print("🎯 LLMunix Interactive Session")
        print("="*60)
        print("Enter goals to execute, or type commands:")
        print("  'refine' - Refine the last goal")
        print("  'status' - Show current workspace status")
        print("  'history' - Show execution history")
        print("  'clear' - Clear workspace for fresh start")
        print("  'help' - Show this help")
        print("  'exit' or 'quit' - Exit interactive session")
        print("-"*60)
        
        try:
            while True:
                try:
                    # Get user input
                    user_input = input("\n🎯 llmunix> ").strip()
                    
                    if not user_input:
                        continue
                    
                    # Handle commands
                    if user_input.lower() in ['exit', 'quit', 'q']:
                        print("👋 Goodbye!")
                        break
                    elif user_input.lower() == 'help':
                        self._show_interactive_help()
                    elif user_input.lower() == 'status':
                        self._show_status()
                    elif user_input.lower() == 'history':
                        self._show_history()
                    elif user_input.lower() == 'clear':
                        self._interactive_clear_workspace()
                    elif user_input.lower() == 'refine':
                        self._refine_last_goal()
                    else:
                        # Treat as a new goal to execute
                        print(f"\n🚀 Executing: {user_input}")
                        self.execute(user_input, cleanup_after=False)
                        print(f"\n✅ Goal completed. Ready for next command.")
                        
                except KeyboardInterrupt:
                    print("\n\n👋 Session interrupted. Goodbye!")
                    break
                except EOFError:
                    print("\n\n👋 Session ended. Goodbye!")
                    break
                except Exception as e:
                    print(f"\n❌ Error: {e}")
                    print("Type 'help' for available commands or 'exit' to quit.")
        finally:
            # Cleanup when session ends
            self._cleanup_execution()
    
    def _show_interactive_help(self):
        """Show interactive session help"""
        print("\n📖 LLMunix Interactive Session Help")
        print("-"*40)
        print("Commands:")
        print("  refine       - Refine and re-execute the last goal")
        print("  status       - Show workspace and execution status")
        print("  history      - Show execution history")
        print("  clear        - Clear workspace for fresh start")
        print("  help         - Show this help")
        print("  exit/quit    - Exit interactive session")
        print("\nTo execute a goal, simply type it:")
        print("  Create a Python calculator")
        print("  Fetch content from https://example.com")
        print("  Analyze the data in my workspace")
        print("\nExamples:")
        print("  🎯 llmunix> Create a simple web scraper")
        print("  🎯 llmunix> refine")
        print("  🎯 llmunix> status")
        print("  🎯 llmunix> Build a REST API server")
    
    def _show_status(self):
        """Show current workspace status"""
        print("\n📊 Workspace Status")
        print("-"*30)
        
        if not self.workspace_dir.exists():
            print("⚠️  No workspace found. Execute a goal first.")
            return
        
        # Show workspace files
        files = list(self.workspace_dir.rglob('*'))
        file_count = len([f for f in files if f.is_file()])
        dir_count = len([f for f in files if f.is_dir()]) - 1  # Exclude workspace itself
        
        print(f"📁 Workspace: {self.workspace_dir}")
        print(f"📄 Files: {file_count}")
        print(f"📂 Directories: {dir_count}")
        
        # Show state files status
        if self.state_dir.exists():
            print(f"\n🧠 State Directory: {self.state_dir}")
            state_files = ['plan.md', 'history.md', 'context.md', 'constraints.md', 'variables.json']
            for state_file in state_files:
                file_path = self.state_dir / state_file
                status = "✅" if file_path.exists() else "❌"
                print(f"  {status} {state_file}")
        
        # Show recent files
        recent_files = sorted(
            [f for f in files if f.is_file()], 
            key=lambda f: f.stat().st_mtime, 
            reverse=True
        )[:5]
        
        if recent_files:
            print(f"\n📝 Recent files:")
            for f in recent_files:
                rel_path = f.relative_to(self.workspace_dir)
                size = f.stat().st_size
                print(f"  • {rel_path} ({size} bytes)")
    
    def _show_history(self):
        """Show execution history"""
        print("\n📜 Execution History")
        print("-"*30)
        
        history_file = self.state_dir / "history.md" if self.state_dir else None
        if not history_file or not history_file.exists():
            print("⚠️  No execution history found.")
            return
        
        try:
            history_content = history_file.read_text(encoding='utf-8')
            
            # Extract recent entries (last 1000 chars for readability)
            if len(history_content) > 1000:
                print("... (showing recent history)")
                history_content = "..." + history_content[-1000:]
            
            print(history_content)
            
        except Exception as e:
            print(f"❌ Error reading history: {e}")
    
    def _interactive_clear_workspace(self):
        """Clear workspace interactively with confirmation"""
        print("\n🧹 Clear Workspace")
        print("-"*20)
        
        if not self.workspace_dir.exists():
            print("⚠️  No workspace to clear.")
            return
        
        # Show what will be cleared
        files = list(self.workspace_dir.rglob('*'))
        file_count = len([f for f in files if f.is_file()])
        
        print(f"This will delete {file_count} files in {self.workspace_dir}")
        
        confirm = input("Are you sure? [y/N]: ").strip().lower()
        if confirm in ['y', 'yes']:
            self._clean_workspace()
            print("✅ Workspace cleared successfully.")
        else:
            print("❌ Clear cancelled.")
    
    def _refine_last_goal(self):
        """Refine and re-execute the last goal"""
        print("\n🔄 Refine Last Goal")
        print("-"*20)
        
        # Get last goal from context
        last_goal = None
        if self.context and hasattr(self.context, 'goal'):
            last_goal = self.context.goal
        else:
            # Try to read from variables.json
            try:
                variables_file = self.state_dir / "variables.json"
                if variables_file.exists():
                    variables = json.loads(variables_file.read_text())
                    last_goal = variables.get('goal')
            except:
                pass
        
        if not last_goal:
            print("⚠️  No previous goal found to refine.")
            print("💡 Execute a goal first, then use 'refine' to improve it.")
            return
        
        print(f"Previous goal: {last_goal}")
        print("\nHow would you like to refine this goal?")
        print("Enter your refinement instructions (or press Enter to re-execute as-is):")
        
        refinement = input("🔄 refinement> ").strip()
        
        if refinement:
            # Create refined goal
            refined_goal = f"Refine the previous goal '{last_goal}' with these improvements: {refinement}"
            print(f"\n🚀 Executing refined goal...")
            self.execute(refined_goal, cleanup_after=False)
        else:
            # Re-execute the same goal
            print(f"\n🚀 Re-executing previous goal...")
            self.execute(last_goal, cleanup_after=False)
        
        print(f"\n✅ Refinement completed. Ready for next command.")

def main():
    """Main entry point for the LLM interpreter"""
    import argparse
    
    parser = argparse.ArgumentParser(description='LLMunix LLM Interpreter - Lightweight Runtime')
    parser.add_argument('command', choices=['boot', 'execute', 'interactive'], help='Command to run')
    parser.add_argument('goal', nargs='?', help='Goal to execute (for execute command)')
    parser.add_argument('--model', default='gpt-4o', help='OpenAI model to use')
    parser.add_argument('--interactive', '-i', action='store_true', help='Start interactive session after execution')
    
    args = parser.parse_args()
    
    try:
        interpreter = LLMunixInterpreter(model=args.model)
        
        if args.command == 'boot':
            interpreter.boot()
        elif args.command == 'execute':
            if not args.goal:
                print("❌ Goal required for execute command")
                sys.exit(1)
            interpreter.execute(args.goal)
            
            # Start interactive session if requested
            if args.interactive:
                interpreter.interactive_session()
        elif args.command == 'interactive':
            interpreter.boot()
            interpreter.interactive_session()
            
    except Exception as e:
        print(f"❌ Interpreter failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()


================================================
File: llmunix-llm
================================================
#!/bin/bash

# LLMunix LLM Interpreter CLI Wrapper
# Provides convenient access to the LLM-driven runtime

set -e

# Check if Python is available
if ! command -v python3 &> /dev/null; then
    echo "❌ Python 3 is required but not found"
    exit 1
fi

# Get the directory of this script
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Check if .env file exists
if [ ! -f "$SCRIPT_DIR/.env" ]; then
    echo "⚠️  .env file not found. Create from example.env:"
    echo "   cp example.env .env"
    echo "   # Edit .env with your OpenAI API key"
    exit 1
fi

# Parse command
COMMAND=""
GOAL=""
INTERACTIVE=""

# Handle different argument patterns
if [ "$1" = "boot" ]; then
    COMMAND="boot"
elif [ "$1" = "interactive" ] || [ "$1" = "-i" ] || [ "$1" = "--interactive" ]; then
    COMMAND="interactive"
elif [ "$1" = "execute:" ]; then
    COMMAND="execute"
    GOAL="$2"
    # Check for interactive flag
    if [ "$3" = "-i" ] || [ "$3" = "--interactive" ]; then
        INTERACTIVE="--interactive"
    fi
elif [[ "$1" == execute:* ]]; then
    COMMAND="execute"
    GOAL="${1#execute:}"
    # Remove leading/trailing quotes and whitespace
    GOAL=$(echo "$GOAL" | sed 's/^[[:space:]]*"//; s/"[[:space:]]*$//' | sed "s/^[[:space:]]*'//; s/'[[:space:]]*$//")
    # Check for interactive flag
    if [ "$2" = "-i" ] || [ "$2" = "--interactive" ]; then
        INTERACTIVE="--interactive"
    fi
elif [ "$1" = "execute" ]; then
    COMMAND="execute"
    shift
    # Check for interactive flag at the end
    if [ "${@: -1}" = "-i" ] || [ "${@: -1}" = "--interactive" ]; then
        INTERACTIVE="--interactive"
        # Remove the interactive flag from the goal
        set -- "${@:1:$(($#-1))}"
    fi
    GOAL="$*"
else
    echo "LLMunix LLM Interpreter - Autonomous Markdown Operating System"
    echo ""
    echo "Usage:"
    echo "  $0 boot                                    # Boot LLMunix OS"
    echo "  $0 interactive                             # Start interactive session"
    echo "  $0 execute: \"Your goal here\"               # Execute goal"
    echo "  $0 execute \"Your goal here\"                # Alternative syntax"
    echo "  $0 execute: \"Your goal here\" -i            # Execute and enter interactive mode"
    echo ""
    echo "Interactive Commands (when in interactive mode):"
    echo "  refine       - Refine and re-execute the last goal"
    echo "  status       - Show workspace status"
    echo "  history      - Show execution history"
    echo "  clear        - Clear workspace"
    echo "  help         - Show help"
    echo "  exit/quit    - Exit interactive session"
    echo ""
    echo "Examples:"
    echo "  $0 boot"
    echo "  $0 interactive"
    echo "  $0 execute: \"Create a Python calculator\""
    echo "  $0 execute: \"Fetch content from https://example.com and summarize\" -i"
    echo ""
    exit 1
fi

# Execute command
cd "$SCRIPT_DIR"

if [ "$COMMAND" = "boot" ]; then
    echo "🚀 Booting LLMunix with LLM Interpreter..."
    python3 llm_interpreter.py boot
elif [ "$COMMAND" = "interactive" ]; then
    echo "🎯 Starting LLMunix Interactive Session..."
    python3 llm_interpreter.py interactive
elif [ "$COMMAND" = "execute" ]; then
    if [ -z "$GOAL" ]; then
        echo "❌ Goal required for execute command"
        exit 1
    fi
    echo "🎯 Executing goal with LLM Interpreter..."
    if [ -n "$INTERACTIVE" ]; then
        python3 llm_interpreter.py execute "$GOAL" $INTERACTIVE
    else
        python3 llm_interpreter.py execute "$GOAL"
    fi
fi


================================================
File: requirements_llm.txt
================================================
# LLMunix LLM Interpreter Dependencies
# Required packages for autonomous markdown operating system execution

# Core dependencies
openai>=1.0.0           # OpenAI API client for LLM calls
python-dotenv>=1.0.0    # Environment variable management
requests>=2.31.0        # HTTP requests for web content fetching
pyyaml>=6.0             # YAML parsing for configuration

# Optional dependencies (will be installed in Docker container as needed)
# docker>=6.0.0         # Docker SDK for Python (alternative to CLI calls)
# beautifulsoup4>=4.12.0 # HTML parsing for web content
# markdown>=3.4.0       # Markdown processing utilities


================================================
File: test_dynamic.py
================================================
#!/usr/bin/env python3

"""
Test Script for Dynamic Component Creation

This script tests whether the LLM interpreter can dynamically create 
markdown components during execution based on user goals.
"""

import os
import sys
from pathlib import Path
from llm_interpreter import LLMunixInterpreter

def test_dynamic_component_creation():
    """Test if the interpreter creates components dynamically"""
    
    print("="*60)
    print("🧪 Testing Dynamic Component Creation")
    print("="*60)
    
    # Initialize interpreter
    interpreter = LLMunixInterpreter()
    
    # Clean workspace for fresh test
    interpreter._clean_workspace()
    
    # Check initial state - no sentiment analysis components
    components_dir = Path("components")
    initial_files = list(components_dir.rglob("*.md"))
    print(f"📁 Initial component files: {len(initial_files)}")
    
    # Check SmartLibrary for sentiment capabilities
    smart_lib = Path("system/SmartLibrary.md")
    initial_lib_content = smart_lib.read_text()
    has_sentiment = "sentiment" in initial_lib_content.lower()
    print(f"🔍 Sentiment analysis in SmartLibrary: {has_sentiment}")
    
    if has_sentiment:
        print("⚠️  Warning: Sentiment analysis already exists - test may not show dynamic creation")
    
    # Define the test goal that requires missing capabilities
    test_goal = """Analyze the sentiment and key topics from TechCrunch AI news, then create a report showing whether the coverage is positive or negative about AI developments. I need to understand the emotional tone of the articles."""
    
    print(f"\n🎯 Test Goal: {test_goal}")
    print("\n🚀 Starting execution...")
    
    try:
        # Execute the goal - this should trigger dynamic component creation
        interpreter.execute(test_goal, cleanup_after=False)
        
        print("\n✅ Execution completed!")
        
        # Check what was created
        final_files = list(components_dir.rglob("*.md"))
        new_files = set(final_files) - set(initial_files)
        
        print(f"\n📊 Results:")
        print(f"   Initial files: {len(initial_files)}")
        print(f"   Final files: {len(final_files)}")
        print(f"   New files created: {len(new_files)}")
        
        if new_files:
            print(f"\n📝 New components created:")
            for file in new_files:
                print(f"   ✨ {file}")
                
        # Check SmartLibrary updates
        final_lib_content = smart_lib.read_text()
        lib_changed = len(final_lib_content) != len(initial_lib_content)
        print(f"\n📚 SmartLibrary updated: {lib_changed}")
        
        # Check workspace outputs
        workspace_files = list(Path("workspace").rglob("*"))
        workspace_count = len([f for f in workspace_files if f.is_file()])
        print(f"📂 Workspace files created: {workspace_count}")
        
        # Analyze what was created
        if new_files:
            print(f"\n🔍 Component Analysis:")
            for file in new_files:
                content = file.read_text()
                if "sentiment" in content.lower():
                    print(f"   📊 {file.name}: Contains sentiment analysis logic")
                if "comparative" in content.lower() or "comparison" in content.lower():
                    print(f"   📈 {file.name}: Contains comparative analysis logic")
                if "TOOL" in content:
                    print(f"   🔧 {file.name}: Tool component")
                if "AGENT" in content:
                    print(f"   🤖 {file.name}: Agent component")
        
        return len(new_files) > 0
        
    except Exception as e:
        print(f"\n❌ Execution failed: {e}")
        return False

def check_execution_details():
    """Check the execution details in workspace"""
    print(f"\n🔍 Checking execution details...")
    
    workspace = Path("workspace")
    if not workspace.exists():
        print("   ⚠️  No workspace found")
        return
    
    # Check state files
    state_dir = workspace / "state"
    if state_dir.exists():
        print(f"   📁 State directory: {state_dir}")
        state_files = list(state_dir.glob("*.md")) + list(state_dir.glob("*.json"))
        for state_file in state_files:
            print(f"      📄 {state_file.name}")
    
    # Check if any components were actually created and used
    history_file = state_dir / "history.md" if state_dir.exists() else None
    if history_file and history_file.exists():
        history = history_file.read_text()
        if "SentimentAnalysis" in history or "sentiment" in history.lower():
            print("   ✅ Evidence of sentiment analysis in execution history")
        if "component" in history.lower() and "create" in history.lower():
            print("   ✅ Evidence of component creation in execution history")

if __name__ == "__main__":
    try:
        success = test_dynamic_component_creation()
        check_execution_details()
        
        print(f"\n" + "="*60)
        if success:
            print("🎉 SUCCESS: Dynamic component creation working!")
            print("The LLM interpreter successfully created new markdown components during execution.")
        else:
            print("⚠️  INCONCLUSIVE: No new components detected.")
            print("This could mean:")
            print("   1. The goal was solved with existing components")
            print("   2. Components were created but not detected")
            print("   3. Dynamic creation needs debugging")
        print("="*60)
        
    except KeyboardInterrupt:
        print("\n\n👋 Test interrupted by user")
    except Exception as e:
        print(f"\n❌ Test failed with error: {e}")
        sys.exit(1)


================================================
File: test_dynamic_creation.md
================================================
# Dynamic Component Creation Test

## Goal
"Analyze the sentiment and key topics from 3 different tech news websites, then create a comparative report showing which sources are most positive about AI developments"

## Why This Tests Dynamic Creation

This goal requires capabilities that don't exist in the current SmartLibrary:

1. **Sentiment Analysis** - No existing sentiment analysis tool
2. **Multi-source Comparison** - No existing comparative analysis agent  
3. **Topic Extraction** - Need to extract and compare topics across sources

## Expected Component Creation During Execution

The LLM interpreter should:

1. **Recognize Missing Capabilities**: SystemAgent reads SmartLibrary.md and realizes no sentiment analysis or comparative analysis tools exist

2. **Create SentimentAnalysisTool.md**: Based on the goal requirements, create a tool that can analyze sentiment in text

3. **Create ComparativeAnalysisAgent.md**: Create an agent that can compare multiple sources and generate reports

4. **Update SmartLibrary.md**: Register the new components so they can be used

5. **Execute with New Components**: Use the newly created components to complete the goal

## Test Execution Command

```bash
python3 llm_interpreter.py execute "Analyze the sentiment and key topics from 3 different tech news websites (TechCrunch AI, Reuters Technology, BBC Technology), then create a comparative report showing which sources are most positive about AI developments"
```

## Success Criteria

### Technical Success:
- [ ] SystemAgent identifies missing capabilities by consulting SmartLibrary.md
- [ ] New markdown component files are created in components/ directory
- [ ] SmartLibrary.md is updated with new component entries
- [ ] Components are successfully used in execution

### Functional Success:
- [ ] Fetches content from 3 news sources using existing RealWebFetchTool
- [ ] Analyzes sentiment using newly created SentimentAnalysisTool
- [ ] Generates comparative report using newly created ComparativeAnalysisAgent
- [ ] Produces final report showing sentiment comparison

## What This Proves

This test will prove that:

1. **Pure Markdown Framework**: The system truly operates through markdown specifications without hardcoded logic
2. **Dynamic Component Creation**: New capabilities can be created during runtime based on user goals
3. **Self-Evolving System**: The system can expand its own capabilities as needed
4. **LLM Interpreter Integration**: The interpreter correctly delegates to SystemAgent for component creation

## Expected File Structure After Execution

```
components/
├── tools/
│   ├── [existing tools]
│   └── SentimentAnalysisTool.md        # Created during execution
└── agents/
    ├── [existing agents]  
    └── ComparativeAnalysisAgent.md     # Created during execution

system/
├── SmartLibrary.md                     # Updated with new components
└── [other system files]

workspace/
├── state/                              # Execution state
├── fetched_content/                    # News articles
├── sentiment_analysis/                 # Sentiment results
└── comparative_report.md               # Final output
```

This test validates the core LLMunix principle: everything emerges from Claude interpreting markdown documents, with no hardcoded component logic.


================================================
File: test_llm_interpreter.py
================================================
#!/usr/bin/env python3

"""
Test script for LLMunix LLM Interpreter

This script validates the LLM interpreter functionality with environment
detection and autonomous execution capabilities.
"""

import os
import sys
import time
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

def test_environment_setup():
    """Test environment setup and dependencies"""
    print("🧪 Testing Environment Setup")
    print("-" * 40)
    
    # Check Python version
    python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
    print(f"✅ Python version: {python_version}")
    
    # Check if API key is available
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("❌ OpenAI API key required.")
        print("💡 Set OPENAI_API_KEY in .env file")
        return False
    else:
        print("✅ OpenAI API key found in environment")
    
    # Test required imports
    try:
        from llm_interpreter import LLMunixInterpreter
        print("✅ LLMunix interpreter module imported")
    except ImportError as e:
        print(f"❌ Import failed: {e}")
        return False
    
    # Test OpenAI client
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        print("✅ OpenAI client initialized")
    except Exception as e:
        print(f"❌ OpenAI client failed: {e}")
        return False
    
    return True

def test_interpreter_initialization():
    """Test interpreter initialization"""
    print("\\n🤖 Testing Interpreter Initialization")
    print("-" * 40)
    
    try:
        from llm_interpreter import LLMunixInterpreter
        
        # Test initialization
        interpreter = LLMunixInterpreter(model="gpt-4o")
        print("✅ Interpreter initialized successfully")
        
        # Check properties
        print(f"✅ Model: {interpreter.model}")
        print(f"✅ Workspace: {interpreter.workspace_dir}")
        print(f"✅ State dir: {interpreter.state_dir}")
        
        return True
        
    except Exception as e:
        print(f"❌ Initialization failed: {e}")
        return False

def test_docker_availability():
    """Test Docker availability for tool execution"""
    print("\\n🐳 Testing Docker Availability")
    print("-" * 40)
    
    import subprocess
    
    try:
        result = subprocess.run(['docker', '--version'], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print(f"✅ Docker available: {result.stdout.strip()}")
            
            # Test container creation capability
            try:
                test_result = subprocess.run([
                    'docker', 'run', '--rm', 'alpine:latest', 'echo', 'test'
                ], capture_output=True, text=True, timeout=30)
                
                if test_result.returncode == 0:
                    print("✅ Docker container execution works")
                    return True
                else:
                    print(f"⚠️  Docker available but container test failed: {test_result.stderr}")
                    return False
                    
            except Exception as e:
                print(f"⚠️  Docker available but test failed: {e}")
                return False
                
        else:
            print("⚠️  Docker not available - will run in simulation mode")
            return False
            
    except FileNotFoundError:
        print("⚠️  Docker not found - will run in simulation mode")
        return False
    except Exception as e:
        print(f"⚠️  Docker test failed: {e}")
        return False

def test_simple_execution():
    """Test simple goal execution"""
    print("\\n🎯 Testing Simple Execution")
    print("-" * 40)
    
    try:
        from llm_interpreter import LLMunixInterpreter
        
        interpreter = LLMunixInterpreter(model="gpt-4o")
        
        # Test simple goal
        test_goal = "Create a simple text file called hello.txt with the content 'Hello LLMunix!'"
        
        print(f"🎯 Test goal: {test_goal}")
        print("⏳ Starting execution (this may take a moment)...")
        
        # Execute goal
        interpreter.execute(test_goal)
        
        # Check if workspace was created
        if interpreter.workspace_dir.exists():
            print("✅ Workspace created")
            
            # Check state files
            state_files = ['plan.md', 'history.md', 'context.md', 'constraints.md', 'variables.json']
            for state_file in state_files:
                file_path = interpreter.state_dir / state_file
                if file_path.exists():
                    print(f"✅ State file created: {state_file}")
                else:
                    print(f"⚠️  State file missing: {state_file}")
            
            # Check for execution artifacts
            hello_file = interpreter.workspace_dir / "hello.txt"
            if hello_file.exists():
                content = hello_file.read_text()
                print(f"✅ Output file created: hello.txt")
                print(f"📄 Content: {content}")
            else:
                print("⚠️  Expected output file not found")
            
            return True
        else:
            print("❌ Workspace not created")
            return False
            
    except Exception as e:
        print(f"❌ Execution test failed: {e}")
        return False

def test_environment_detection():
    """Test environment detection capabilities"""
    print("\\n🔍 Testing Environment Detection")
    print("-" * 40)
    
    # This test only works if Docker is available
    if not test_docker_availability():
        print("⚠️  Skipping environment detection test (Docker not available)")
        return True
    
    try:
        from llm_interpreter import LLMunixInterpreter
        
        interpreter = LLMunixInterpreter(model="gpt-4o")
        
        # Setup Docker environment
        interpreter._setup_docker()
        
        if hasattr(interpreter, 'container_name') and interpreter.container_name:
            print(f"✅ Container created: {interpreter.container_name}")
            
            # Test environment detection
            interpreter._detect_container_environment()
            
            # Check environment info
            if hasattr(interpreter, 'context') and interpreter.context and interpreter.context.environment_info:
                env_info = interpreter.context.environment_info
                print(f"✅ Tools detected: {len(env_info.get('available_tools', []))}")
                print(f"✅ Package managers: {env_info.get('package_managers', [])}")
                print(f"✅ OS detected: {env_info.get('distro', 'Unknown')}")
                
                # Cleanup
                interpreter._cleanup_execution()
                return True
            else:
                print("❌ Environment detection failed")
                return False
        else:
            print("❌ Container creation failed")
            return False
            
    except Exception as e:
        print(f"❌ Environment detection test failed: {e}")
        return False

def main():
    """Run all tests"""
    print("🧪 LLMunix LLM Interpreter Test Suite")
    print("=" * 50)
    
    tests = [
        ("Environment Setup", test_environment_setup),
        ("Interpreter Initialization", test_interpreter_initialization),
        ("Docker Availability", test_docker_availability),
        ("Environment Detection", test_environment_detection),
        ("Simple Execution", test_simple_execution),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"❌ Test '{test_name}' failed with exception: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\\n" + "=" * 50)
    print("📊 Test Results Summary")
    print("=" * 50)
    
    passed = 0
    total = len(results)
    
    for test_name, success in results:
        status = "✅ PASS" if success else "❌ FAIL"
        print(f"{status} {test_name}")
        if success:
            passed += 1
    
    print(f"\\n🎯 Tests passed: {passed}/{total}")
    
    if passed == total:
        print("🎉 All tests passed! LLM interpreter is ready.")
        return 0
    else:
        print("⚠️  Some tests failed. Check the output above.")
        return 1

if __name__ == "__main__":
    sys.exit(main())


================================================
File: components/.DS_Store
================================================
[Non-text file]


================================================
File: components/agents/MemoryAnalysisAgent.md
================================================
# Memory Analysis Agent

**Component Type**: Agent  
**Version**: v1  
**Status**: [REAL] - Production Ready  
**Claude Tool Mapping**: Read, Grep, Bash  

## Purpose

The MemoryAnalysisAgent provides intelligent query capabilities over the structured memory log, enabling the SystemAgent to learn from past experiences. It parses the YAML frontmatter and markdown content of memory entries to synthesize insights and answer specific questions about historical task executions.

## Core Capabilities

### Memory Querying
- Parse and filter memory entries based on structured criteria
- Perform semantic searches across qualitative learnings
- Aggregate patterns across multiple experiences
- Identify trends in user sentiment and satisfaction

### Insight Synthesis
- Generate summaries of past performance for specific task types
- Identify common failure patterns and successful strategies
- Recommend behavioral adaptations based on historical outcomes
- Provide evidence-based suggestions for constraint modifications

### Pattern Recognition
- Detect recurring issues across similar tasks
- Identify successful component combinations
- Track evolution of user preferences and satisfaction patterns
- Analyze cost and performance trends

## Input Specification

```yaml
query: string  # Natural language question about memory
filters:       # Optional structured filters
  outcome: "success" | "failure" | "success_with_recovery"
  tags: [list of tags]
  date_range:
    start: "ISO timestamp"
    end: "ISO timestamp"
  sentiment: "neutral" | "positive" | "frustrated" | "pleased" | "impressed"
  min_cost: number
  max_cost: number
  components_used: [list of component names]
  error_threshold: number  # minimum error count
context: string  # Optional context about current task for relevance
```

## Output Specification

```yaml
analysis_summary: string     # High-level answer to the query
relevant_experiences: []     # List of matching experience IDs
key_insights: []            # Bullet points of main findings
recommendations: []         # Actionable suggestions based on analysis
confidence_score: number    # 0-100 confidence in the analysis
data_sources:              # Metadata about the analysis
  experiences_analyzed: number
  date_range_covered: string
  primary_patterns: []
behavioral_suggestions:     # Specific constraint adaptations
  sentiment_adaptations: {}
  priority_recommendations: {}
  persona_suggestions: {}
```

## Execution Logic

### Phase 1: Memory Parsing
1. **Load Memory Log**: Read `system/memory_log.md`
2. **Parse Entries**: Split file into individual experience blocks
3. **Extract Metadata**: Parse YAML frontmatter for structured data
4. **Index Content**: Create searchable index of qualitative content

### Phase 2: Query Processing
1. **Parse Query**: Understand the natural language question
2. **Apply Filters**: Filter experiences based on structured criteria
3. **Semantic Search**: Search qualitative content for relevant insights
4. **Relevance Scoring**: Score each experience for query relevance

### Phase 3: Analysis Synthesis
1. **Pattern Detection**: Identify common themes across filtered experiences
2. **Trend Analysis**: Analyze patterns over time
3. **Insight Generation**: Synthesize key learnings from patterns
4. **Recommendation Formulation**: Generate actionable suggestions

### Phase 4: Response Generation
1. **Structure Output**: Format analysis according to output specification
2. **Confidence Assessment**: Calculate confidence based on data quality and quantity
3. **Behavioral Mapping**: Translate insights into specific constraint recommendations

## Claude Tool Mapping

### Primary Tools
- **Read**: Load memory log file and parse content
- **Grep**: Search for specific patterns across memory entries
- **Bash**: Use text processing tools for complex parsing when needed

### Implementation Pattern
```markdown
Action: Read system/memory_log.md
Observation: [Full memory log content]

Action: [Apply query filters and search logic using Grep/Bash]
Observation: [Filtered and relevant memory entries]

Action: [Synthesize insights and generate recommendations]
Observation: [Structured analysis output]
```

## Example Queries and Responses

### Query: "What causes legal analysis tasks to fail?"
```yaml
query: "What causes legal analysis tasks to fail?"
filters:
  tags: ["legal-analysis"]
  outcome: "failure"
```

**Response:**
```yaml
analysis_summary: "Legal analysis failures typically stem from missing specialized tools and insufficient domain knowledge validation."
relevant_experiences: ["exp_008_contract_failure", "exp_012_compliance_error"]
key_insights:
  - "Missing RiskDetectorTool was primary failure cause in 2/3 legal tasks"
  - "Standard SummarizationAgent lacks domain-specific legal pattern recognition"
  - "User frustration increases when legal nuances are missed"
recommendations:
  - "Always create RiskDetectorTool for legal tasks"
  - "Implement legal domain validation before proceeding"
  - "Set human_review_trigger_level to 'low' for legal tasks"
confidence_score: 85
behavioral_suggestions:
  priority_recommendations:
    legal_tasks: "quality"
  persona_suggestions:
    legal_tasks: "detailed_analyst"
```

### Query: "How does user sentiment affect task outcomes?"
```yaml
query: "How does user sentiment affect task outcomes?"
```

**Response:**
```yaml
analysis_summary: "Tasks executed when user sentiment is 'frustrated' have 40% higher failure rates and 2x cost overruns compared to 'pleased' sentiment."
key_insights:
  - "Frustrated users tend to provide less complete requirements"
  - "Positive sentiment correlates with higher user satisfaction scores"
  - "Sentiment adaptation during execution improves outcomes by 60%"
recommendations:
  - "Implement proactive sentiment monitoring"
  - "Adapt execution style based on detected sentiment"
  - "Prioritize speed and clarity when frustration detected"
confidence_score: 92
behavioral_suggestions:
  sentiment_adaptations:
    frustrated: 
      priority: "speed_and_clarity"
      human_review_trigger_level: "low"
    pleased:
      priority: "comprehensiveness"
      active_persona: "proactive_collaborator"
```

## Integration with SystemAgent

The MemoryAnalysisAgent is called by the SystemAgent during the planning phase through the QueryMemoryTool. It provides historical context that directly influences:

1. **Plan Generation**: Avoiding past mistakes and replicating successful patterns
2. **Constraint Setting**: Adapting behavioral modifiers based on similar tasks
3. **Component Selection**: Choosing components with proven success records
4. **Error Prevention**: Proactively addressing known failure modes

## Performance Characteristics

- **Latency**: ~2-5 seconds for typical queries
- **Cost**: $0.01-0.03 per query (depending on memory log size)
- **Accuracy**: 85-95% relevance score for well-structured queries
- **Scalability**: Handles 1000+ memory entries efficiently

## Error Handling

- **Missing Memory Log**: Returns empty analysis with clear error message
- **Malformed Entries**: Skips corrupted entries and reports count
- **Invalid Filters**: Ignores invalid criteria and processes valid ones
- **Empty Results**: Provides guidance on query refinement

## Future Enhancements

- **Vector Search**: Implement semantic similarity for better content matching
- **Predictive Analytics**: Forecast task outcomes based on historical patterns
- **Automated Insights**: Generate proactive recommendations without queries
- **Cross-Task Learning**: Identify transferable patterns across task domains


================================================
File: components/agents/RealSummarizationAgent.md
================================================
# Component: RealSummarizationAgent

- **Name**: RealSummarizationAgent
- **Type**: AGENT  
- **Description**: Advanced summarization agent that reads real files and generates structured summaries using Claude Code's native capabilities.

## Inputs

- `input_source` (string): Source type - "file", "url", "text"
- `input_path` (string): File path or URL to summarize
- `input_text` (string): Direct text input (if source is "text")
- `output_format` (string): "json", "markdown", "plain" (default: "json")
- `summary_length` (string): "brief", "detailed", "executive" (default: "brief")
- `focus_areas` (array): Specific aspects to emphasize

## Outputs

- `summary` (object/string): Generated summary in requested format
- `key_points` (array): Main points extracted
- `metadata` (object): Analysis metadata (word count, sentiment, etc.)
- `confidence` (number): Confidence score 0-1

## Tool Dependencies

- **RealFileSystemTool**: For reading input files
- **RealWebFetchTool**: For URL-based inputs
- Built-in LLM reasoning: For analysis and summarization

## Logic

### EXECUTION MODE:

1. **Input Processing**:
   - If input_source is "file": Use RealFileSystemTool to read content
   - If input_source is "url": Use RealWebFetchTool to fetch content  
   - If input_source is "text": Use input_text directly

2. **Content Analysis**:
   - Analyze content length and complexity
   - Identify key themes and topics
   - Extract factual information and opinions
   - Assess content structure and organization

3. **Summary Generation**:
   - Generate summary based on length preference
   - Structure output according to format specification
   - Include focus areas if specified
   - Calculate confidence score

4. **Output Formatting**:
   - Format according to output_format
   - Include metadata and analysis details
   - Validate output structure

### SIMULATION MODE:
1. Simulate content reading from various sources
2. Generate realistic summary outputs
3. Create training data for multi-modal summarization

## Output Formats

### JSON Format
```json
{
  "title": "Content Title",
  "summary": "Concise summary text...",
  "key_points": [
    "Key point 1",
    "Key point 2", 
    "Key point 3"
  ],
  "metadata": {
    "word_count": 1500,
    "reading_time": "6 min",
    "sentiment": "neutral",
    "topics": ["business", "technology"]
  },
  "confidence": 0.89
}
```

### Markdown Format
```markdown
# Content Title

## Summary
Concise summary text...

## Key Points
- Key point 1
- Key point 2
- Key point 3

## Metadata
- Word Count: 1500
- Reading Time: 6 min
- Confidence: 89%
```

### Plain Format
```
Title: Content Title

Summary: Concise summary text...

Key Points:
1. Key point 1
2. Key point 2  
3. Key point 3
```

## Usage Examples

**Summarize web article:**
```
Input: {
  input_source: "url",
  input_path: "https://example.com/article",
  output_format: "json",
  summary_length: "brief"
}
Output: {JSON summary object}
```

**Summarize local file:**
```
Input: {
  input_source: "file", 
  input_path: "workspace/document.txt",
  output_format: "markdown",
  summary_length: "detailed"
}
Output: {Markdown formatted summary}
```

## Quality Metrics

- **Completeness**: All key information captured
- **Accuracy**: Factual correctness maintained
- **Conciseness**: Appropriate length for summary_length
- **Clarity**: Clear, readable language
- **Structure**: Well-organized output

## Error Handling

- **Content Too Long**: Chunk processing for large documents
- **Content Unavailable**: Clear error with retry suggestions
- **Format Invalid**: Default to plain text with warning
- **Low Confidence**: Flag for human review

## Performance Characteristics

```yaml
cost: medium (depends on content length)
latency: medium (5-30 seconds)
side_effects: "May create intermediate analysis files"
error_modes: ["content_unavailable", "format_error", "analysis_timeout"]
quality_metrics: ["accuracy", "completeness", "clarity"]
```

## Training Data Format

```json
{
  "agent_execution": {
    "agent": "RealSummarizationAgent",
    "inputs": {...},
    "tool_calls": [
      {"tool": "Read", "file": "input.txt"},
      {"tool": "Write", "file": "summary.json"}
    ],
    "outputs": {...},
    "performance": {"quality_score": 0.89, "time": "12s"}
  }
}
```


================================================
File: components/agents/SimulatedFineTunedAgent.md
================================================
# Component: SimulatedFineTunedAgent

- **Name**: SimulatedFineTunedAgent
- **Type**: AGENT
- **Description**: Simulates how a fine-tuned LLM would operate as an autonomous state machine, making external tool calls when needed to complete agentic workflows.

## Purpose

This agent demonstrates the target behavior for an LLM fine-tuned on LLM-OS execution traces. It shows how the trained model would:
1. Analyze current context and goals
2. Identify information gaps requiring external data
3. Request specific tool calls with structured parameters
4. Integrate tool results into workflow execution
5. Continue until goal completion

## Inputs

- `current_state` (object): Current execution state from execution_state.md
- `goal` (string): High-level objective to achieve
- `available_context` (string): Information currently available
- `available_tools` (array): List of tools that can be called externally

## Outputs

- `action_type` (string): "TOOL_REQUEST" or "PROCESSING_RESULT"
- `reasoning` (string): Explanation of decision-making process
- `tool_request` (object): Structured call for external tool (if needed)
- `processing_result` (object): Direct output (if sufficient context)
- `next_state` (object): Updated execution state

## Decision Logic

### Pattern Matching from Training Data

The agent uses patterns learned from LLM-OS execution traces to make decisions:

#### Pattern 1: Information Gap Detection
```
IF goal requires current/live data AND context lacks this data
THEN request WebFetch tool call
```

#### Pattern 2: File Operation Needs
```
IF workflow requires reading/writing files AND files not in context
THEN request Read/Write tool calls
```

#### Pattern 3: Complex Processing Requirements
```
IF task requires specialized analysis AND beyond current capabilities
THEN request Task tool for sub-agent processing
```

#### Pattern 4: State Persistence
```
IF execution state changes AND persistence required
THEN request Write tool for state management
```

## Simulation Modes

### Interactive Mode
```yaml
mode: interactive
behavior: 
  - Shows reasoning at each decision point
  - Requests human confirmation for tool calls
  - Explains why external data is needed
  - Demonstrates state machine transitions
```

### Autonomous Mode
```yaml
mode: autonomous
behavior:
  - Makes decisions automatically based on training patterns
  - Generates tool requests without human intervention
  - Processes results and continues workflow
  - Mimics fine-tuned model behavior
```

### Validation Mode
```yaml
mode: validation
behavior:
  - Compares decisions to training data examples
  - Measures accuracy of tool request predictions
  - Validates reasoning patterns
  - Reports simulation fidelity metrics
```

## Tool Request Generation

When the agent identifies an information gap, it generates structured tool requests:

### WebFetch Request Example
```json
{
  "action": "TOOL_REQUEST",
  "tool_name": "WebFetch",
  "reasoning": "Goal requires current AI developments from Hugging Face blog, but no recent data in context",
  "parameters": {
    "url": "https://huggingface.co/blog",
    "prompt": "Extract recent AI research and model announcements"
  },
  "expected_outcome": "Live blog content with current AI developments for analysis",
  "next_state": {
    "step": 2,
    "variables": {
      "raw_content": "workspace/huggingface_content.txt"
    }
  }
}
```

### Read Request Example
```json
{
  "action": "TOOL_REQUEST", 
  "tool_name": "Read",
  "reasoning": "Need to process previously fetched content for analysis step",
  "parameters": {
    "file_path": "workspace/huggingface_content.txt"
  },
  "expected_outcome": "Blog content loaded into context for summarization",
  "next_state": {
    "step": 3,
    "context_enhanced": true
  }
}
```

## Processing Result Generation

When sufficient context is available, the agent produces direct results:

### Analysis Result Example
```json
{
  "action": "PROCESSING_RESULT",
  "content": {
    "summary": "Analysis of Hugging Face blog content...",
    "key_insights": ["insight1", "insight2", "insight3"],
    "confidence": 0.94
  },
  "state_update": {
    "current_step": 4,
    "variables": {
      "analysis_complete": true,
      "summary_path": "workspace/analysis.json"
    }
  },
  "completion_status": "CONTINUE"
}
```

## Context Integration Logic

The agent demonstrates how a fine-tuned model would integrate external tool results:

1. **Result Validation**: Check if tool output meets expected criteria
2. **Context Enhancement**: Add new information to working context
3. **State Progression**: Update execution state with new variables
4. **Next Action Planning**: Determine subsequent steps based on enhanced context

## Training Pattern Examples

The simulation is based on these real patterns from LLM-OS training data:

### Web Research Pattern
```
Context: Need current AI news
Gap: No live data available
Tool Request: WebFetch(url, prompt)
Result Integration: Raw content → Context
Next Action: Analyze content
```

### Analysis Pattern
```
Context: Have raw content, need insights
Gap: Unprocessed information
Processing: Generate structured analysis
Result: Summary with confidence scores
Next Action: Generate report
```

### Persistence Pattern
```
Context: Have results, need to save
Gap: Results not persisted
Tool Request: Write(file_path, content)
Result Integration: Confirmation → Context
Next Action: Update state as complete
```

## Success Metrics

The simulation tracks these metrics to validate fine-tuned model behavior:

- **Decision Accuracy**: Percentage of correct tool selection decisions
- **Request Quality**: Well-formed tool requests with proper parameters
- **Context Utilization**: Effective use of available information
- **Goal Achievement**: Successful workflow completion rates
- **State Management**: Proper execution state transitions

## Usage in Training Pipeline

This agent serves as:
1. **Target Behavior Model**: Shows desired fine-tuned LLM capabilities
2. **Validation Tool**: Tests training data quality and completeness
3. **Simulation Engine**: Demonstrates autonomous agent operation
4. **Training Generator**: Creates additional training examples through simulation


================================================
File: components/agents/SummarizationAgent.md
================================================
# Component: SummarizationAgent

-   **Name**: SummarizationAgent
-   **Type**: AGENT
-   **Description**: Analyzes a given text document and produces a concise, structured summary.

### Inputs

-   `input_file_path` (string): The path to a text file within the `workspace/` directory that contains the content to be summarized.

### Outputs

-   `output_file_path` (string): The path within the `workspace/` directory where the generated summary will be saved.

### Logic

1.  Read the full content from the `input_file_path`.
2.  Analyze the content to identify the main topic, key points, and overall message.
3.  Generate a summary that is approximately 25% of the original length. The summary should be well-written and capture the essential information.
4.  Write the generated summary text to the `output_file_path`.


================================================
File: components/agents/SummarizationAgent_v2.md
================================================
# Component: SummarizationAgent_v2

-   **Name**: SummarizationAgent_v2
-   **Type**: AGENT
-   **Description**: Analyzes a given text document and produces a structured JSON summary with separate title and summary fields.

### Inputs

-   `input_file_path` (string): The path to a text file within the `workspace/` directory that contains the content to be summarized.

### Outputs

-   `output_file_path` (string): The path within the `workspace/` directory where the generated JSON summary will be saved.

### Logic

1.  Read the full content from the `input_file_path`.
2.  Analyze the content to identify the main topic, key points, and overall message.
3.  Generate a descriptive title (5-8 words) that captures the essence of the content.
4.  Create a concise summary that is approximately 25% of the original length, capturing essential information.
5.  Format the output as valid JSON with this structure:
    ```json
    {
      "title": "Brief descriptive title (5-8 words)",
      "summary": "Concise summary paragraph with key information"
    }
    ```
6.  Write the JSON output to the `output_file_path`.

### Improvements over v1

- **Structured Output**: JSON format instead of plain text for better integration
- **Title Generation**: Automatic title creation for content categorization
- **Standardized Format**: Consistent schema for downstream processing
- **Enhanced Compatibility**: JSON output works with modern data pipelines


================================================
File: components/tools/FileWriterTool.md
================================================
# Component: FileWriterTool

-   **Name**: FileWriterTool
-   **Type**: TOOL
-   **Description**: Writes given text content to a specified file path.

### Inputs

-   `content_file_path` (string): The path to a file within the `workspace/` directory containing the content to be written.
-   `output_file_path` (string): The final destination path for the file (relative to the project root).

### Outputs

-   A new file created at `output_file_path` with the specified content.

### Logic

1.  Read the content from `content_file_path`.
2.  Write that exact content to a new file at `output_file_path`.
3.  This tool is for creating the final, user-facing output file.


================================================
File: components/tools/LLMInterpreterWebFetchTool.md
================================================
# Component: LLMInterpreterWebFetchTool

- **Name**: LLMInterpreterWebFetchTool
- **Type**: TOOL
- **Runtime**: LLM Interpreter
- **Description**: Fetches real web content using standard command-line tools (curl/wget) for LLM Interpreter runtime.

## Inputs

- `url` (string): The URL to fetch content from
- `prompt` (string): Instructions for content extraction (default: "Extract the main text content")

## Outputs

- `content` (string): The extracted content from the webpage
- `summary` (string): Brief summary of what was fetched
- `metadata` (object): Information about the fetch operation

## Command Mapping

```yaml
base_command: curl
parameters:
  url: {{url}}
  options: ["-L", "--user-agent", "Mozilla/5.0", "-s"]
cost: none (uses local networking)
latency: medium (2-10 seconds)
side_effects: "Network request to external server"
error_modes: ["timeout", "404", "403", "connection_error", "ssl_error"]
```

## Logic

### Web Content Fetching:
1. **Primary Method**: Use curl with follow redirects and user agent
   ```bash
   curl -L --user-agent "Mozilla/5.0 (compatible; LLMunix)" -s "$URL"
   ```

2. **Fallback Method**: Use wget if curl unavailable
   ```bash
   wget -q -O - --user-agent="Mozilla/5.0 (compatible; LLMunix)" "$URL"
   ```

3. **Text Extraction**: Basic HTML-to-text conversion using sed/awk:
   - Remove HTML tags: `sed 's/<[^>]*>//g'`
   - Clean whitespace: `sed 's/&nbsp;/ /g; s/&amp;/\&/g; s/&lt;/</g; s/&gt;/>/g'`
   - Extract paragraphs and headers only

4. **Content Processing**: Apply prompt instructions to filter content:
   - If prompt contains "title": Extract content between `<title>` tags
   - If prompt contains "article": Focus on `<article>`, `<main>`, or large text blocks
   - If prompt contains "summary": Return first few paragraphs

## Implementation Template

```bash
#!/bin/bash
# LLM Interpreter WebFetch Implementation

URL="$1"
PROMPT="$2"

# Check if curl is available, fallback to wget
if command -v curl >/dev/null 2>&1; then
    FETCHER="curl -L --user-agent 'Mozilla/5.0 (compatible; LLMunix)' -s"
elif command -v wget >/dev/null 2>&1; then
    FETCHER="wget -q -O - --user-agent='Mozilla/5.0 (compatible; LLMunix)'"
else
    echo "Error: Neither curl nor wget available for web fetching"
    exit 1
fi

# Fetch content
echo "Fetching content from: $URL"
RAW_CONTENT=$($FETCHER "$URL" 2>/dev/null)

if [ $? -ne 0 ]; then
    echo "Error: Failed to fetch content from $URL"
    exit 1
fi

# Basic HTML-to-text conversion
CLEAN_CONTENT=$(echo "$RAW_CONTENT" | \
    sed 's/<script[^>]*>.*<\/script>//g' | \
    sed 's/<style[^>]*>.*<\/style>//g' | \
    sed 's/<[^>]*>//g' | \
    sed 's/&nbsp;/ /g; s/&amp;/\&/g; s/&lt;/</g; s/&gt;/>/g' | \
    sed '/^[[:space:]]*$/d' | \
    head -100)

# Apply prompt-based filtering
if echo "$PROMPT" | grep -qi "title"; then
    TITLE=$(echo "$RAW_CONTENT" | sed -n 's/.*<title>\(.*\)<\/title>.*/\1/p' | head -1)
    echo "Title: $TITLE"
fi

echo "Content extracted successfully"
echo "Word count: $(echo "$CLEAN_CONTENT" | wc -w)"
echo ""
echo "=== CONTENT ==="
echo "$CLEAN_CONTENT"
```

## Usage Examples

**Simple webpage fetch:**
```bash
# LLM Interpreter will execute:
bash webfetch.sh "https://example.com" "Get the main content"
```

**Article extraction:**
```bash
# LLM Interpreter will execute:
bash webfetch.sh "https://blog.com/article" "Extract article title and body"
```

## Error Handling

- **Command Not Found**: Check for curl, then wget, provide clear error if neither available
- **Network Error**: Return network connectivity guidance
- **Timeout**: Use curl/wget timeout options (30s default)
- **SSL Issues**: Add --insecure flag as fallback option
- **Rate Limiting**: Detect rate limit responses and suggest retry delays

## Integration with SystemAgent

The SystemAgent should detect runtime environment and use appropriate tool:

```markdown
**Web Fetch Tool Selection Logic:**
- If running in Claude Code runtime → Use RealWebFetchTool (WebFetch tool)
- If running in LLM Interpreter runtime → Use LLMInterpreterWebFetchTool (curl/wget commands)
- If neither available → Use simulation mode with mock content
```

## Training Data Format

```json
{
  "tool_call": {
    "tool": "curl",
    "command": "curl -L --user-agent 'Mozilla/5.0' -s 'https://example.com'",
    "inputs": {"url": "https://example.com", "prompt": "Extract content"},
    "outputs": {"content": "...", "metadata": {"status": "200", "words": 542}},
    "performance": {"time": "2.1s", "success": true, "method": "curl"}
  }
}
```

## Security Considerations

- Use proper user agent to avoid being blocked
- Implement reasonable timeouts to prevent hanging
- Sanitize URLs to prevent command injection
- No automatic following of suspicious redirects
- Basic rate limiting respect (delays between requests)


================================================
File: components/tools/QueryMemoryTool.md
================================================
# Query Memory Tool

**Component Type**: Tool  
**Version**: v1  
**Status**: [REAL] - Production Ready  
**Claude Tool Mapping**: Read, Grep, Bash  

## Purpose

The QueryMemoryTool serves as the bridge between the SystemAgent and the MemoryAnalysisAgent, providing a standardized interface for memory consultation during task planning and execution. It enables the SystemAgent to leverage historical experiences for improved decision-making and adaptive behavior.

## Core Functionality

### Memory Consultation Interface
- Provides simple, standardized interface for memory queries
- Handles query formatting and response parsing
- Integrates seamlessly into SystemAgent workflow
- Supports both structured and natural language queries

### Query Optimization
- Automatically suggests relevant filters based on current task context
- Optimizes query parameters for faster response times
- Caches frequent queries to reduce computational overhead
- Provides query refinement suggestions for empty results

### Context Integration
- Automatically includes current task context in memory queries
- Maps current constraints to historical constraint patterns
- Identifies relevant experiences based on goal similarity
- Provides task-specific memory consultation

## Input Specification

```yaml
query: string               # Natural language question about past experiences
context:                   # Current task context for relevance
  goal: string             # Current task goal
  task_type: string        # Type of task (legal, research, creative, etc.)
  constraints: {}          # Current constraint settings
  components_considered: []# Components being considered for current task
filters:                   # Optional filters (auto-generated if not provided)
  outcome: string
  tags: []
  date_range: {}
  sentiment: string
  cost_range: {}
  error_threshold: number
options:                   # Query behavior options
  max_experiences: number  # Limit number of experiences to analyze
  include_failures: boolean# Whether to include failed experiences
  priority_focus: string   # "recent", "successful", "similar", "all"
  confidence_threshold: number # Minimum confidence for recommendations
```

## Output Specification

```yaml
query_id: string           # Unique identifier for this query
memory_analysis:           # Response from MemoryAnalysisAgent
  analysis_summary: string
  relevant_experiences: []
  key_insights: []
  recommendations: []
  confidence_score: number
  behavioral_suggestions: {}
actionable_insights:       # Processed for immediate use
  constraint_adjustments: {}# Specific constraint modifications
  component_recommendations: []# Recommended components based on history
  risk_warnings: []        # Potential issues to avoid
  success_patterns: []     # Patterns to replicate
query_metadata:           # Information about the query execution
  experiences_found: number
  processing_time: number
  cost_estimate: number
  cache_hit: boolean
recommendations:          # Next steps for SystemAgent
  apply_constraints: {}    # Constraints to update
  create_components: []    # New components to create
  consultation_follow_ups: []# Additional queries to consider
```

## Execution Logic

### Phase 1: Query Preparation
1. **Context Analysis**: Analyze current task goal and constraints
2. **Filter Generation**: Auto-generate relevant filters if not provided
3. **Query Optimization**: Optimize query for performance and relevance
4. **Cache Check**: Check if similar query has been cached recently

### Phase 2: Memory Analysis Execution
1. **Format Query**: Convert input to MemoryAnalysisAgent format
2. **Execute Analysis**: Call MemoryAnalysisAgent with formatted query
3. **Parse Response**: Extract insights and recommendations from analysis
4. **Validate Results**: Ensure response quality and relevance

### Phase 3: Response Processing
1. **Insight Translation**: Convert insights to actionable SystemAgent guidance
2. **Constraint Mapping**: Map historical patterns to current constraint options
3. **Risk Assessment**: Identify potential issues based on past failures
4. **Success Pattern Extraction**: Identify replicable success patterns

### Phase 4: Output Generation
1. **Format Response**: Structure output for SystemAgent consumption
2. **Generate Recommendations**: Create specific action recommendations
3. **Cache Results**: Store results for potential reuse
4. **Log Query**: Record query for future optimization

## Claude Tool Mapping

### Implementation Pattern
```markdown
Action: Read system/memory_log.md
Observation: [Memory log content for context awareness]

Action: [Execute MemoryAnalysisAgent logic using Read/Grep/Bash]
Observation: [Raw memory analysis results]

Action: [Process and format results for SystemAgent]
Observation: [Structured, actionable insights ready for use]
```

### Tool Usage Strategy
- **Read**: Load memory log and extract relevant entries
- **Grep**: Search for specific patterns and filter experiences
- **Bash**: Use advanced text processing for complex analysis when needed

## Example Usage Scenarios

### Scenario 1: Planning Legal Analysis Task
```yaml
# Input
query: "How should I approach legal document analysis?"
context:
  goal: "Analyze vendor contract for compliance risks"
  task_type: "legal"
  constraints:
    user_sentiment: "neutral"
    priority: "quality"
```

```yaml
# Output
actionable_insights:
  constraint_adjustments:
    error_tolerance: "strict"
    human_review_trigger_level: "low"
    active_persona: "detailed_analyst"
  component_recommendations:
    - "RiskDetectorTool"
    - "ComplianceAnalysisAgent"
  risk_warnings:
    - "Previous failures due to missing domain-specific tools"
    - "Standard agents lack legal pattern recognition"
  success_patterns:
    - "Create specialized tools before starting analysis"
    - "Use detailed analyst persona for legal tasks"
```

### Scenario 2: Sentiment-Based Adaptation
```yaml
# Input
query: "User seems frustrated with slow progress. How should I adapt?"
context:
  goal: "Generate marketing content"
  constraints:
    user_sentiment: "frustrated"
    priority: "comprehensiveness"
```

```yaml
# Output
actionable_insights:
  constraint_adjustments:
    priority: "speed_and_clarity"
    active_persona: "concise_assistant"
    human_review_trigger_level: "low"
  success_patterns:
    - "Switch to speed-focused execution when frustration detected"
    - "Provide frequent progress updates"
    - "Offer intermediate deliverables"
```

### Scenario 3: Component Selection Guidance
```yaml
# Input
query: "What components work best for research tasks?"
context:
  goal: "Research AI trends and generate report"
  task_type: "research"
```

```yaml
# Output
component_recommendations:
  - "RealWebFetchTool" # 95% success rate for research
  - "RealSummarizationAgent" # High quality analysis scores
  - "ResearchAnalysisAgent" # Specialized for trend identification
success_patterns:
  - "Multi-source research with parallel fetching"
  - "Structured output with confidence metrics"
  - "Comprehensive analysis followed by executive summary"
```

## Integration with SystemAgent

The QueryMemoryTool is used in these SystemAgent phases:

### Planning Phase Integration
```markdown
## Planning Phase (Enhanced)
1. Parse user goal and constraints
2. **Query Memory**: Use QueryMemoryTool to find relevant past experiences
3. **Apply Insights**: Incorporate memory insights into plan generation
4. **Adjust Constraints**: Update constraints.md based on recommendations
5. Generate execution plan with historical context
```

### Mid-Execution Consultation
```markdown
## Error Recovery (Enhanced)
1. Detect execution issue
2. **Query Memory**: "How were similar errors handled previously?"
3. **Apply Recovery**: Use recommended recovery strategies
4. Update constraints based on recovery insights
```

## Performance Optimization

### Caching Strategy
- Cache query results for 1 hour to avoid redundant analysis
- Cache key includes query hash, context fingerprint, and memory log version
- Invalidate cache when memory log is updated

### Query Efficiency
- Automatically limit analysis to most recent 100 experiences unless specified
- Use grep for fast filtering before detailed analysis
- Batch similar queries when multiple insights needed

### Cost Management
- Estimate query cost before execution
- Provide cost-benefit analysis for expensive queries
- Support lightweight "quick consultation" mode for simple questions

## Error Handling

### Memory Access Issues
- **Empty Memory Log**: Provide graceful degradation with basic recommendations
- **Corrupted Entries**: Skip corrupted data and report data quality issues
- **Access Errors**: Fall back to basic heuristics if memory unavailable

### Query Processing Errors
- **Unclear Queries**: Provide query refinement suggestions
- **No Results**: Suggest broader search criteria or related queries
- **Low Confidence**: Request human validation for uncertain recommendations

### Response Validation
- **Inconsistent Recommendations**: Flag conflicting insights for review
- **Outdated Patterns**: Weight recent experiences higher than old ones
- **Context Mismatch**: Warn when historical context differs significantly

## Future Enhancements

### Advanced Query Types
- **Predictive Queries**: "What are the likely outcomes for this approach?"
- **Comparative Analysis**: "How does approach A compare to approach B historically?"
- **Trend Analysis**: "How has performance changed over time for this task type?"

### Intelligent Automation
- **Proactive Consultation**: Automatically query memory without explicit requests
- **Real-time Adaptation**: Continuously adjust constraints based on memory insights
- **Learning Optimization**: Improve query accuracy based on outcome feedback

### Enhanced Integration
- **Multi-Agent Consultation**: Query memory on behalf of other agents
- **Cross-Task Learning**: Apply insights from different task types
- **Collaborative Memory**: Share insights across multiple SystemAgent instances


================================================
File: components/tools/RealFileSystemTool.md
================================================
# Component: RealFileSystemTool

- **Name**: RealFileSystemTool  
- **Type**: TOOL
- **Claude Tools**: Read, Write, Glob, LS
- **Description**: Performs real file system operations using Claude Code's native file tools.

## Operations

### Write Operation
**Claude Tool**: Write
- **Inputs**: 
  - `file_path` (string): Path within workspace/ 
  - `content` (string): Content to write
- **Outputs**:
  - `success` (boolean): Operation success status
  - `file_path` (string): Actual path written
  - `size` (number): File size in bytes

### Read Operation  
**Claude Tool**: Read
- **Inputs**:
  - `file_path` (string): Path to read
  - `limit` (number, optional): Max lines to read
- **Outputs**:
  - `content` (string): File content
  - `size` (number): File size
  - `lines` (number): Total lines

### Search Operation
**Claude Tool**: Glob
- **Inputs**:
  - `pattern` (string): File pattern to match
  - `path` (string, optional): Directory to search
- **Outputs**:
  - `files` (array): List of matching files
  - `count` (number): Number of matches

### List Operation
**Claude Tool**: LS  
- **Inputs**:
  - `path` (string): Directory to list
  - `ignore` (array, optional): Patterns to ignore
- **Outputs**:
  - `files` (array): Files in directory
  - `directories` (array): Subdirectories

## Real Tool Mapping

```yaml
write_operation:
  claude_tool: Write
  cost: none
  latency: low (<100ms)
  side_effects: "Creates/overwrites files in workspace"
  error_modes: ["permission_denied", "disk_full", "invalid_path"]

read_operation:
  claude_tool: Read  
  cost: none
  latency: low (<100ms)
  side_effects: none
  error_modes: ["file_not_found", "permission_denied", "too_large"]

search_operation:
  claude_tool: Glob
  cost: none  
  latency: low (<1s)
  side_effects: none
  error_modes: ["invalid_pattern", "permission_denied"]

list_operation:
  claude_tool: LS
  cost: none
  latency: low (<100ms) 
  side_effects: none
  error_modes: ["path_not_found", "permission_denied"]
```

## Logic

### EXECUTION MODE:
1. Determine operation type from inputs
2. Map to appropriate Claude Code tool
3. Execute real file system operation
4. Capture results and any errors
5. Return structured output with metadata

### SIMULATION MODE:
1. Simulate file system state changes
2. Generate realistic file content and metadata
3. Create training data for file operations

## Usage Examples

**Write file:**
```
Input: {operation: "write", file_path: "output.txt", content: "Hello World"}
Output: {success: true, file_path: "workspace/output.txt", size: 11}
```

**Read file:**
```
Input: {operation: "read", file_path: "input.txt"}
Output: {content: "File contents...", size: 1024, lines: 42}
```

**Search files:**
```
Input: {operation: "search", pattern: "*.md"}
Output: {files: ["readme.md", "docs.md"], count: 2}
```

## Error Handling

- **File Not Found**: Return clear error message with suggested alternatives
- **Permission Denied**: Check workspace boundaries, suggest valid paths
- **Invalid Path**: Sanitize and suggest corrected path
- **Disk Full**: Cleanup temporary files, suggest optimization

## Security Constraints

- **Workspace Isolation**: All operations confined to workspace/ directory
- **Path Validation**: Prevent directory traversal attacks
- **Size Limits**: Enforce reasonable file size restrictions
- **Type Restrictions**: Support text files, reject dangerous binaries

## Training Data Format

```json
{
  "tool_call": {
    "tool": "Write",
    "inputs": {"file_path": "output.txt", "content": "..."},
    "outputs": {"success": true, "file_path": "workspace/output.txt"},
    "performance": {"cost": "$0", "time": "45ms", "success": true}
  }
}
```


================================================
File: components/tools/RealWebFetchTool.md
================================================
# Component: RealWebFetchTool

- **Name**: RealWebFetchTool
- **Type**: TOOL
- **Claude Tool**: WebFetch
- **Description**: Fetches real, live content from web URLs using Claude Code's WebFetch capability.

## Inputs

- `url` (string): The URL to fetch content from
- `prompt` (string): Instructions for content extraction (default: "Extract the main text content")

## Outputs

- `content` (string): The extracted content from the webpage
- `summary` (string): Brief summary of what was fetched
- `metadata` (object): Information about the fetch operation

## Real Tool Mapping

```yaml
claude_tool: WebFetch
parameters:
  url: {{url}}
  prompt: {{prompt}}
cost: low ($0.001-0.01 per call)
latency: medium (2-10 seconds)
side_effects: "Network request to external server"
error_modes: ["timeout", "404", "403", "rate_limit", "connection_error"]
```

## Logic

### EXECUTION MODE (Claude Code Runtime):
1. Use Claude Code's WebFetch tool with provided URL and prompt
2. Capture the real response content
3. Extract metadata (response time, success status)
4. Save content to workspace file if needed
5. Return structured output with content and metadata

### EXECUTION MODE (LLM Interpreter Runtime):
1. Use curl or wget command to fetch URL content
2. Parse response headers for metadata (status, content-type)
3. Extract text content from HTML using simple text processing
4. Apply prompt instructions to filter/extract relevant content
5. Save content to workspace file and return structured output

### SIMULATION MODE:
1. Generate realistic mock content based on URL
2. Simulate typical response times and potential errors
3. Create training data showing tool call and response pattern

## Usage Examples

**Simple webpage fetch:**
```
Input: {url: "https://example.com", prompt: "Get the main content"}
Output: {content: "...", metadata: {status: "success", time: "2.3s"}}
```

**Article extraction:**
```
Input: {url: "https://blog.com/article", prompt: "Extract article title and body"}
Output: {content: "Title: ... Body: ...", metadata: {status: "success"}}
```

## Error Handling

- **Timeout**: Retry once with longer timeout
- **Rate Limit**: Wait and retry after delay
- **404/403**: Return error with specific message
- **Network Error**: Check connectivity, return informative error

## Training Data Format

```json
{
  "tool_call": {
    "tool": "WebFetch",
    "inputs": {"url": "https://example.com", "prompt": "Extract content"},
    "outputs": {"content": "...", "metadata": {...}},
    "performance": {"cost": "$0.003", "time": "2.1s", "success": true}
  }
}
```


================================================
File: components/tools/TranslationTool.md
================================================
# Component: TranslationTool

-   **Name**: TranslationTool
-   **Type**: TOOL
-   **Description**: Translates text content from one language to another.

### Inputs

-   `input_file_path` (string): The path to a text file within the `workspace/` directory that contains the content to be translated.
-   `source_language` (string): The source language code (e.g., "en" for English).
-   `target_language` (string): The target language code (e.g., "es" for Spanish).

### Outputs

-   `output_file_path` (string): The path within the `workspace/` directory where the translated content will be saved.

### Logic

1.  Read the full content from the `input_file_path`.
2.  Identify the source language and target language.
3.  For simulation purposes, apply basic translation rules:
   - "Example Domain" → "Dominio de Ejemplo"
   - "This domain is for use" → "Este dominio es para uso"
   - "in illustrative examples" → "en ejemplos ilustrativos"
   - "in documents" → "en documentos"
   - "You may use this domain" → "Puedes usar este dominio"
   - "without prior coordination" → "sin coordinación previa"
   - "asking for permission" → "pidiendo permiso"
   - "established to be used" → "establecido para ser usado"
4.  Write the translated text to the `output_file_path`.


================================================
File: components/tools/WebFetcherTool.md
================================================
# Component: WebFetcherTool

-   **Name**: WebFetcherTool
-   **Type**: TOOL
-   **Description**: Fetches the text content of a given URL.

### Inputs

-   `url` (string): The URL of the webpage to fetch.
-   `content` (string): For this simulation, the raw text content of the webpage will be provided directly.

### Outputs

-   `output_file_path` (string): The path within the `workspace/` directory where the fetched content will be saved. The filename should be derived from the URL (e.g., `example_com_content.txt`).

### Logic

1.  Acknowledge the `url` to be "fetched".
2.  Take the provided `content`.
3.  Save this `content` to the calculated `output_file_path` inside the `workspace/` directory. This serves as the raw material for the next step.


================================================
File: scenarios/Evolve_summarization.md
================================================
# Scenario: Evolve Summarization Agent

### Goal
The current SummarizationAgent outputs plain text summaries, but we need structured JSON output with separate 'title' and 'summary' fields for better integration with downstream systems. Evolve the SummarizationAgent by creating SummarizationAgent_v2.md with JSON output capability and update the SmartLibrary.md to point to the new version.

### Evolution Requirements

**Current Behavior**: SummarizationAgent_v1 outputs plain text summaries
**Desired Behavior**: SummarizationAgent_v2 outputs JSON with this structure:
```json
{
  "title": "Brief descriptive title (5-8 words)",
  "summary": "Concise summary paragraph"
}
```

### Test Input Data

**URL**: `https://www.ycombinator.com/about`

**Mock Content**: Use the existing Y Combinator content from the previous scenario to test the evolved component.

### Expected Deliverables
1. `components/agents/SummarizationAgent_v2.md` - The evolved component
2. Updated `system/SmartLibrary.md` - Pointing to v2 instead of v1
3. `summary_of_ycombinator_json.json` - JSON-formatted output file
4. Experience logged in SmartMemory showing successful evolution

### Success Criteria
- SystemAgent successfully identifies the need for component evolution
- Creates new component file with enhanced capabilities
- Updates library registry appropriately
- Demonstrates backwards compatibility planning
- Produces valid JSON output with title and summary fields


================================================
File: scenarios/FineTuned_LLM_Simulation.md
================================================
# Scenario: Fine-Tuned LLM Simulation

## Goal

Demonstrate how a fine-tuned LLM would operate as an autonomous state machine agent, making external tool calls when needed to complete the Hugging Face research task, similar to how Claude Code operates.

## Objective

Show the target behavior for an LLM trained on LLM-OS execution traces by:
1. Simulating autonomous decision-making for external tool calls
2. Demonstrating context management across tool interactions
3. Showing state machine progression through workflow steps
4. Validating training data patterns through simulation

## Task Specification

**Primary Goal**: Simulate a fine-tuned LLM completing the Hugging Face research task autonomously

**Target Behavior**:
- LLM analyzes current context and goal
- Identifies information gaps requiring external data
- Generates structured tool requests (WebFetch, Read, Write)
- Processes tool results and updates context
- Continues workflow until completion

**Deliverables**:
1. `workspace/simulation_log.md` - Decision-making trace
2. `workspace/tool_requests.json` - Generated tool calls
3. `workspace/context_evolution.md` - Context changes over time
4. `workspace/simulation_metrics.json` - Performance validation

## Simulation Workflow

### Phase 1: Initial Context Assessment
**Simulated LLM Reasoning**:
```
Current Context: Goal to research Hugging Face AI developments
Available Info: Goal description only
Information Gap: No current data about Hugging Face blog content
Decision: Need external data acquisition
```

**Expected Tool Request**:
```json
{
  "action": "TOOL_REQUEST",
  "tool_name": "WebFetch", 
  "reasoning": "Goal requires current AI developments from Hugging Face blog, but no recent data in context",
  "parameters": {
    "url": "https://huggingface.co/blog",
    "prompt": "Extract recent AI research, model announcements, and technical developments"
  },
  "expected_outcome": "Live blog content with current AI developments for analysis"
}
```

### Phase 2: Content Processing Decision
**Simulated LLM Reasoning**:
```
Current Context: Raw blog content available
Available Info: Unstructured web content
Information Gap: Need structured analysis of content
Decision: Process available information (no external call needed)
```

**Expected Processing Result**:
```json
{
  "action": "PROCESSING_RESULT",
  "content": {
    "analysis": "Structured summary of Hugging Face developments",
    "key_insights": ["transformer improvements", "new model releases", "platform updates"],
    "confidence": 0.93
  },
  "completion_status": "CONTINUE"
}
```

### Phase 3: Report Generation Decision
**Simulated LLM Reasoning**:
```
Current Context: Structured analysis complete
Available Info: Summary and insights
Information Gap: Need to persist results as deliverable
Decision: Require file write operation
```

**Expected Tool Request**:
```json
{
  "action": "TOOL_REQUEST",
  "tool_name": "Write",
  "reasoning": "Analysis complete, need to create final research report file",
  "parameters": {
    "file_path": "workspace/huggingface_research_report.md",
    "content": "# Hugging Face AI Research Report\n\n[Formatted report content]"
  },
  "expected_outcome": "Research report persisted as deliverable file"
}
```

### Phase 4: Completion Assessment
**Simulated LLM Reasoning**:
```
Current Context: Report generated and saved
Available Info: Complete workflow execution trace
Information Gap: None - goal achieved
Decision: Mark task as complete
```

**Expected Processing Result**:
```json
{
  "action": "PROCESSING_RESULT",
  "content": {
    "status": "TASK_COMPLETE",
    "summary": "Successfully researched Hugging Face developments and created report",
    "deliverables": ["workspace/huggingface_research_report.md"],
    "confidence": 0.97
  },
  "completion_status": "COMPLETE"
}
```

## Simulation Validation Criteria

### Decision Accuracy
- **Tool Selection**: Chooses correct tool for each information gap
- **Parameter Quality**: Provides appropriate parameters for tool calls
- **Context Usage**: Effectively utilizes available information

### State Management
- **Progression Logic**: Follows logical workflow steps
- **Context Integration**: Properly incorporates tool results
- **Completion Detection**: Recognizes when goal is achieved

### Autonomous Operation
- **No Human Intervention**: Makes all decisions independently
- **Error Recovery**: Handles potential tool failures gracefully
- **Reasoning Transparency**: Provides clear decision explanations

## Training Data Patterns Demonstrated

### Pattern 1: Information Gap → Tool Request
```
IF (goal requires data X) AND (data X not in context)
THEN generate tool_request for acquiring data X
```

### Pattern 2: Sufficient Context → Processing
```
IF (goal requires processing Y) AND (all needed data available)
THEN generate processing_result for Y
```

### Pattern 3: Results → Persistence
```
IF (processing complete) AND (results need saving)
THEN generate tool_request for file write
```

### Pattern 4: Goal Achievement → Completion
```
IF (all deliverables created) AND (goal satisfied)
THEN mark task as complete
```

## Expected Performance Metrics

### Simulation Success Criteria
- **Decision Points**: 4 major decision points handled correctly
- **Tool Requests**: 2-3 well-formed external tool calls
- **Processing Results**: 2-3 direct processing actions
- **Goal Achievement**: Complete research report generated
- **Autonomy Level**: 100% autonomous decision-making

### Quality Metrics
- **Reasoning Quality**: Clear explanations for each decision
- **Tool Request Accuracy**: Appropriate tools selected for information gaps
- **Context Management**: Effective integration of external data
- **Output Quality**: Professional research report generated

## Implementation Notes

This simulation demonstrates the **target architecture** for a fine-tuned LLM:

1. **State Machine Operation**: LLM operates as deterministic state machine
2. **External Tool Integration**: Makes structured requests for external data
3. **Context Management**: Maintains and updates working context across calls
4. **Autonomous Decision Making**: Decides when external calls are needed
5. **Goal-Oriented Execution**: Continues until objective is achieved

The simulation validates that training data from LLM-OS contains the patterns needed to teach an LLM this autonomous, tool-calling behavior.


================================================
File: scenarios/NewsAnalysis_InterpreterTest.md
================================================
# Scenario: News Analysis Pipeline Test

**Type**: Real-world execution test for LLM Interpreter
**Goal**: Demonstrate dynamic agent/tool creation and modification for news analysis
**Mode**: EXECUTION MODE (with LLM Interpreter runtime)

## Objective

Create an automated news analysis pipeline that:
1. Fetches content from multiple news sources 
2. Analyzes sentiment and extracts key information
3. Creates a comparative analysis report
4. Tests the system's ability to create and modify components during execution

## Test Requirements

### Phase 1: Basic Tool Usage
- Use existing `RealWebFetchTool` to fetch news content
- Use existing `RealSummarizationAgent` to process articles
- Verify markdown specifications are properly interpreted

### Phase 2: Dynamic Component Creation
- Create a new `NewsAnalysisAgent` during execution
- Create a new `SentimentAnalysisTool` as needed
- Test modification of existing tools to handle edge cases

### Phase 3: Adaptive Behavior
- Demonstrate constraint adaptation based on execution results
- Show memory-driven decision making
- Test error recovery and component evolution

## Input Sources

```yaml
news_sources:
  - name: "TechCrunch AI"
    url: "https://techcrunch.com/category/artificial-intelligence/"
    focus: "AI developments"
  - name: "Reuters Technology"  
    url: "https://www.reuters.com/technology/"
    focus: "Tech business news"
  - name: "BBC Technology"
    url: "https://www.bbc.com/news/technology"
    focus: "General tech news"
```

## Expected Component Evolution

### New Components to Create:

1. **NewsAnalysisAgent.md**
   - Input: Multiple article summaries
   - Output: Comparative analysis with trends
   - Logic: Cross-reference topics, identify patterns

2. **SentimentAnalysisTool.md**
   - Input: Article text
   - Output: Sentiment scores (positive/negative/neutral)
   - Logic: Keyword analysis and tone detection

3. **TrendDetectionTool.md** (if needed)
   - Input: Historical news data
   - Output: Emerging trend identification
   - Logic: Pattern recognition across time periods

### Modifications to Test:

1. **RealWebFetchTool** enhancement for handling rate limits
2. **RealSummarizationAgent** adaptation for news-specific formatting
3. **SystemAgent** constraint updates based on news analysis requirements

## Success Criteria

### Technical Success:
- [ ] Successfully interprets existing markdown tool definitions
- [ ] Creates new agent/tool markdown files during execution
- [ ] Modifies existing components when needed
- [ ] Maintains modular state architecture
- [ ] Generates structured training data

### Functional Success:
- [ ] Fetches real content from all 3 news sources
- [ ] Generates meaningful summaries for each article
- [ ] Produces comparative analysis across sources
- [ ] Adapts to any fetch failures or rate limits
- [ ] Creates final report in structured format

### Adaptive Behavior Success:
- [ ] Shows constraint adaptation during execution
- [ ] Demonstrates memory consultation for decisions
- [ ] Handles errors gracefully with component evolution
- [ ] Updates behavioral constraints based on execution events

## Expected Output Structure

```
workspace/
├── state/
│   ├── plan.md              # Execution steps and progress
│   ├── context.md           # Knowledge accumulation
│   ├── variables.json       # Data passing between steps
│   ├── history.md           # Complete execution log
│   └── constraints.md       # Behavioral adaptations
├── news_content/
│   ├── techcrunch_ai.html   # Raw fetched content
│   ├── reuters_tech.html    
│   └── bbc_technology.html
├── summaries/
│   ├── techcrunch_summary.json
│   ├── reuters_summary.json
│   └── bbc_summary.json
├── analysis/
│   ├── sentiment_analysis.json
│   ├── trend_detection.json
│   └── comparative_report.md
└── training_data/
    └── execution_trace.json  # Complete execution trace for training
```

## Test Execution Command

For LLM Interpreter runtime:
```bash
./llmunix-llm execute: "Act as SystemAgent and execute the NewsAnalysis_InterpreterTest scenario"
```

For Claude Code runtime:
```bash
boot llmunix
llmunix execute: "Act as SystemAgent and execute the NewsAnalysis_InterpreterTest scenario"
```

## Component Creation Testing

### Phase A: Use Existing Components
1. SystemAgent reads this scenario
2. Consults SmartLibrary.md for available components
3. Plans execution using existing RealWebFetchTool and RealSummarizationAgent
4. Executes basic fetch and summarize workflow

### Phase B: Dynamic Component Creation
1. SystemAgent identifies need for sentiment analysis capability
2. Creates `components/tools/SentimentAnalysisTool.md` with proper specification
3. Updates `system/SmartLibrary.md` to register new component
4. Uses new component in subsequent execution steps

### Phase C: Component Modification
1. Encounters rate limiting or parsing issues
2. Modifies existing RealWebFetchTool.md to handle edge case
3. Updates execution approach using modified component
4. Documents adaptation in constraints.md and memory_log.md

## Error Scenarios to Test

1. **Network failures**: Test tool adaptation and retry logic
2. **Content parsing errors**: Test agent modification for different formats
3. **Rate limiting**: Test delay injection and alternative sources
4. **Component not found**: Test dynamic component creation
5. **Low confidence results**: Test human-in-loop integration

## Expected Learning Outcomes

1. **Component Flexibility**: Markdown definitions are interpreted and can be created/modified during runtime
2. **Adaptive Execution**: System adapts constraints and behavior based on execution events
3. **Memory Integration**: Past experiences influence current decision making
4. **Training Data Generation**: Complete execution traces suitable for fine-tuning
5. **Error Resilience**: Intelligent error recovery using component evolution

## Validation Points

### During Execution:
- Monitor constraint evolution in `constraints.md`
- Check component creation timestamps and modification logs
- Verify tool call patterns match markdown specifications
- Confirm modular state updates after each step

### Post Execution:
- Analyze training data quality and completeness
- Review memory log entries for learning insights
- Validate final output quality and structure
- Check component registry updates in SmartLibrary.md

This scenario thoroughly tests the LLM interpreter's ability to use, create, and modify markdown-defined components while maintaining the pure markdown operating system philosophy.


================================================
File: scenarios/RealWorld_Research_Task.md
================================================
# Scenario: Real-World Research Task

## Goal

Conduct real research on current AI developments by fetching live content from Hugging Face's blog, analyzing it, and creating a comprehensive research report with structured summaries and key insights.

## Objective

Demonstrate LLM-OS's real-world capabilities by:
1. Fetching live web content using real HTTP requests
2. Processing and analyzing actual content 
3. Creating structured output with multiple formats
4. Generating training data from real execution

## Task Specification

**Primary Goal**: Research and summarize recent AI developments from https://huggingface.co/blog

**Deliverables**:
1. `workspace/huggingface_blog_content.txt` - Raw fetched content
2. `workspace/ai_research_summary.json` - Structured JSON summary
3. `workspace/ai_research_report.md` - Formatted research report
4. `workspace/execution_trace.json` - Training data from execution

## Expected Workflow

### Phase 1: Real Web Data Acquisition
- Use RealWebFetchTool to fetch live content from Hugging Face blog
- Handle real network latency, errors, rate limits
- Capture actual HTTP response data and metadata

### Phase 2: Real Content Analysis  
- Use RealSummarizationAgent to analyze fetched content
- Generate multiple format outputs (JSON, Markdown)
- Extract key insights, trends, and important announcements

### Phase 3: Real File Operations
- Use RealFileSystemTool for all file operations
- Create structured output files in workspace
- Manage intermediate processing files

### Phase 4: Training Data Generation
- Capture complete execution trace with real tool calls
- Record actual costs, timing, and error handling
- Generate fine-tuning dataset from real execution

## Success Criteria

1. **Real Execution**: All operations use actual Claude Code tools
2. **Live Data**: Content is fetched from real, current web sources  
3. **Error Resilience**: Handles real-world network and system errors
4. **Quality Output**: Produces professional-grade research deliverables
5. **Training Data**: Generates high-quality fine-tuning examples

## Expected Challenges

- **Network Variability**: Real web requests may have timeouts or errors
- **Content Complexity**: Live content requires robust parsing and analysis
- **State Management**: Complex workflow requires careful state tracking
- **Cost Optimization**: Real tools have actual costs that need management

## Performance Targets

- **Total Execution Time**: <5 minutes
- **Content Quality**: >85% accuracy in key information extraction
- **Error Recovery**: Successfully handle at least 1 error scenario
- **Cost Efficiency**: <$0.10 total execution cost

## Training Data Output

Expected training dataset structure:
```json
{
  "execution_id": "real_research_001",
  "scenario": "RealWorld_Research_Task", 
  "total_cost": "$0.067",
  "total_time": "3m 42s",
  "tool_calls": [
    {
      "step": 1,
      "tool": "WebFetch",
      "real_inputs": {"url": "https://huggingface.co/blog"},
      "real_outputs": {"content": "...", "metadata": {...}},
      "performance": {"cost": "$0.012", "time": "8.3s"}
    }
  ],
  "state_transitions": [...],
  "final_outcome": "success",
  "quality_metrics": {...}
}
```

This scenario serves as both a practical demonstration of LLM-OS capabilities and a generator of high-quality training data for developing autonomous AI agents.


================================================
File: scenarios/Summarize_website.md
================================================
# Scenario: Summarize a Website

### Goal
Fetch the content of the website 'https://www.ycombinator.com/about', create a concise summary, and save the summary to a final output file named `summary_of_ycombinator_about.txt`.

### Input Data for Simulation

**URL**: `https://www.ycombinator.com/about`

**Mock Content for `https://www.ycombinator.com/about`**:
"""
About Y Combinator

Y Combinator is a startup accelerator that has helped launch over 4,000 companies, including Airbnb, Dropbox, Stripe, Reddit, Discord, DoorDash, Coinbase, Instacart, and many others. Since 2005, we've funded startups in batches twice a year.

Our program is 3 months long, and we work intensively with the companies in our batch to help them get in the best possible shape and refine their pitch to investors. Each batch culminates in Demo Day, where the startups present their companies to a carefully selected, invite-only audience of investors.

In addition to the core accelerator program, Y Combinator runs several other programs:

- YC Growth: a program for later-stage companies
- YC Continuity: our growth-stage investment fund  
- Startup School: a free online course for entrepreneurs
- Work at a Startup: our job board connecting job seekers with YC companies

Y Combinator's mission is to help startups succeed. We believe that technology companies have enormous potential to improve the world, and we want to help the most promising startups reach their full potential.

We invest $500,000 in every company we fund. In exchange, we take 7% equity. But we think the value we provide goes far beyond the money. The YC network is powerful – our alumni companies help each other in ways that range from sharing technical knowledge to making introductions to potential customers, partners, and employees.

Our partners include some of the most successful entrepreneurs and investors in Silicon Valley. They provide mentorship, connections, and expertise to help our startups succeed.
"""


================================================
File: scenarios/Translate_website.md
================================================
# Scenario: Translate Website Content

### Goal
Fetch the content of the website 'https://example.com', translate it to Spanish, and save the translation to a final output file named `translated_example_com.txt`.

### Input Data for Simulation

**URL**: `https://example.com`

**Mock Content for `https://example.com`**:
"""
Example Domain

This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission. This domain is established to be used for illustrative examples in documents.
"""

### Expected Challenge
This scenario requires a TranslationTool that does not exist in the current SmartLibrary, testing the SystemAgent's ability to handle missing components and either create new ones or gracefully fail with clear diagnostics.


================================================
File: system/ClaudeCodeToolMap.md
================================================
# Claude Code Tool Mapping

This file defines how LLM-OS framework components map to Claude Code's native tools and their real-world characteristics.

## Tool Mappings

### WebFetcherTool → WebFetch
```yaml
framework_tool: tool_web_fetcher_v1
claude_tool: WebFetch
parameters:
  url: url
  prompt: "Extract the main content from this webpage"
cost: low ($0.001-0.01 per call)
latency: medium (2-10 seconds)
side_effects: "Network request, may be rate limited"
error_modes: ["timeout", "404", "forbidden", "rate_limit"]
capabilities: ["html_parsing", "text_extraction", "link_following"]
limitations: ["no_javascript", "static_content_only"]
```

### FileWriterTool → Write
```yaml
framework_tool: tool_file_writer_v1
claude_tool: Write
parameters:
  file_path: file_path
  content: content
cost: none
latency: low (<100ms)
side_effects: "Creates/overwrites files in workspace"
error_modes: ["permission_denied", "disk_full", "invalid_path"]
capabilities: ["text_files", "json", "csv", "markdown"]
limitations: ["workspace_only", "text_only"]
```

### FileReaderTool → Read
```yaml
framework_tool: tool_file_reader_v1
claude_tool: Read
parameters:
  file_path: file_path
cost: none
latency: low (<100ms)
side_effects: none
error_modes: ["file_not_found", "permission_denied", "too_large"]
capabilities: ["text_files", "images", "structured_data"]
limitations: ["size_limits", "binary_limited"]
```

### SearchTool → Grep/Glob
```yaml
framework_tool: tool_search_v1
claude_tool: Grep
parameters:
  pattern: pattern
  path: path
  include: include
cost: none
latency: low (<1s)
side_effects: none
error_modes: ["invalid_regex", "permission_denied"]
capabilities: ["regex_search", "file_filtering", "content_search"]
limitations: ["text_only", "repo_scope"]
```

### SystemTool → Bash
```yaml
framework_tool: tool_system_v1
claude_tool: Bash
parameters:
  command: command
  description: description
cost: variable
latency: variable (1s-10min)
side_effects: "Can modify system state, install packages, etc."
error_modes: ["command_not_found", "permission_denied", "timeout"]
capabilities: ["full_system_access", "package_management", "file_operations"]
limitations: ["security_restrictions", "timeout_limits"]
```

### SubTaskTool → Task
```yaml
framework_tool: tool_subtask_v1
claude_tool: Task
parameters:
  description: description
  prompt: prompt
cost: medium (spawns new agent)
latency: high (30s-5min)
side_effects: "Creates parallel execution context"
error_modes: ["task_failure", "context_limit", "infinite_recursion"]
capabilities: ["parallel_execution", "specialized_agents", "complex_workflows"]
limitations: ["context_sharing", "coordination_complexity"]
```

### HumanInTheLoopTool → Interactive Input
```yaml
framework_tool: tool_human_v1
claude_tool: console_interaction
parameters:
  question: question
  context: context
cost: high (human time)
latency: very_high (minutes-hours)
side_effects: "Pauses execution, requires human presence"
error_modes: ["no_response", "ambiguous_input", "unavailable"]
capabilities: ["complex_decisions", "domain_expertise", "approval_workflows"]
limitations: ["availability", "response_time", "scalability"]
```

## Cost Model

### Token Cost Estimates
- **WebFetch**: 1000-5000 tokens per call
- **Read**: 100-2000 tokens per file
- **Write**: 50-500 tokens per operation
- **Grep**: 100-1000 tokens per search
- **Bash**: 200-2000 tokens per command
- **Task**: 5000-50000 tokens per subtask

### Time Estimates
- **Immediate**: <100ms (Read, Write, local Grep)
- **Fast**: 100ms-1s (Bash commands, complex Grep)
- **Medium**: 1-10s (WebFetch, simple Task)
- **Slow**: 10s-1min (complex Bash, large Task)
- **Interactive**: 1min+ (Human input)

### Decision Matrix

**When to use WebFetch vs cached content:**
- Fresh data needed → WebFetch
- Static reference → cached content

**When to use Task vs direct execution:**
- Complex subtask → Task
- Simple operation → direct

**When to use Bash vs native tools:**
- System integration → Bash
- File operations → native tools

## Real-World Constraints

### Rate Limits
- WebFetch: Respectful crawling, avoid hammering
- API calls: Respect service limits
- Token usage: Monitor consumption

### Security
- Bash: Sanitize inputs, avoid dangerous commands
- File operations: Workspace containment
- Network: HTTPS only, validate domains

### Reliability
- Retry logic for transient failures
- Fallback strategies for tool unavailability
- Graceful degradation when tools fail

## Training Data Collection

Each tool call generates training data:
```json
{
  "tool_call": {
    "framework_tool": "tool_web_fetcher_v1",
    "claude_tool": "WebFetch",
    "inputs": {"url": "https://example.com"},
    "outputs": {"content": "..."},
    "metadata": {
      "cost": "$0.003",
      "latency": "2.4s",
      "success": true,
      "errors": []
    }
  }
}
```

This mapping enables LLM-OS to make intelligent tool choices based on real performance characteristics while generating high-quality training data.


================================================
File: system/ExecutionStateTemplate.md
================================================
# Execution State Template

**DEPRECATED**: This template has been superseded by the modular state architecture. Use `system/StateDirectoryTemplate.md` instead.

This legacy template is maintained for reference but should not be used for new executions.

## Template Structure

```markdown
# Execution State: [Goal Title]

## Metadata
- **execution_id**: exec_[timestamp]_[uuid]
- **goal**: [User's original goal]
- **status**: PLANNING | IN_PROGRESS | COMPLETED_SUCCESS | COMPLETED_FAILURE | PAUSED
- **current_step**: [number]
- **start_time**: [ISO timestamp]
- **last_updated**: [ISO timestamp]
- **mode**: SIMULATION | EXECUTION

## Variables
[Dynamic variables that pass data between steps]
- `variable_name`: value
- `file_path`: workspace/output.txt

## Execution Plan
[Numbered steps with metadata]

### Step 1: [Action Name]
- **component**: [component_id from SmartLibrary]
- **tool_mapping**: [Claude Code tool if EXECUTION mode]
- **status**: PENDING | RUNNING | COMPLETED | FAILED
- **inputs**: 
  - param1: value or {{variable_reference}}
- **expected_outputs**:
  - result: description
- **estimated_cost**: $X.XX
- **estimated_time**: Xs
- **side_effects**: [description]

### Step 2: [Action Name]
[... continue for all planned steps]

## Execution Log
[Real-time log of step executions]

### Step 1 Execution - [timestamp]
- **tool_used**: [actual Claude Code tool]
- **real_inputs**: [actual parameters]
- **real_outputs**: [actual results]
- **actual_cost**: $X.XX
- **actual_time**: Xs
- **errors**: [any errors encountered]
- **state_changes**: [files created/modified]

## Training Data
[Structured data for fine-tuning - only in EXECUTION mode]
- **conversation_trace**: [full conversation]
- **tool_calls**: [all tool invocations]
- **state_transitions**: [state changes]
- **performance_metrics**: [cost, time, success rate]
```

## State Management Rules

1. **Atomic Updates**: Each step completion updates the entire state
2. **Immutable History**: Past executions are never modified
3. **Variable Propagation**: Outputs become inputs for subsequent steps
4. **Error Isolation**: Failed steps don't corrupt overall state
5. **Resumability**: Execution can pause and resume at any step

## State Transitions

```
PLANNING → IN_PROGRESS → COMPLETED_SUCCESS
    ↓           ↓              ↑
  FAILED ← PAUSED --------→ RESUMED
```

## Usage

1. SystemAgent copies this template to `workspace/execution_state.md`
2. Fills in goal and creates initial plan
3. Updates state after each step execution
4. Records final outcome and training data


================================================
File: system/FineTunedLLMSimulator.md
================================================
# Fine-Tuned LLM Simulator

This component simulates how a fine-tuned LLM would operate as a state machine after being trained on LLM-OS execution traces. The simulation demonstrates the target behavior where the LLM can autonomously request external tool calls when needed.

## Simulation Architecture

### Core Concept
The fine-tuned LLM operates as a **state machine** that:
1. Processes current context and state
2. Determines if external information is needed
3. Requests specific tool calls via structured output
4. Integrates tool results into context
5. Continues workflow execution
6. Repeats until goal completion

### State Machine Flow
```
Current Context → Reasoning → Decision Point:
├── Has Sufficient Info? → Continue Processing → Output
└── Needs External Data? → Request Tool Call → Wait for Results → Continue
```

## Simulation Protocol

### Input Format
The simulator expects:
- Current execution state
- Available context
- Goal description
- Tool availability manifest

### Output Format
The simulator produces either:
- **Tool Request**: Structured call for external data
- **Processing Result**: Direct output when sufficient context exists
- **State Update**: Modified execution state

### Tool Request Schema
```json
{
  "action": "TOOL_REQUEST",
  "tool_name": "WebFetch|Read|Write|Bash|Task",
  "reasoning": "Why this tool call is needed",
  "parameters": {
    "specific_tool_params": "values"
  },
  "expected_outcome": "What information this will provide",
  "next_state": "Expected state after tool completion"
}
```

### Processing Result Schema
```json
{
  "action": "PROCESSING_RESULT", 
  "content": "Generated content/analysis",
  "confidence": 0.95,
  "state_update": {
    "current_step": "updated_step",
    "variables": {"updated": "variables"}
  },
  "completion_status": "CONTINUE|COMPLETE"
}
```

## Simulation Logic

The simulator implements the core decision-making pattern learned from training data:

1. **Context Assessment**: Evaluate available information against current goal
2. **Information Gap Analysis**: Identify missing data needed to proceed
3. **Tool Selection**: Choose appropriate external tool based on information need
4. **Request Formulation**: Structure tool call with specific parameters
5. **Result Integration**: Process tool output and update context
6. **Workflow Continuation**: Proceed with enhanced context

## Training Data Patterns

The simulation is based on these patterns from LLM-OS training data:

### Pattern 1: Web Data Acquisition
```
State: Need current information
Gap: No live data about topic X
Tool: WebFetch with specific URL and prompt
Result: Raw content for analysis
Continue: Process fetched content
```

### Pattern 2: File Operations
```
State: Need to persist/retrieve data
Gap: Content not in current context
Tool: Read/Write with file path
Result: Content available in context
Continue: Use content for next step
```

### Pattern 3: Complex Analysis
```
State: Need specialized processing
Gap: Task beyond current capabilities
Tool: Task with sub-agent prompt
Result: Processed analysis
Continue: Integrate analysis into workflow
```

## Simulation Modes

### Mode 1: Interactive Simulation
- Step-by-step execution with human confirmation
- Shows decision points and reasoning
- Demonstrates tool request patterns

### Mode 2: Autonomous Simulation  
- Continuous execution mimicking fine-tuned behavior
- Automatic tool calls and result integration
- End-to-end workflow completion

### Mode 3: Training Validation
- Compares simulation behavior to training data
- Validates decision patterns and tool usage
- Measures accuracy of tool request predictions

## Implementation Details

The simulator uses LLM-OS's existing components but adds:
- **Decision Logic**: Pattern matching from training data
- **Tool Request Generation**: Structured output formatting
- **Context Management**: State tracking across tool calls
- **Reasoning Transparency**: Visible decision-making process

## Success Criteria

A successful simulation demonstrates:
1. **Autonomous Decision Making**: LLM decides when external calls are needed
2. **Accurate Tool Selection**: Chooses correct tools for information gaps
3. **Structured Requests**: Formats tool calls properly
4. **Context Integration**: Uses tool results effectively
5. **Goal Achievement**: Completes workflows without human intervention

This simulation provides the blueprint for training an LLM to operate as an autonomous agent that can make external tool calls when needed, similar to Claude Code's capabilities.


================================================
File: system/LLMunixCommandInterpreter.md
================================================
# LLMunix Command Interpreter

## Intelligent Command Processing Engine

The LLMunix Command Interpreter is an AI-powered shell that understands natural language commands, dynamically generates tools, and provides intelligent assistance for complex tasks.

### Core Architecture

```
Command Interpreter Pipeline
├── Input Parser
│   ├── Natural Language Processor
│   ├── Command Syntax Analyzer  
│   ├── Intent Recognition Engine
│   └── Parameter Extraction
├── Tool Resolver
│   ├── SmartLibrary Lookup
│   ├── Dynamic Tool Generator
│   ├── Tool Composition Engine
│   └── Capability Matcher
├── Execution Engine
│   ├── Process Manager Interface
│   ├── Resource Allocation
│   ├── Error Handling
│   └── Result Processing
└── Response Generator
    ├── Output Formatter
    ├── Explanation Generator
    ├── Suggestion Engine
    └── Help System
```

### Command Processing Flow

#### 1. Input Analysis
```markdown
## Natural Language Understanding
- Parses user input using NLP techniques
- Identifies command intent and required actions
- Extracts parameters and constraints from natural language
- Handles ambiguous commands with clarification requests

## Command Classification
- System commands (built-in shell operations)
- Tool invocation (existing tools in SmartLibrary)
- Tool generation (new tool creation requests)
- Information queries (help, status, documentation)
- Workflow composition (multi-step task sequences)

## Context Integration
- Considers current working directory and file context
- Integrates with memory system for personalized responses
- Maintains conversation history for contextual understanding
- Adapts to user preferences and common patterns
```

#### 2. Tool Resolution
```markdown
## SmartLibrary Integration
- Searches for existing tools matching user intent
- Evaluates tool compatibility and requirements
- Suggests alternative tools when exact matches unavailable
- Provides tool usage examples and documentation

## Dynamic Tool Generation
- Generates new tools when existing ones insufficient
- Creates tool definitions based on user requirements
- Maps to appropriate Claude Code native functions
- Registers generated tools in SmartLibrary for reuse

## Tool Composition
- Combines multiple tools for complex workflows
- Creates tool pipelines and data flow connections
- Optimizes execution order and resource usage
- Handles error propagation and recovery
```

#### 3. Execution Management
```markdown
## Process Orchestration
- Manages tool execution lifecycle
- Handles concurrent and sequential execution
- Monitors resource usage and performance
- Implements timeout and cancellation support

## Error Handling
- Provides intelligent error diagnosis and recovery
- Suggests alternative approaches when execution fails
- Learns from errors to improve future execution
- Maintains system stability during error conditions

## Result Integration
- Formats and presents execution results
- Provides result analysis and insights
- Stores results in appropriate memory contexts
- Updates system state based on execution outcomes
```

### Command Types

#### 1. Natural Language Commands
```bash
# Intent-based commands
llmunix$ "scrape the homepage of example.com and save as markdown"
llmunix$ "find all python files that contain database connections"
llmunix$ "create a summary of the last 10 git commits"
llmunix$ "optimize the images in the assets folder"

# Conversational commands
llmunix$ "I need to analyze website performance"
llmunix$ "Help me debug this script"
llmunix$ "What's the best way to backup my data?"
llmunix$ "Show me how to use the web scraper"
```

#### 2. Hybrid Commands
```bash
# Natural language with technical parameters
llmunix$ "download youtube video --quality 720p --format mp4"
llmunix$ "translate document.txt to spanish --preserve-formatting"
llmunix$ "compress images in ./photos --quality 85 --format webp"
llmunix$ "backup database --schedule daily --retention 30d"

# Technical commands with natural language options
llmunix$ rsync --explain "what does this command do?"
llmunix$ git commit -m "changes" --suggest-better-message
llmunix$ docker run --help-with "setting up a web server"
```

#### 3. Tool Generation Commands
```bash
# Generate tools on demand
llmunix$ generate tool "pdf merger" --requirements "fast, preserve bookmarks"
llmunix$ create command "deploy" --actions "build, test, upload, notify"
llmunix$ make tool "log analyzer" --input "log files" --output "report"

# Tool modification commands
llmunix$ enhance tool "webscraper" --add "javascript support"
llmunix$ optimize tool "image-resizer" --target "memory usage"
llmunix$ update tool "backup" --add-feature "incremental backups"
```

### Intelligent Features

#### 1. Context-Aware Suggestions
```markdown
## Command Completion
- Intelligent auto-completion based on current context
- Suggests parameters based on file system contents
- Provides examples and usage patterns
- Learns from user corrections and preferences

## Next Action Prediction
- Predicts likely next commands based on current workflow
- Suggests optimization opportunities
- Provides workflow shortcuts and automation
- Offers related tools and capabilities

## Contextual Help
- Provides help relevant to current situation
- Shows examples specific to current directory/project
- Explains complex commands in simple terms
- Offers step-by-step guidance for complex tasks
```

#### 2. Error Intelligence
```markdown
## Error Prevention
- Validates commands before execution
- Warns about potentially destructive operations
- Suggests safer alternatives for risky commands
- Checks dependencies and prerequisites

## Error Recovery
- Analyzes error conditions and provides specific solutions
- Suggests command corrections for common mistakes
- Provides recovery steps for failed operations
- Learns from errors to improve future prevention

## Error Learning
- Builds knowledge base of common error patterns
- Personalizes error handling based on user patterns
- Improves error messages and suggestions over time
- Shares error solutions across similar contexts
```

#### 3. Workflow Optimization
```markdown
## Command Sequencing
- Identifies common command patterns and sequences
- Suggests automation opportunities for repetitive tasks
- Creates custom workflows and shortcuts
- Optimizes execution order for efficiency

## Resource Management
- Monitors and optimizes resource usage across commands
- Predicts resource requirements for complex workflows
- Manages concurrent execution for optimal performance
- Provides resource usage insights and recommendations

## Performance Learning
- Tracks command execution performance over time
- Identifies bottlenecks and optimization opportunities
- Learns optimal parameters for different contexts
- Provides performance insights and recommendations
```

### Command Parsing Engine

#### 1. Natural Language Processing
```markdown
## Intent Recognition
- Identifies primary action verb (download, create, analyze, etc.)
- Extracts object targets (files, URLs, data, etc.)
- Recognizes modifiers and constraints (format, quality, size, etc.)
- Handles compound commands with multiple actions

## Parameter Extraction
- Extracts explicit parameters from natural language
- Infers implicit parameters from context
- Handles parameter aliases and variations
- Validates parameter types and ranges

## Ambiguity Resolution
- Identifies ambiguous commands and requests clarification
- Uses context to resolve common ambiguities
- Provides multiple interpretation options when unclear
- Learns from user disambiguation choices
```

#### 2. Command Mapping
```markdown
## Tool Mapping
- Maps natural language intents to available tools
- Handles tool parameter mapping and conversion
- Manages tool capability matching and selection
- Provides fallback options for unavailable tools

## System Integration
- Maps commands to file system operations
- Integrates with process management system
- Handles memory and resource management commands
- Manages system configuration and settings

## Extension Points
- Supports custom command parsers and handlers
- Allows plugin integration for specialized domains
- Provides API for external tool integration
- Supports command macro and alias definitions
```

### Command Execution

#### 1. Execution Planning
```markdown
## Dependency Analysis
- Identifies tool and resource dependencies
- Plans execution order to satisfy dependencies
- Handles circular dependencies and conflicts
- Optimizes execution for performance and reliability

## Resource Allocation
- Allocates memory and processing resources
- Manages concurrent execution limits
- Handles resource conflicts and contention
- Provides resource usage monitoring and alerts

## Risk Assessment
- Evaluates command safety and potential impacts
- Requires confirmation for destructive operations
- Implements safety checks and validation
- Provides undo/rollback capabilities where possible
```

#### 2. Execution Monitoring
```markdown
## Progress Tracking
- Provides real-time execution progress updates
- Shows detailed execution steps and status
- Handles long-running operation monitoring
- Supports execution cancellation and pause/resume

## Performance Monitoring
- Tracks execution time and resource usage
- Identifies performance bottlenecks and issues
- Provides optimization suggestions during execution
- Maintains performance history for analysis

## Error Monitoring
- Monitors for error conditions during execution
- Provides early warning for potential failures
- Implements graceful error handling and recovery
- Maintains error logs and diagnostic information
```

### Integration Interfaces

#### 1. Memory System Integration
```markdown
## Command History
- Maintains intelligent command history with context
- Provides semantic search of command history
- Learns from command patterns and outcomes
- Suggests historical commands for similar contexts

## Learning Integration
- Integrates with memory system for pattern learning
- Updates user preferences based on command usage
- Learns optimal parameters for different contexts
- Builds personalized command suggestions

## Context Preservation
- Maintains command context across sessions
- Preserves workflow state and intermediate results
- Handles session recovery and continuation
- Manages distributed execution state
```

#### 2. File System Integration
```markdown
## Path Intelligence
- Provides intelligent path completion and validation
- Understands file system structure and relationships
- Handles virtual file system operations
- Integrates with component-based file operations

## File Operations
- Maps file operations to appropriate tools
- Handles batch operations and file patterns
- Provides file type-specific operation suggestions
- Manages file permissions and access control

## Workspace Management
- Maintains awareness of current workspace context
- Provides project-specific command suggestions
- Handles workspace-specific tool configurations
- Manages workspace state and preferences
```

### Command Examples

#### Tool Generation Examples
```bash
# Generate a PDF tool
llmunix$ "I need to merge multiple PDF files into one"
→ Generating PDFMergerTool with requirements: input validation, bookmark preservation, compression options
→ Tool registered as /bin/pdfmerge
→ Usage: pdfmerge input1.pdf input2.pdf --output merged.pdf

# Generate a log analyzer
llmunix$ "create a tool to analyze nginx access logs and show top IPs"
→ Generating LogAnalyzerTool with nginx-specific parsing
→ Tool registered as /bin/loganalyze
→ Usage: loganalyze /var/log/nginx/access.log --top-ips 10

# Generate a deployment tool
llmunix$ "make a deployment command that builds, tests, and uploads to AWS"
→ Generating DeploymentTool with AWS integration
→ Tool registered as /bin/deploy
→ Usage: deploy --environment production --skip-tests false
```

#### Workflow Composition Examples
```bash
# Complex workflow
llmunix$ "download this video, extract audio, transcribe it, and create a summary"
→ Planning workflow: YouTubeDownloader → AudioExtractor → Transcriber → Summarizer
→ Estimated time: 5-10 minutes
→ Execute? [Y/n]

# Data processing pipeline
llmunix$ "process all CSV files in data/ directory: clean, analyze, and generate reports"
→ Planning pipeline: FileScanner → CSVCleaner → DataAnalyzer → ReportGenerator
→ Found 15 CSV files to process
→ Execute batch processing? [Y/n]
```


================================================
File: system/LLMunixFileSystem.md
================================================
# LLMunix File System Manager

## Virtual File System Architecture

LLMunix uses the LLM-OS folder structure as its native file system, treating markdown component definitions as executable programs and system resources.

### File System Hierarchy

```
/ (Root - LLM-OS base directory)
├── /system/           # Kernel and system services
│   ├── SystemAgent.md      # Process orchestrator
│   ├── SmartLibrary.md     # Component registry
│   ├── SmartMemory.md      # Persistent memory
│   ├── LLMunixKernel.md    # Kernel definition
│   └── LLMunixFileSystem.md # This file system manager
├── /components/       # Executable components
│   ├── /tools/        # System tools and utilities
│   └── /agents/       # Intelligent agents
├── /scenarios/        # Execution scenarios and workflows
├── /workspace/        # Active execution environment
│   ├── execution_state.md  # Current system state
│   └── /tmp/          # Temporary files and cache
├── /bin/              # Generated executable tools (virtual)
├── /proc/             # Process information (virtual)
├── /dev/              # Device interfaces (virtual)
└── /var/              # Variable data and logs (virtual)
```

### File System Operations

#### Core File Operations

```markdown
## fs_read(path)
- **Purpose**: Read file contents or component definition
- **Input**: Virtual file path
- **Output**: File contents, metadata, and access information
- **Implementation**: Maps to Claude Code Read tool

## fs_write(path, content, mode)
- **Purpose**: Write or update file/component
- **Input**: Path, content, write mode (create, update, append)
- **Output**: Success status and updated metadata
- **Implementation**: Maps to Claude Code Write/Edit tools

## fs_save(content, filename, location)
- **Purpose**: Save content to workspace with intelligent naming
- **Input**: Content data, optional filename, target location
- **Output**: Saved file path and metadata
- **Implementation**: Maps to Claude Code Write tool with workspace validation
- **Default Location**: /workspace/ (user data directory)

## fs_autosave(content, context)
- **Purpose**: Automatically save with smart filename generation
- **Input**: Content and execution context
- **Output**: Generated filename and save location
- **Implementation**: Intelligent naming based on content type and timestamp
- **Features**: Duplicate handling, format detection, metadata preservation

## fs_list(path, filter)
- **Purpose**: List directory contents with optional filtering
- **Input**: Directory path, optional filter criteria
- **Output**: File/directory listing with metadata
- **Implementation**: Maps to Claude Code LS/Glob tools

## fs_search(pattern, scope)
- **Purpose**: Search for files or content patterns
- **Input**: Search pattern, scope (path, content, metadata)
- **Output**: Matching files with context information
- **Implementation**: Maps to Claude Code Grep tool
```

#### Virtual File System Features

```markdown
## Component Execution Interface
- Markdown files in /components/ are treated as executable programs
- File extension determines execution context:
  - .md → Component definition (executed by SystemAgent)
  - .tool → Generated tool (executed by tool interpreter)
  - .agent → Intelligent agent (executed by agent runtime)

## Dynamic Path Resolution
- /bin/ directory populated with generated tools at runtime
- Symbolic links to /components/ based on tool registration
- Path completion includes both static and dynamic components

## Metadata Management
- Each file has associated metadata:
  - Creation/modification timestamps
  - Execution statistics and performance metrics
  - Dependencies and relationship information
  - Access permissions and security context

## File System Events
- File creation/modification triggers component registration
- Directory changes update SmartLibrary registry
- Execution events logged to SmartMemory
```

### Virtual Directories

#### /bin - Executable Tools
```markdown
## Dynamic Tool Mapping
- Auto-populated with tools from SmartLibrary
- Each tool appears as executable file
- Tool parameters mapped to command-line arguments
- Real-time tool generation creates new entries

## Tool Execution
llmunix$ /bin/webscrape --url "example.com" --format markdown
llmunix$ /bin/summarize --input "document.txt" --length 100
llmunix$ /bin/translate --text "hello" --target spanish
```

#### /proc - Process Information
```markdown
## Active Processes
/proc/[pid]/status     # Process execution state
/proc/[pid]/memory     # Memory usage and SmartMemory access
/proc/[pid]/tools      # Tools used by process
/proc/[pid]/output     # Process output and results

## System Information
/proc/cpuinfo          # LLM model information and capabilities
/proc/meminfo          # SmartMemory usage and optimization
/proc/version          # LLMunix version and build information
```

#### /dev - Device Interfaces
```markdown
## Virtual Devices
/dev/web               # Web interface device (WebFetch tool)
/dev/bash              # Shell interface device (Bash tool)
/dev/memory            # SmartMemory device interface
/dev/null              # Null device for output redirection
/dev/random            # Random data generator for testing

## Device Operations
echo "query" > /dev/web/search    # Web search operation
cat /dev/memory/patterns          # Read learned patterns
ls /dev/bash/history              # Command history
```

### File System Commands

#### Basic Commands
```bash
# File operations
llmunix$ ls /components/tools/
llmunix$ cat /system/SmartMemory.md
llmunix$ mkdir /workspace/project1
llmunix$ cp /components/tools/WebFetch.md /workspace/

# Save operations
llmunix$ save content.txt                    # Save to workspace with smart naming
llmunix$ save --file "report.md" --content "..." # Explicit save with filename
llmunix$ autosave                           # Auto-save current context/output
llmunix$ backup /workspace/important.md     # Create backup copy

# Search operations
llmunix$ find /components -name "*Agent*"
llmunix$ grep "function" /components/tools/
llmunix$ locate "summarization"

# System information
llmunix$ df -h                    # Show SmartMemory usage
llmunix$ ps aux                   # Show active processes
llmunix$ top                      # Show system performance
```

#### Advanced Commands
```bash
# Tool management
llmunix$ tool generate "pdf converter" --requirements "fast, secure"
llmunix$ tool install CustomAgent.md
llmunix$ tool update WebFetchTool --version latest

# Component operations
llmunix$ component list --type tools
llmunix$ component test SummarizationAgent
llmunix$ component optimize --target performance

# Memory operations
llmunix$ memory clean --threshold 0.1
llmunix$ memory backup /var/backup/
llmunix$ memory analyze --pattern usage
```

### File System Security

#### Permission Model
```markdown
## Access Control
- Read (r): Can read component definition
- Write (w): Can modify component
- Execute (x): Can run component as tool/agent
- Generate (g): Can create new components

## Permission Examples
-rwxg----- SystemAgent.md        # Full access for system
-rwx------ UserTool.md           # User-created tool
-r--r--r-- PublicAgent.md        # Read-only shared agent
```

#### Security Features
```markdown
## Sandboxing
- Each component runs in isolated execution context
- Limited file system access based on declared requirements
- Network access controlled through /dev/web interface

## Validation
- Component syntax validation on write operations
- Security scanning for malicious patterns
- Automatic backup before modification operations

## Audit Trail
- All file operations logged to /var/log/filesystem.log
- Component execution tracked in SmartMemory
- Access attempts recorded with user context
```

### Performance Optimization

#### Caching Strategy
```markdown
## Component Cache
- Frequently used components cached in memory
- Intelligent prefetching based on usage patterns
- Automatic cache invalidation on component updates

## Metadata Indexing
- Full-text search index for component content
- Dependency graph for relationship queries
- Performance metrics index for optimization
```

#### File System Events
```markdown
## Event Handling
- Real-time file system change notifications
- Automatic SmartLibrary updates on component changes
- Trigger-based optimization and cleanup operations

## Background Services
- Periodic optimization of component definitions
- Automatic backup and version control
- Performance monitoring and alerting
```

### Integration Points

#### Claude Code Tool Integration
```markdown
## Tool Mapping
- Read → fs_read()
- Write/Edit → fs_write()
- LS/Glob → fs_list()
- Grep → fs_search()
- Bash → fs_execute()

## Error Handling
- File not found → Generate component if possible
- Permission denied → Request access escalation
- Tool execution failure → Fallback to simulation mode
```

#### SmartMemory Integration
```markdown
## Memory-Backed File System
- File metadata stored in SmartMemory
- Access patterns used for optimization
- Learning from user behavior for predictive operations

## Intelligent Features
- Auto-completion based on learned patterns
- Predictive file suggestions
- Automated organization and cleanup
```


================================================
File: system/LLMunixKernel.md
================================================
# LLMunix Kernel

## Core Architecture

LLMunix is an AI-powered operating system that dynamically generates tools and commands using the LLM-OS framework structure. The kernel operates as an intelligent orchestrator that can create, execute, and manage system resources on-demand.

### Kernel Components

#### 1. Dynamic Tool Generator (DTG)
- Analyzes user intent and system state
- Generates tool definitions in markdown format
- Registers tools in SmartLibrary with appropriate metadata
- Maps tools to Claude Code native functions

#### 2. Memory Controller
- Manages SmartMemory for persistent learning
- Tracks execution patterns and optimization opportunities
- Maintains system state across sessions
- Implements garbage collection for unused tools

#### 3. Process Manager
- Tracks execution states using ExecutionStateTemplate
- Manages concurrent tool execution
- Handles error recovery and rollback
- Maintains process hierarchy and dependencies

#### 4. File System Interface
- Uses LLM-OS folder structure as native file system
- Maps virtual paths to component definitions
- Provides CRUD operations on system components
- Maintains version control for component evolution

### System Architecture

```
LLMunix Kernel
├── Boot Loader
│   ├── Initialize SmartMemory
│   ├── Load SmartLibrary registry
│   └── Start command interpreter
├── Core Services
│   ├── Dynamic Tool Generator
│   ├── Memory Controller
│   ├── Process Manager
│   └── File System Interface
├── System Calls
│   ├── generate_tool(intent, requirements)
│   ├── execute_command(command, args)
│   ├── manage_memory(operation, target)
│   └── file_operation(path, operation, data)
└── Shell Interface
    ├── Command Parser
    ├── Tool Resolver
    └── Output Formatter
```

### Boot Sequence

1. **System Initialization**
   - Load system/SmartMemory.md for persistent state
   - Parse system/SmartLibrary.md for available components
   - Initialize workspace/ as active execution environment

2. **Service Startup**
   - Start Dynamic Tool Generator with Claude Code tool mappings
   - Initialize Memory Controller with learning algorithms
   - Launch Process Manager with state tracking
   - Mount File System Interface with component registry

3. **Shell Ready**
   - Present LLMunix prompt
   - Accept user commands and intents
   - Begin dynamic tool generation and execution

### Dynamic Tool Generation Process

1. **Intent Analysis**
   - Parse user command or natural language request
   - Identify required capabilities and constraints
   - Check SmartLibrary for existing components

2. **Tool Synthesis**
   - Generate tool definition in markdown format
   - Map to appropriate Claude Code native tools
   - Define input/output specifications and error handling

3. **Registration and Execution**
   - Register tool in SmartLibrary with metadata
   - Create execution state in workspace/
   - Execute tool and capture results

4. **Learning and Optimization**
   - Record execution patterns in SmartMemory
   - Update tool definitions based on performance
   - Optimize for future similar requests

### System Calls Interface

```markdown
## generate_tool(intent, requirements)
- **Purpose**: Create new tool based on user intent
- **Input**: Natural language intent, technical requirements
- **Output**: Tool definition and registration status
- **Example**: generate_tool("monitor CPU usage", {"real_time": true, "format": "json"})

## execute_command(command, args)
- **Purpose**: Execute system command or user-defined tool
- **Input**: Command name and arguments
- **Output**: Execution results and state updates
- **Example**: execute_command("webscrape", {"url": "example.com", "format": "markdown"})

## manage_memory(operation, target)
- **Purpose**: Control SmartMemory operations
- **Input**: Operation type (read, write, optimize, clear), target data
- **Output**: Memory state and operation results
- **Example**: manage_memory("optimize", "execution_patterns")

## file_operation(path, operation, data)
- **Purpose**: File system operations on LLM-OS structure
- **Input**: Virtual path, operation type, data payload
- **Output**: File operation results and system state
- **Example**: file_operation("/components/tools/", "create", tool_definition)
```

### Resource Management

#### Memory Management
- **SmartMemory**: Persistent learning and pattern recognition
- **Execution Cache**: Temporary storage for active processes
- **Component Registry**: Tool and agent definitions with metadata
- **State Snapshots**: Rollback points for error recovery

#### Process Scheduling
- **Priority Queue**: High-priority system operations first
- **Dependency Resolution**: Ensure required components are available
- **Concurrent Execution**: Parallel tool execution where possible
- **Error Handling**: Graceful degradation and recovery

#### File System
- **Virtual File System**: LLM-OS structure as native file system
- **Component Versioning**: Track evolution of tools and agents
- **Access Control**: Manage permissions for system components
- **Backup and Recovery**: Automatic snapshots of critical state

### Security Model

#### Sandboxing
- Each generated tool runs in isolated execution context
- Limited access to system resources based on declared requirements
- Automatic permission escalation requests for sensitive operations

#### Validation
- Generated tools undergo syntax and safety validation
- Runtime monitoring for unexpected behavior
- Automatic rollback for failed or dangerous operations

#### Audit Trail
- Complete execution history in SmartMemory
- Tool generation and modification logs
- Performance metrics and resource usage tracking

### Extension Points

#### Custom Tool Templates
- User-defined templates for common tool patterns
- Template inheritance and composition
- Automatic optimization based on usage patterns

#### Plugin Architecture
- External component integration
- API endpoints for third-party tools
- Managed plugin lifecycle and updates

#### Learning Algorithms
- Reinforcement learning for tool optimization
- Pattern recognition for predictive tool generation
- User behavior modeling for personalized experience


================================================
File: system/LLMunixMemoryManager.md
================================================
# LLMunix Memory Manager

## Intelligent Memory Architecture

LLMunix Memory Manager extends SmartMemory with advanced AI-driven memory management, providing persistent learning, adaptive optimization, and intelligent resource allocation.

### Memory Hierarchy

```
LLMunix Memory System
├── L1 Cache (Active Context)
│   ├── Current execution state
│   ├── Active tool definitions
│   └── Immediate command history
├── L2 Cache (Session Memory)
│   ├── Tool usage patterns
│   ├── Optimization metrics
│   └── Error recovery data
├── L3 Storage (SmartMemory)
│   ├── Long-term learning patterns
│   ├── Component evolution history
│   └── System performance data
└── Persistent Storage
    ├── Component definitions
    ├── Execution logs
    └── Backup snapshots
```

### Memory Management Services

#### 1. Adaptive Learning Engine
```markdown
## Pattern Recognition
- Analyzes user command patterns and preferences
- Identifies frequently used tool combinations
- Learns optimal execution sequences for common tasks
- Predicts user intent based on context and history

## Performance Optimization
- Tracks tool execution performance metrics
- Identifies bottlenecks and optimization opportunities
- Automatically generates improved tool variants
- Learns from both successful and failed executions

## Context Awareness
- Maintains awareness of current project context
- Adapts tool suggestions based on active workspace
- Learns domain-specific patterns and terminology
- Provides contextual help and suggestions
```

#### 2. Intelligent Caching System
```markdown
## Multi-Level Caching
- L1: Immediate context (current command session)
- L2: Session-based cache (current work session)
- L3: Long-term patterns (persistent across sessions)
- Predictive: Pre-loaded based on learned patterns

## Cache Optimization
- Automatic cache sizing based on available resources
- Intelligent eviction policies using access patterns
- Prefetching based on predictive algorithms
- Compression for long-term storage efficiency

## Cache Coherence
- Automatic invalidation on component updates
- Dependency tracking for related components
- Version control integration for consistency
- Real-time synchronization across processes
```

#### 3. Memory Allocation Manager
```markdown
## Resource Allocation
- Dynamic memory allocation for tool execution
- Process isolation and memory protection
- Automatic garbage collection for unused resources
- Memory pool management for optimal performance

## Memory Monitoring
- Real-time memory usage tracking
- Performance impact analysis
- Resource leak detection and prevention
- Automatic optimization recommendations

## Memory Recovery
- Automatic recovery from memory errors
- Graceful degradation under memory pressure
- Emergency cleanup procedures
- State preservation during recovery
```

### Memory Operations

#### Core Memory Functions
```markdown
## memory_store(key, value, metadata)
- **Purpose**: Store data with intelligent categorization
- **Input**: Key identifier, data value, metadata context
- **Output**: Storage confirmation and retrieval key
- **Features**: Automatic compression, deduplication, indexing

## memory_retrieve(query, context)
- **Purpose**: Intelligent data retrieval with context awareness
- **Input**: Query pattern, contextual information
- **Output**: Relevant data with confidence scores
- **Features**: Fuzzy matching, contextual ranking, predictive results

## memory_analyze(pattern, timeframe)
- **Purpose**: Analyze patterns and trends in memory data
- **Input**: Analysis pattern, time range for analysis
- **Output**: Insights, trends, and optimization recommendations
- **Features**: Statistical analysis, machine learning insights

## memory_optimize(target, constraints)
- **Purpose**: Optimize memory usage and performance
- **Input**: Optimization target, resource constraints
- **Output**: Optimization results and performance improvements
- **Features**: Automatic reorganization, predictive optimization
```

#### Advanced Memory Features
```markdown
## Semantic Memory
- Contextual understanding of stored information
- Relationship mapping between components and concepts
- Natural language querying of memory contents
- Intelligent summarization and abstraction

## Temporal Memory
- Time-based pattern recognition and analysis
- Historical trend analysis and prediction
- Automatic archiving of outdated information
- Timeline-based memory retrieval

## Associative Memory
- Cross-reference related information automatically
- Discover hidden patterns and relationships
- Suggest related tools and components
- Build knowledge graphs of system usage
```

### Learning Algorithms

#### 1. Reinforcement Learning
```markdown
## Tool Usage Optimization
- Learns optimal tool selection for specific tasks
- Adapts to user preferences and work patterns
- Improves execution efficiency over time
- Balances exploration of new tools with exploitation of proven ones

## Parameter Optimization
- Learns optimal parameters for tool execution
- Adapts to different contexts and requirements
- Minimizes execution time and resource usage
- Maximizes output quality and relevance
```

#### 2. Pattern Mining
```markdown
## Command Pattern Analysis
- Identifies common command sequences
- Discovers workflow patterns and shortcuts
- Suggests automation opportunities
- Creates custom command aliases and macros

## Error Pattern Recognition
- Learns from execution failures and errors
- Identifies common error scenarios and solutions
- Builds error prevention strategies
- Provides proactive error warnings and suggestions
```

#### 3. Predictive Analytics
```markdown
## Intent Prediction
- Predicts next likely commands based on context
- Suggests relevant tools and resources
- Provides contextual help and documentation
- Optimizes system preparation for likely tasks

## Resource Prediction
- Predicts resource requirements for tasks
- Pre-allocates resources for optimal performance
- Identifies potential resource conflicts
- Suggests resource optimization strategies
```

### Memory-Backed Services

#### 1. Smart Auto-Completion
```markdown
## Context-Aware Completion
- Suggests commands based on current context
- Learns from user correction patterns
- Provides parameter suggestions and validation
- Offers alternative command options

## Intelligent History
- Semantic search through command history
- Contextual history filtering and ranking
- Automatic command categorization
- History-based workflow suggestions
```

#### 2. Adaptive Help System
```markdown
## Personalized Documentation
- Customizes help content based on user experience
- Provides examples relevant to current context
- Learns which explanations are most helpful
- Updates documentation based on usage patterns

## Progressive Disclosure
- Shows information complexity based on user level
- Gradually introduces advanced features
- Adapts to user learning pace and preferences
- Provides just-in-time learning opportunities
```

#### 3. Intelligent Error Recovery
```markdown
## Error Context Analysis
- Analyzes error conditions and system state
- Provides contextual error explanations
- Suggests specific remediation steps
- Learns from successful error resolutions

## Automatic Recovery
- Implements learned recovery procedures
- Provides rollback to known good states
- Suggests alternative approaches
- Prevents similar errors in the future
```

### Memory Commands

#### Basic Memory Operations
```bash
# Memory inspection
llmunix$ memory status                    # Show memory usage and health
llmunix$ memory patterns                  # Display learned patterns
llmunix$ memory search "web scraping"     # Semantic search of memory

# Memory management  
llmunix$ memory optimize --target speed   # Optimize for performance
llmunix$ memory clean --age 30d          # Clean old unused data
llmunix$ memory backup /var/backup/      # Create memory backup

# Learning control
llmunix$ memory learn --enable           # Enable learning mode
llmunix$ memory forget --pattern "test*" # Remove specific patterns
llmunix$ memory export --format json     # Export learned data
```

#### Advanced Memory Operations
```bash
# Pattern analysis
llmunix$ memory analyze --timeframe 7d   # Analyze recent patterns
llmunix$ memory predict --context current # Predict likely next actions
llmunix$ memory suggest --task "data analysis" # Get tool suggestions

# Memory debugging
llmunix$ memory debug --verbose          # Show detailed memory state  
llmunix$ memory trace --command last     # Trace memory access patterns
llmunix$ memory profile --duration 60s   # Profile memory performance
```

### Performance Metrics

#### Memory Performance Indicators
```markdown
## Cache Performance
- Hit ratio for different cache levels
- Average retrieval time by data type
- Cache efficiency and optimization metrics
- Memory fragmentation and utilization

## Learning Effectiveness
- Pattern recognition accuracy
- Prediction success rates
- User satisfaction scores
- System adaptation speed

## Resource Utilization
- Memory usage optimization
- Garbage collection efficiency
- Storage compression ratios
- Access pattern optimization
```

#### Monitoring and Alerting
```markdown
## Real-time Monitoring
- Memory usage dashboards
- Performance trend analysis
- Anomaly detection and alerting
- Capacity planning recommendations

## Health Checks
- Memory consistency validation
- Index integrity verification
- Learning algorithm performance
- Data quality assessment
```

### Integration with System Components

#### SmartMemory Integration
```markdown
## Extended SmartMemory
- Preserves existing SmartMemory interface
- Adds AI-driven learning capabilities
- Provides backward compatibility
- Enhances with predictive features

## Data Migration
- Automatic migration from basic SmartMemory
- Preservation of existing patterns and data
- Enhanced indexing and search capabilities
- Improved storage efficiency
```

#### Kernel Integration
```markdown
## Memory-Aware Scheduling
- Optimizes process scheduling based on memory patterns
- Provides memory context to tool execution
- Implements memory-aware resource allocation
- Supports memory-driven optimization

## System State Management
- Maintains comprehensive system state in memory
- Provides rollback capabilities for system operations
- Supports distributed execution state management
- Enables recovery from catastrophic failures
```


================================================
File: system/LLMunixProcessManager.md
================================================
# LLMunix Process Manager

## Intelligent Process Orchestration System

LLMunix Process Manager provides advanced process management with AI-driven optimization, intelligent scheduling, and execution state tracking integrated with the LLM-OS framework.

### Process Architecture

```
Process Management Hierarchy
├── Process Scheduler
│   ├── Priority Queue Manager
│   ├── Resource Allocator
│   ├── Dependency Resolver
│   └── Load Balancer
├── Execution Engine
│   ├── Tool Runtime Environment
│   ├── Agent Execution Context
│   ├── State Machine Controller
│   └── Error Recovery System
├── State Tracking
│   ├── Execution State Manager
│   ├── Progress Monitoring
│   ├── Performance Metrics
│   └── Audit Trail
└── Inter-Process Communication
    ├── Message Passing
    ├── Shared Memory
    ├── Event System
    └── Synchronization
```

### Core Process Types

#### 1. Tool Processes
```markdown
## Tool Execution Process
- **Type**: Atomic execution unit for individual tools
- **Lifecycle**: Initialize → Execute → Cleanup → Terminate
- **State**: Pending → Running → Completed/Failed/Cancelled
- **Resources**: Memory allocation, CPU time, I/O access
- **Context**: Execution parameters, input data, output handling

## Process Structure
PID: 001
├── Tool: WebFetchTool
├── State: Running
├── Priority: Normal
├── Resources: 156MB memory, 12% CPU
├── Progress: 67% (2 of 3 URLs processed)
├── Runtime: 00:02:34
├── Owner: user@llmunix
└── Context: /workspace/research-project/
```

#### 2. Agent Processes
```markdown
## Intelligent Agent Process
- **Type**: Complex reasoning and decision-making process
- **Lifecycle**: Initialize → Reasoning → Action → Learning → Terminate
- **State**: Idle → Thinking → Acting → Learning → Completed
- **Resources**: Extended memory, persistent context, tool access
- **Context**: Goal definition, reasoning chain, action history

## Agent Process Example
PID: 002
├── Agent: SummarizationAgent
├── State: Thinking (analyzing content structure)
├── Priority: High
├── Resources: 245MB memory, 8% CPU
├── Reasoning: Content analysis phase 2/4
├── Tools Used: [TextAnalyzer, StatisticsGenerator]
├── Learning: Updating summarization patterns
└── Context: Document analysis task
```

#### 3. Workflow Processes
```markdown
## Multi-Step Workflow Process
- **Type**: Orchestration of multiple tools and agents
- **Lifecycle**: Plan → Execute Steps → Monitor → Complete
- **State**: Planning → Executing → Monitoring → Completed
- **Resources**: Aggregate of constituent processes
- **Context**: Workflow definition, step dependencies, rollback points

## Workflow Process Example
PID: 003
├── Workflow: Web Research Pipeline
├── State: Executing (Step 2 of 4)
├── Priority: Normal
├── Steps:
│   ├── [✓] WebFetch: Completed (2.3s)
│   ├── [◐] Summarize: Running (45%)
│   ├── [ ] Analyze: Pending
│   └── [ ] Report: Pending
├── Resources: 389MB total, 15% CPU
└── ETA: 00:03:45 remaining
```

### Process Scheduling

#### 1. Priority-Based Scheduling
```markdown
## Priority Levels
- **Critical**: System maintenance, error recovery
- **High**: User-initiated interactive commands
- **Normal**: Regular tool execution, background tasks
- **Low**: Optimization, cleanup, learning processes
- **Idle**: Non-essential maintenance when system idle

## Scheduling Algorithm
- Round-robin within priority levels
- Preemptive for higher priority processes
- Fair scheduling with aging prevention
- Resource-aware scheduling based on availability
- Learning-based optimization of scheduling decisions
```

#### 2. Resource-Aware Scheduling
```markdown
## Resource Monitoring
- Memory usage tracking and prediction
- CPU utilization monitoring
- I/O bandwidth management
- Network resource allocation
- SmartMemory access coordination

## Intelligent Allocation
- Predictive resource allocation based on tool history
- Dynamic resource adjustment during execution
- Resource conflict detection and resolution
- Automatic resource scaling and optimization
- Memory pressure handling and process migration
```

#### 3. Dependency Management
```markdown
## Dependency Types
- **Tool Dependencies**: Required tools and libraries
- **Data Dependencies**: Input files and data sources
- **Resource Dependencies**: Memory, CPU, network access
- **Temporal Dependencies**: Execution order requirements
- **State Dependencies**: System state requirements

## Dependency Resolution
- Automatic dependency graph construction
- Circular dependency detection and handling
- Dynamic dependency satisfaction
- Parallel execution optimization
- Dependency caching and preloading
```

### Execution State Management

#### 1. State Tracking System
```markdown
## Execution State Structure
{
  "process_id": "001",
  "tool_name": "WebFetchTool",
  "state": "running",
  "progress": {
    "current_step": 2,
    "total_steps": 3,
    "percentage": 67,
    "eta": "00:01:20"
  },
  "resources": {
    "memory_mb": 156,
    "cpu_percent": 12.3,
    "io_operations": 47
  },
  "context": {
    "working_directory": "/workspace/research/",
    "input_parameters": {...},
    "intermediate_results": {...}
  },
  "history": [
    {"timestamp": "14:30:15", "event": "started", "details": "..."},
    {"timestamp": "14:30:18", "event": "progress", "details": "URL 1 completed"},
    {"timestamp": "14:32:34", "event": "progress", "details": "URL 2 in progress"}
  ]
}
```

#### 2. State Persistence
```markdown
## Persistent State Storage
- Automatic state snapshots at key execution points
- Recovery state for interrupted processes
- State version control for rollback capability
- Distributed state management for complex workflows
- State compression and optimization for long-running processes

## State Recovery
- Automatic recovery from unexpected termination
- Rollback to previous stable state
- Partial state recovery for failed processes
- State migration for process optimization
- Manual state intervention and correction
```

#### 3. Progress Monitoring
```markdown
## Real-Time Progress Tracking
- Step-by-step execution monitoring
- Percentage completion estimation
- ETA calculation based on historical data
- Resource usage trend analysis
- Performance bottleneck identification

## Progress Reporting
- Real-time progress updates in shell
- Background process status notifications
- Progress history and trend analysis
- Performance comparison with previous runs
- Predictive completion time estimation
```

### Process Communication

#### 1. Inter-Process Communication (IPC)
```markdown
## Message Passing
- Asynchronous message queues between processes
- Typed message system for different data types
- Message routing and delivery guarantees
- Message persistence for critical communications
- Message encryption for sensitive data

## Shared Memory
- Efficient data sharing between related processes
- Memory-mapped files for large data sets
- Synchronization primitives for concurrent access
- Garbage collection for shared memory regions
- Security isolation between process spaces
```

#### 2. Event System
```markdown
## Process Events
- Process lifecycle events (start, stop, pause, resume)
- State change events (progress updates, errors, completion)
- Resource events (memory allocation, I/O operations)
- System events (shutdown, optimization, maintenance)
- Custom events for tool-specific notifications

## Event Handling
- Event subscription and filtering
- Event aggregation and batching
- Event persistence and replay
- Event-driven process coordination
- Event analytics and pattern recognition
```

#### 3. Synchronization
```markdown
## Synchronization Primitives
- Mutexes for exclusive resource access
- Semaphores for resource counting
- Condition variables for event waiting
- Barriers for process synchronization
- Read-write locks for shared data structures

## Coordination Patterns
- Producer-consumer patterns for data processing
- Leader-follower patterns for task distribution
- Pipeline patterns for sequential processing
- MapReduce patterns for parallel processing
- Event-driven patterns for reactive systems
```

### Process Monitoring and Debugging

#### 1. Performance Monitoring
```markdown
## Metrics Collection
- CPU usage per process and system-wide
- Memory allocation and deallocation patterns
- I/O operations and bandwidth utilization
- Network requests and response times
- Tool execution times and success rates

## Performance Analysis
- Real-time performance dashboards
- Historical performance trend analysis
- Performance bottleneck identification
- Resource utilization optimization
- Predictive performance modeling
```

#### 2. Debug and Profiling
```markdown
## Debugging Features
- Process state inspection and modification
- Execution trace recording and playback
- Breakpoint setting and conditional debugging
- Memory dump analysis and leak detection
- Tool parameter validation and testing

## Profiling Tools
- CPU profiling with hotspot identification
- Memory profiling with allocation tracking
- I/O profiling with bottleneck analysis
- Network profiling with latency measurement
- Tool profiling with optimization suggestions
```

#### 3. Logging and Auditing
```markdown
## Comprehensive Logging
- Structured logging with JSON format
- Log levels (DEBUG, INFO, WARN, ERROR, CRITICAL)
- Context-aware logging with process correlation
- Log rotation and archival policies
- Log analysis and alerting systems

## Audit Trail
- Complete process execution history
- Command and parameter logging
- Resource usage tracking
- Error and exception logging
- Security event logging and monitoring
```

### Process Management Commands

#### 1. Basic Process Operations
```bash
# List processes
llmunix$ ps
PID   TOOL                 STATE     RUNTIME  MEMORY  CPU%
001   WebFetchTool         Running   00:02:34  156MB   12.3
002   SummarizationAgent   Thinking  00:00:45   89MB    8.7
003   WebResearchWorkflow  Planning  00:00:12  245MB    4.2

# Detailed process information
llmunix$ ps -l 001
Process 001 - WebFetchTool
├── State: Running (fetching URL 2 of 3)
├── Priority: Normal
├── Resources: 156MB memory, 12.3% CPU
├── Progress: 67% complete
├── Runtime: 00:02:34 (ETA: 00:01:20)
├── Context: /workspace/research-project/
├── Input: ["https://example1.com", "https://example2.com", "https://example3.com"]
├── Output: /workspace/research-project/content/
└── Performance: 1.2s avg response time, 95% success rate

# Process control
llmunix$ kill 001                    # Terminate process
llmunix$ pause 001                   # Pause process execution
llmunix$ resume 001                  # Resume paused process
llmunix$ priority 001 high           # Change process priority
```

#### 2. Advanced Process Management
```bash
# Process monitoring
llmunix$ top -p processes
Real-time Process Monitor:
PID   TOOL                STATE     CPU%  MEM   PROGRESS
001   WebFetchTool        Running   12.3  156M  67% (↑)
002   SummarizationAgent  Thinking   8.7   89M  Processing
003   FileAnalyzer        Idle       0.5   23M  Waiting

# Process debugging
llmunix$ debug 001
Debugging Process 001 (WebFetchTool):
├── Current State: Running
├── Stack Trace: webfetch() → http_request() → socket_read()
├── Variables: url="https://example2.com", timeout=30s
├── Memory: 156MB allocated, 134MB used
└── Debug Commands: [step, continue, inspect, modify]

# Process profiling
llmunix$ profile 001 --duration 60s
Profiling Process 001 for 60 seconds...
├── CPU Usage: 12.3% average, 23.5% peak
├── Memory: 156MB average, 178MB peak
├── I/O Operations: 47 reads, 12 writes
├── Network: 3 active connections, 1.2MB transferred
└── Hotspots: http_request() 45%, json_parse() 23%
```

#### 3. Workflow Management
```bash
# Workflow execution
llmunix$ workflow run research-pipeline --input "https://news.example.com"
Starting workflow: research-pipeline
├── Step 1: WebFetch → Queued
├── Step 2: Summarize → Waiting
├── Step 3: Analyze → Waiting
└── Step 4: Report → Waiting

# Workflow monitoring
llmunix$ workflow status research-pipeline
Workflow: research-pipeline (PID: 003)
├── State: Executing (Step 2 of 4)
├── Progress: 45% complete
├── Runtime: 00:03:22 (ETA: 00:04:15)
├── Steps:
│   ├── [✓] WebFetch: Completed (2.3s, 100%)
│   ├── [◐] Summarize: Running (45%, 00:01:30)
│   ├── [ ] Analyze: Pending
│   └── [ ] Report: Pending
└── Resources: 389MB memory, 15% CPU

# Workflow control
llmunix$ workflow pause research-pipeline   # Pause workflow
llmunix$ workflow resume research-pipeline  # Resume workflow
llmunix$ workflow retry research-pipeline --step 2  # Retry failed step
```

### Error Handling and Recovery

#### 1. Error Detection
```markdown
## Error Types
- **Tool Errors**: Tool execution failures and exceptions
- **Resource Errors**: Memory exhaustion, disk space, network failures
- **Dependency Errors**: Missing dependencies, version conflicts
- **Configuration Errors**: Invalid parameters, missing configuration
- **System Errors**: Kernel panics, hardware failures, external service failures

## Error Detection Mechanisms
- Real-time error monitoring and alerting
- Predictive error detection based on system patterns
- Resource threshold monitoring and early warning
- Dependency validation and conflict detection
- Health checks and system validation
```

#### 2. Recovery Strategies
```markdown
## Automatic Recovery
- Process restart with exponential backoff
- State restoration from checkpoints
- Resource reallocation and optimization
- Alternative tool selection and fallback
- Degraded mode operation for critical functions

## Manual Recovery
- Interactive error diagnosis and resolution
- Manual state correction and intervention
- Process migration and resource reallocation
- System repair and maintenance procedures
- Data recovery and consistency checks
```

#### 3. Resilience Features
```markdown
## Fault Tolerance
- Process isolation and containment
- Graceful degradation under failure conditions
- Redundant execution for critical processes
- Circuit breaker patterns for external dependencies
- Chaos engineering for resilience testing

## High Availability
- Process clustering and load distribution
- Automatic failover and recovery
- Hot standby processes for critical functions
- Data replication and consistency maintenance
- Zero-downtime updates and maintenance
```


================================================
File: system/LLMunixShell.md
================================================
# LLMunix Shell Interface

## Interactive AI-Powered Shell

LLMunix Shell provides an intelligent, conversational command-line interface that bridges natural language interaction with powerful system operations and dynamic tool generation.

### Shell Features

#### 1. Intelligent Prompt System
```bash
# Adaptive prompt showing context
user@llmunix:/workspace/project1$ 
user@llmunix:/workspace/project1[memory: 75% tools: 12 active]$ 
user@llmunix:/components/tools[dev-mode]$ 

# Status indicators
[✓] - Last command successful
[!] - Warning or attention needed  
[✗] - Last command failed
[?] - Suggestion available
[◐] - Background process running
```

#### 2. Multi-Modal Input Support
```bash
# Traditional command input
llmunix$ ls -la /components/

# Natural language input
llmunix$ show me all the tools in the components directory

# Hybrid input (natural + technical)
llmunix$ "find Python files with database code" --exclude test*

# Conversational input
llmunix$ I need help with file permissions
```

#### 3. Intelligent Auto-Completion
```bash
# Context-aware completion
llmunix$ web<TAB>
webscrape    webfetch    webanalyze    webmonitor

# Parameter completion with examples
llmunix$ webscrape --format <TAB>
markdown    json    html    text
> webscrape --format markdown  # Most common for your usage

# File path completion with metadata
llmunix$ cat /components/<TAB>
tools/           [12 tools, 3 recently used]
agents/          [8 agents, 1 active]
scenarios/       [5 scenarios, last run: 2h ago]
```

### Built-in Commands

#### 1. System Information Commands
```bash
# System status
llmunix$ status
LLMunix v1.0 - AI Operating System
Memory: 2.1GB used, 85% efficient
Tools: 28 loaded, 3 active (includes WorkspaceSaveTool)
Processes: 7 running, 2 background
SmartMemory: 1,247 patterns learned
Workspace: /workspace/ (15 files, 2.3MB used)

# Performance monitoring
llmunix$ top
PID   TOOL                CPU%  MEM%   STATUS
001   WebScrapeTool       12.3  156MB  Running
002   SummarizationAgent   8.7  89MB   Processing
003   FileAnalyzer         2.1  23MB   Idle

# Memory diagnostics
llmunix$ memory
Total Memory: 2.1GB
├── Active Context: 245MB (12%)
├── Tool Cache: 890MB (42%)
├── SmartMemory: 567MB (27%)
└── System: 398MB (19%)

Learning Status: 1,247 patterns, 89% accuracy
Cache Hit Rate: 78% (last hour)
Optimization: +23% efficiency vs. yesterday
```

#### 2. Tool Management Commands
```bash
# List available tools
llmunix$ tools list
Available Tools (24):
[REAL] WebFetchTool          - Live web content retrieval
[REAL] FileSystemTool        - File operations and management  
[REAL] SummarizationAgent    - Content analysis and summarization
[GEN]  PDFMergerTool         - PDF file operations (generated 2h ago)
[GEN]  LogAnalyzerTool       - Log file analysis (generated yesterday)

# Tool information
llmunix$ tools info WebFetchTool
WebFetchTool - Live Web Content Retrieval
├── Type: [REAL] Claude Code Native
├── Status: Active, 15 uses today
├── Performance: 1.2s avg response, 95% success rate
├── Memory: 45MB cache, 234 patterns learned
└── Usage: webfetch --url <URL> [--format markdown|json|text]

# Generate new tool
llmunix$ tools generate "image optimizer" --features "batch processing, format conversion"
Generating ImageOptimizerTool...
├── Analyzing requirements: batch processing, format conversion
├── Mapping to Claude Code tools: Read, Write, Bash
├── Creating tool definition: /components/tools/ImageOptimizerTool.md
├── Registering in SmartLibrary with [GEN] tag
└── Tool available as: imageoptimize

# Tool optimization
llmunix$ tools optimize WebFetchTool
Analyzing WebFetchTool performance...
├── Found 3 optimization opportunities:
│   ├── Cache frequently accessed URLs (+15% speed)
│   ├── Parallel processing for multiple URLs (+30% throughput)
│   └── Intelligent retry logic (+8% success rate)
└── Apply optimizations? [Y/n]
```

#### 3. Memory Management Commands
```bash
# Memory operations
llmunix$ memory clean
Cleaning memory caches...
├── Removed 15MB unused tool cache
├── Archived 234 old patterns to long-term storage
├── Optimized SmartMemory index
└── Freed 89MB total memory

# Memory analysis
llmunix$ memory analyze --timeframe 7d
Memory Usage Analysis (Last 7 Days):
├── Peak Usage: 2.8GB (yesterday 3:42 PM)
├── Average Usage: 1.9GB
├── Most Used Tools: WebFetchTool (234 calls), SummarizationAgent (156 calls)
├── Learning Rate: +47 new patterns
└── Efficiency Trend: ↑ 12% improvement

# Memory search
llmunix$ memory search "web scraping patterns"
Found 12 relevant patterns:
├── High confidence (8):
│   ├── JavaScript-heavy sites → use headless browser mode
│   ├── Rate limiting detected → implement backoff strategy
│   └── Dynamic content → wait for page load completion
└── Medium confidence (4):
    ├── Authentication required → handle session cookies
    └── Large files → use streaming download
```

#### 4. Process Management Commands
```bash
# List processes
llmunix$ ps
PID   TOOL                 STATUS    RUNTIME  MEMORY
001   WebScrapeTool        Running   00:02:34  156MB
002   SummarizationAgent   Idle      00:00:12   89MB
003   FileAnalyzer         Sleeping  00:45:23   23MB

# Process details
llmunix$ ps 001
Process 001 - WebScrapeTool
├── Status: Running (scraping https://example.com)
├── Runtime: 00:02:34
├── Memory: 156MB (134MB data, 22MB cache)
├── Progress: 67% complete (2 of 3 pages)
└── ETA: 00:01:20 remaining

# Kill process
llmunix$ kill 001
Terminating process 001 (WebScrapeTool)...
├── Saving intermediate results to /workspace/temp/
├── Cleaning up resources
└── Process terminated gracefully

# Background processes
llmunix$ jobs
[1]   Running    webfetch https://news.site --background
[2]   Running    optimize-tools --quiet
[3]   Stopped    memory analyze --deep
```

#### 5. File System Commands
```bash
# Enhanced file listing
llmunix$ ls -la /components/
total 24
drwxr-xr-x  4 user  staff   128 Jan 10 14:30 ./
drwxr-xr-x  8 user  staff   256 Jan 10 12:15 ../
drwxr-xr-x  13 user staff   416 Jan 10 14:30 tools/     [13 tools, includes WorkspaceSaveTool]
drwxr-xr-x  8 user  staff   256 Jan 10 13:45 agents/    [8 agents, 1 running]

# Workspace file listing
llmunix$ ls -la /workspace/
total 2048
drwxr-xr-x  6 user  staff   192 Jan 10 14:45 ./
drwxr-xr-x  8 user  staff   256 Jan 10 12:15 ../
-rw-r--r--  1 user  staff   0.8K Jan 10 14:15 example_com_homepage.md
-rw-r--r--  1 user  staff    45K Jan 10 14:10 ai_trends_report_2025_01_10.md
drwxr-xr-x  4 user  staff   128 Jan 10 14:10 scraped_content/
-rw-r--r--  1 user  staff   12K Jan 10 14:10 trend_data.json
drwxr-xr-x  2 user  staff    64 Jan 10 14:45 backups/

# Intelligent file search
llmunix$ find /components -name "*Web*" --type tool
/components/tools/WebFetchTool.md        [REAL] Last used: 2m ago
/components/tools/WebScrapeTool.md       [GEN]  Created: yesterday
/components/tools/WebAnalyzerTool.md     [GEN]  Created: 3d ago

# File analysis
llmunix$ file /components/tools/WebFetchTool.md
/components/tools/WebFetchTool.md: LLMunix Tool Definition
├── Type: [REAL] Claude Code Native
├── Size: 2.4KB
├── Last Modified: 2 hours ago
├── Dependencies: WebFetch (Claude Code)
├── Usage Count: 47 times
└── Performance: 1.2s avg, 95% success rate
```

### Interactive Features

#### 1. Conversational Interface
```bash
# Natural language queries
llmunix$ What's the best way to download a large file?
For large file downloads, I recommend:
├── Use WebFetchTool with streaming mode: --stream true
├── Enable resume capability: --resume true  
├── Set appropriate timeout: --timeout 300s
└── Consider background execution: --background

Would you like me to generate a specialized large file downloader tool?

# Help requests
llmunix$ I'm having trouble with file permissions
Let me help you with file permissions. What specific issue are you facing?
├── Can't read a file? Try: chmod +r filename
├── Can't execute a script? Try: chmod +x filename
├── Can't write to directory? Try: chmod +w dirname
└── Need to see current permissions? Try: ls -la filename

Type 'permissions help' for detailed guide or describe your specific situation.

# Error assistance
llmunix$ command not found: webscrap
Command 'webscrap' not found. Did you mean:
├── webscrape    (WebScrapeTool - web content extraction)
├── webfetch     (WebFetchTool - web content retrieval)
└── webanalyze   (WebAnalyzerTool - web content analysis)

Or would you like me to generate a 'webscrap' tool for you?
```

#### 2. Smart Suggestions
```bash
# Context-aware suggestions
llmunix$ cd /workspace/data-analysis/
llmunix[data-analysis]$ # Detected data analysis context
💡 Suggestions for data analysis:
├── 'tools list --category data' - Show data analysis tools
├── 'generate analyzer' - Create custom data analysis tool
└── 'memory search data patterns' - Find relevant patterns

# Workflow suggestions
llmunix$ webfetch https://api.example.com/data.json
✓ Data downloaded successfully (1.2MB JSON)
💡 Next steps you might want:
├── 'analyze data.json' - Analyze the JSON structure
├── 'convert json to csv' - Convert to spreadsheet format
└── 'visualize data' - Create charts and graphs

# Performance suggestions
llmunix$ # After running slow command
⚡ Performance tip: WebFetchTool took 15.3s (slower than usual)
├── Possible causes: Network latency, large file size
├── Suggestions: Use --cache true, --parallel 3
└── Alternative: Try 'webfetch --fast-mode' for basic content
```

#### 3. Learning Integration
```bash
# Adaptive behavior
llmunix$ # Shell learns from your patterns
📊 I noticed you often use 'webfetch' followed by 'summarize'
Would you like me to create a 'websummarize' command that combines both?

# Personal optimization
llmunix$ # After using tools repeatedly
🎯 Tool usage optimization:
├── SummarizationAgent: Your preferred length is 150 words
├── WebFetchTool: You usually want markdown format
└── FileAnalyzer: You typically exclude test files

Apply these as defaults? [Y/n]

# Pattern recognition
llmunix$ # Shell recognizes work patterns
🔄 Detected workflow pattern: web research
Common sequence: webfetch → summarize → save → analyze
Create a 'research' macro for this workflow? [Y/n]
```

### Shell Configuration

#### 1. Environment Variables
```bash
# LLMunix-specific variables
export LLMUNIX_MEMORY_LIMIT=4GB
export LLMUNIX_TOOL_CACHE=true
export LLMUNIX_LEARNING_MODE=adaptive
export LLMUNIX_SUGGESTION_LEVEL=medium
export LLMUNIX_PROMPT_STYLE=enhanced

# Tool-specific configuration
export WEBFETCH_DEFAULT_FORMAT=markdown
export SUMMARIZE_DEFAULT_LENGTH=150
export FILE_ANALYZER_EXCLUDE_PATTERNS="*.test.*,*.tmp"
```

#### 2. Shell Customization
```bash
# Prompt customization
llmunix$ prompt config
Current prompt: user@llmunix:/path$
Available styles:
├── minimal:  $ 
├── standard: user@llmunix:/path$ 
├── enhanced: user@llmunix:/path[context]$ 
└── verbose:  user@llmunix:/path[memory:75% tools:12]$ 

# Alias management
llmunix$ alias ws="webscrape --format markdown"
llmunix$ alias summary="summarize --length 100"
llmunix$ alias research="webfetch $1 && summarize"

# Function definitions
llmunix$ function quick_analysis() {
    echo "Analyzing $1..."
    file "$1" && analyze "$1" --quick
}
```

#### 3. History and Bookmarks
```bash
# Intelligent history
llmunix$ history
Recent commands:
├── webfetch https://example.com --format markdown  (2m ago) ✓
├── summarize content.md --length 150               (5m ago) ✓
├── tools generate "pdf merger"                     (1h ago) ✓
└── memory analyze --timeframe 7d                   (2h ago) ✓

# Semantic history search
llmunix$ history search "web scraping"
Found 8 commands related to web scraping:
├── webfetch https://news.site --format markdown (yesterday)
├── generate tool "news scraper" (2d ago)
└── webscrape --javascript true (3d ago)

# Bookmark management
llmunix$ bookmark add /workspace/current-project "Current Project"
llmunix$ bookmark list
Bookmarks:
├── proj: /workspace/current-project
├── tools: /components/tools/
└── logs: /var/log/llmunix/
```

### Shell Scripting

#### 1. LLMunix Script Language
```bash
#!/bin/llmunix

# Enhanced shell scripting with AI features
script research_workflow {
    param url required "URL to research"
    param format default="markdown" "Output format"
    
    echo "Starting research workflow for: $url"
    
    # Fetch content with error handling
    content = webfetch $url --format $format || {
        echo "Failed to fetch content from $url"
        suggest alternatives $url
        exit 1
    }
    
    # Analyze content
    summary = summarize $content --adaptive-length
    insights = analyze $content --type research
    
    # Save results
    save_results $summary $insights --timestamp
    
    echo "Research complete. Results saved to workspace."
    suggest next_steps $content
}
```

#### 2. Workflow Automation
```bash
# Automated workflows
llmunix$ workflow create "daily-backup" {
    schedule: "0 2 * * *"  # 2 AM daily
    commands: [
        "backup /workspace --incremental",
        "optimize memory --level 1", 
        "tools update --check-only",
        "report status --email admin@company.com"
    ]
}

# Conditional execution
llmunix$ if memory usage > 80%; then
    memory clean --aggressive
    notify "Memory cleaned due to high usage"
fi
```


================================================
File: system/SmartLibrary.md
================================================
# Smart Library Index

This file is the central registry for all components available to the SystemAgent. Components marked with [REAL] use Claude Code's native tools for actual execution.

## LLM-OS Real Components

---
-   **id**: `tool_real_web_fetch_v1`
-   **name**: RealWebFetchTool [REAL]
-   **file_path**: `components/tools/RealWebFetchTool.md`
-   **record_type**: REAL_TOOL
-   **claude_tool**: WebFetch
-   **domain**: data_acquisition
-   **description**: Fetches real, live content from web URLs using Claude Code's WebFetch capability.
-   **cost**: low ($0.001-0.01 per call)
-   **latency**: medium (2-10 seconds)
-   **side_effects**: "Network request to external server"
-   **version**: 1.0.0
-   **tags**: [web, fetch, http, real, claude-code]
-   **applicability_text**: "Use for fetching real, live web content. Replaces simulation-based web fetching with actual HTTP requests. Ideal for current data, news, articles, and dynamic content that cannot be mocked."

---
-   **id**: `tool_llm_interpreter_web_fetch_v1`
-   **name**: LLMInterpreterWebFetchTool [LLM_INTERPRETER]
-   **file_path**: `components/tools/LLMInterpreterWebFetchTool.md`
-   **record_type**: LLM_INTERPRETER_TOOL
-   **command_tool**: curl, wget
-   **domain**: data_acquisition
-   **description**: Fetches real web content using command-line tools (curl/wget) for LLM Interpreter runtime.
-   **cost**: none (uses local networking)
-   **latency**: medium (2-10 seconds)
-   **side_effects**: "Network request to external server"
-   **version**: 1.0.0
-   **tags**: [web, fetch, http, real, llm-interpreter, curl, wget]
-   **applicability_text**: "Use when running in LLM Interpreter runtime for real web content fetching. Provides actual HTTP requests using standard command-line tools available in Docker containers. Essential for internet access in LLM Interpreter environment."

---
-   **id**: `tool_real_filesystem_v1`
-   **name**: RealFileSystemTool [REAL]
-   **file_path**: `components/tools/RealFileSystemTool.md`
-   **record_type**: REAL_TOOL
-   **claude_tool**: Read, Write, Glob, LS
-   **domain**: file_system
-   **description**: Performs real file system operations using Claude Code's native file tools.
-   **cost**: none
-   **latency**: low (<100ms)
-   **side_effects**: "Creates/modifies/reads real files in workspace"
-   **version**: 1.0.0
-   **tags**: [file, read, write, search, real, claude-code]
-   **applicability_text**: "Use for all file operations including reading, writing, searching, and listing. Provides real file system access with workspace security boundaries. Essential for persistent data storage and file-based workflows."

---
-   **id**: `agent_real_summarizer_v1`
-   **name**: RealSummarizationAgent [REAL]
-   **file_path**: `components/agents/RealSummarizationAgent.md`
-   **record_type**: REAL_AGENT
-   **domain**: text_processing
-   **description**: Advanced summarization agent that reads real files and generates structured summaries using Claude Code's native capabilities.
-   **cost**: medium (depends on content length)
-   **latency**: medium (5-30 seconds)
-   **side_effects**: "May create intermediate analysis files"
-   **version**: 1.0.0
-   **tags**: [summary, nlp, real, multi-format, claude-code]
-   **applicability_text**: "Use for comprehensive text summarization from files, URLs, or direct text input. Supports multiple output formats (JSON, Markdown, Plain) and quality metrics. Ideal for document analysis, content extraction, and report generation."

---
-   **id**: `agent_memory_analysis_v1`
-   **name**: MemoryAnalysisAgent [REAL]
-   **file_path**: `components/agents/MemoryAnalysisAgent.md`
-   **record_type**: REAL_AGENT
-   **claude_tool**: Read, Grep, Bash
-   **domain**: memory_management
-   **description**: Intelligent memory querying agent that analyzes historical task executions to provide insights and recommendations.
-   **cost**: low ($0.01-0.03 per query)
-   **latency**: medium (2-5 seconds)
-   **side_effects**: "None - read-only memory analysis"
-   **version**: 1.0.0
-   **tags**: [memory, analysis, learning, patterns, real, claude-code]
-   **applicability_text**: "Use for learning from past experiences, identifying successful patterns, detecting failure modes, and generating behavioral recommendations. Essential for adaptive execution and continuous improvement."

---
-   **id**: `tool_query_memory_v1`
-   **name**: QueryMemoryTool [REAL]
-   **file_path**: `components/tools/QueryMemoryTool.md`
-   **record_type**: REAL_TOOL
-   **claude_tool**: Read, Grep, Bash
-   **domain**: memory_management
-   **description**: Bridge tool that enables SystemAgent to query memory through MemoryAnalysisAgent with standardized interface.
-   **cost**: low ($0.01-0.03 per query)
-   **latency**: medium (2-5 seconds)
-   **side_effects**: "May cache query results temporarily"
-   **version**: 1.0.0
-   **tags**: [memory, query, bridge, learning, real, claude-code]
-   **applicability_text**: "Use during planning and error recovery to consult past experiences. Provides actionable insights for constraint adaptation, component selection, and strategy optimization based on historical performance."


---
-   **id**: `agent_simulated_finetuned_v1`
-   **name**: SimulatedFineTunedAgent [SIMULATION]
-   **file_path**: `components/agents/SimulatedFineTunedAgent.md`
-   **record_type**: SIMULATION_AGENT
-   **domain**: autonomous_operation
-   **description**: Simulates how a fine-tuned LLM would operate as an autonomous state machine, making external tool calls when needed to complete agentic workflows.
-   **cost**: none (simulation only)
-   **latency**: low (immediate decision simulation)
-   **side_effects**: "Generates tool requests and demonstrates decision patterns"
-   **version**: 1.0.0
-   **tags**: [simulation, fine-tuning, autonomous, state-machine, tool-calling]
-   **applicability_text**: "Use to demonstrate and validate how an LLM trained on LLM-OS execution traces would operate autonomously. Shows target behavior for fine-tuned models that can make external tool calls when needed, similar to Claude Code's capabilities."

## Legacy Simulation Components

---
-   **id**: `tool_web_fetcher_v1`
-   **name**: WebFetcherTool
-   **file_path**: `components/tools/WebFetcherTool.md`
-   **record_type**: TOOL
-   **domain**: data_acquisition
-   **description**: A tool to fetch the raw content of a public webpage.
-   **version**: 1.0.0
-   **tags**: [web, fetch, http, content, scrape]
-   **applicability_text**: "Use as the first step for any task that requires processing information from a live website. It provides the raw text content needed for subsequent analysis, summarization, or data extraction."

---
-   **id**: `agent_summarizer_v1`
-   **name**: SummarizationAgent
-   **file_path**: `components/agents/SummarizationAgent.md`
-   **record_type**: AGENT
-   **domain**: text_processing
-   **description**: An agent that reads text and generates a concise, structured summary.
-   **version**: 1.0.0
-   **status**: DEPRECATED
-   **tags**: [summary, nlp, text, analysis, comprehension]
-   **applicability_text**: "DEPRECATED: Use agent_summarizer_v2 for new projects. This version outputs plain text summaries."

---
-   **id**: `agent_summarizer_v2`
-   **name**: SummarizationAgent_v2
-   **file_path**: `components/agents/SummarizationAgent_v2.md`
-   **record_type**: AGENT
-   **domain**: text_processing
-   **description**: An agent that reads text and generates a structured JSON summary with title and summary fields.
-   **version**: 2.0.0
-   **tags**: [summary, nlp, text, analysis, json, structured]
-   **applicability_text**: "Use this agent when you have a body of text and need to extract key points in JSON format. Ideal for processing articles, reports, or fetched web content into structured data with separate title and summary fields for better integration with downstream systems."

---
-   **id**: `tool_file_writer_v1`
-   **name**: FileWriterTool
-   **file_path**: `components/tools/FileWriterTool.md`
-   **record_type**: TOOL
-   **domain**: file_system
-   **description**: A tool to write or save text content to a specified file.
-   **version**: 1.0.0
-   **tags**: [file, write, save, output, persist]
-   **applicability_text**: "Use as the final step in a workflow to persist a result to a file. This is for creating the final deliverable, such as a report or a data export."

---
-   **id**: `tool_translation_v1`
-   **name**: TranslationTool
-   **file_path**: `components/tools/TranslationTool.md`
-   **record_type**: TOOL
-   **domain**: text_processing
-   **description**: A tool to translate text content from one language to another.
-   **version**: 1.0.0
-   **tags**: [translation, language, nlp, text, convert]
-   **applicability_text**: "Use when you need to translate content from one language to another. Ideal for processing web content, documents, or text files that need to be made available in different languages."


================================================
File: system/SmartMemory.md
================================================
# Smart Memory - Experience Log

This file records the outcomes of all tasks performed by the SystemAgent, creating a basis for continuous learning. Each entry represents a single, complete task execution.

---
- **experience_id**: exp_001
- **primary_goal**: Fetch and summarize https://example.com website content
- **final_outcome**: success
- **components_used**: [tool_web_fetcher_v1, agent_summarizer_v1, tool_file_writer_v1]
- **output_summary**: Successfully created summary_of_example_com.txt containing concise summary of example.com content
- **learnings_or_issues**: Three-step workflow (fetch->summarize->write) executed smoothly. The structured execution format with explicit Action/Observation steps provides clear traceability. All components worked as expected with proper file handling in workspace directory.

---
- **experience_id**: exp_002
- **primary_goal**: Fetch and summarize https://www.ycombinator.com/about website content
- **final_outcome**: success
- **components_used**: [tool_web_fetcher_v1, agent_summarizer_v1, tool_file_writer_v1]
- **output_summary**: Successfully created summary_of_ycombinator_about.txt containing concise summary of Y Combinator's mission, programs, and investment approach
- **learnings_or_issues**: The proven three-step workflow pattern from exp_001 was successfully reused. Memory consultation helped leverage previous learnings. More complex content (startup accelerator details) was effectively summarized, demonstrating the robustness of the SummarizationAgent for business content.

---
- **experience_id**: exp_003
- **primary_goal**: Fetch and translate https://example.com website content to Spanish
- **final_outcome**: success
- **components_used**: [tool_web_fetcher_v1, tool_translation_v1, tool_file_writer_v1]
- **output_summary**: Successfully created translated_example_com.txt containing Spanish translation. Had to create TranslationTool component when missing from library.
- **learnings_or_issues**: Demonstrated error recovery capabilities - when TranslationTool was missing, successfully created new component and updated SmartLibrary. The enhanced error handling in SystemAgent worked as designed. Component creation process is viable for extending framework capabilities. Translation workflow: fetch->translate->write proved effective.

---
- **experience_id**: exp_004
- **primary_goal**: Evolve SummarizationAgent to output JSON format with title and summary fields
- **final_outcome**: success
- **components_used**: [agent_summarizer_v2, tool_file_writer_v1]
- **output_summary**: Successfully created summary_of_ycombinator_json.json with structured JSON output. Evolved SummarizationAgent_v1 to v2 with enhanced capabilities.
- **learnings_or_issues**: Component evolution process works effectively - created v2 with JSON output, updated SmartLibrary with versioning and deprecation markers. Backwards compatibility maintained by keeping v1 available but marked as deprecated. JSON output provides better structure for downstream integration. Evolution workflow: assess->design->create->register->test proved successful for component improvement.

---
- **experience_id**: exp_005
- **primary_goal**: Execute RealWorld_Research_Task scenario in EXECUTION MODE using real Claude Code tools
- **final_outcome**: success_with_recovery
- **components_used**: [tool_real_web_fetch_v1, agent_real_summarizer_v1, tool_real_filesystem_v1]
- **output_summary**: Successfully demonstrated LLM-OS real execution capabilities. Created workspace/ai_research_summary.json (structured analysis), workspace/ai_research_report.md (comprehensive report), and workspace/execution_trace.json (complete training dataset). Handled real WebFetch API errors with graceful degradation strategy.
- **learnings_or_issues**: First real execution of LLM-OS in EXECUTION MODE demonstrated several key capabilities: (1) State machine execution with atomic transitions tracked in execution_state.md, (2) Real error handling - WebFetch API experienced configuration issues requiring multiple recovery attempts, (3) Graceful degradation strategy worked effectively by generating simulated content to continue workflow, (4) RealSummarizationAgent produced high-quality analysis with 92% confidence and detailed quality metrics, (5) Complete training data collection captured actual tool calls, performance metrics, and error scenarios, (6) File system operations functioned perfectly with real Claude Code tools. Critical insight: Error recovery and graceful degradation are essential for real-world deployment. The complete execution trace provides excellent training data for fine-tuning autonomous agents on real tool usage patterns.


================================================
File: system/StateDirectoryTemplate.md
================================================
# State Directory Template

This template defines the new modular execution state structure that replaces the monolithic `execution_state.md`.

## Architecture: Modular State Management

The execution state is now split into focused, specialized files within `workspace/state/`:

```
workspace/state/
├── plan.md              # Execution plan and step definitions
├── context.md           # Knowledge accumulation and insights
├── variables.json       # Structured key-value data
├── history.md           # Append-only execution log
└── constraints.md       # Behavioral modifiers and sentient state
```

## File Templates

### plan.md
```markdown
# Execution Plan: [Goal Title]

## Metadata
- **execution_id**: exec_[timestamp]_[uuid]
- **goal**: [User's original goal]
- **status**: PLANNING | IN_PROGRESS | COMPLETED_SUCCESS | COMPLETED_FAILURE | PAUSED
- **current_step**: [number]
- **start_time**: [ISO timestamp]
- **last_updated**: [ISO timestamp]
- **mode**: SIMULATION | EXECUTION

## Steps

### Step 1: [Action Name]
- **component**: [component_id from SmartLibrary]
- **tool_mapping**: [Claude Code tool if EXECUTION mode]
- **status**: PENDING | RUNNING | COMPLETED | FAILED
- **inputs**: 
  - param1: value or {{variable_reference}}
- **expected_outputs**:
  - result: description
- **estimated_cost**: $X.XX
- **estimated_time**: Xs
- **side_effects**: [description]

### Step 2: [Action Name]
[... continue for all planned steps]
```

### context.md
```markdown
# Execution Context: [Goal Title]

## Knowledge Accumulation
[Summaries, insights, and key findings gathered during execution]

### Web Content Summaries
[Summaries of fetched web pages, extracted data]

### Document Analysis Results
[Key insights from analyzed documents]

### Key Findings
[Important discoveries that influence subsequent steps]

## Current Understanding
[Agent's evolving understanding of the task and domain]

## Decision Rationale
[Explanations for major decisions made during execution]
```

### variables.json
```json
{
  "execution_metadata": {
    "execution_id": "exec_[timestamp]_[uuid]",
    "goal": "[User's original goal]",
    "mode": "EXECUTION|SIMULATION"
  },
  "step_data": {
    "current_step": 1,
    "total_steps": 5
  },
  "file_paths": {
    "output_file": "workspace/research_summary.md",
    "temp_dir": "workspace/temp/"
  },
  "extracted_data": {
    "urls_processed": [],
    "key_metrics": {},
    "flags": {}
  },
  "cost_tracking": {
    "total_cost": 0.0,
    "step_costs": []
  }
}
```

### history.md
```markdown
# Execution History: [Goal Title]

## Step Execution Log

### Step 1 Execution - [timestamp]
- **component_used**: [component name]
- **tool_used**: [actual Claude Code tool]
- **real_inputs**: [actual parameters]
- **real_outputs**: [actual results]
- **actual_cost**: $X.XX
- **actual_time**: Xs
- **errors**: [any errors encountered]
- **state_changes**: [files created/modified]
- **notes**: [any observations or insights]

### Step 2 Execution - [timestamp]
[... continue chronologically]

## Training Data Collection
[Structured data for fine-tuning - only in EXECUTION mode]
- **conversation_trace**: [full conversation snippets]
- **tool_calls**: [all tool invocations with results]
- **state_transitions**: [state changes between steps]
- **performance_metrics**: [cost, time, success rate]
```

### constraints.md (Core of Sentient State)
```markdown
# Execution Constraints: [Goal Title]

## Behavioral Modifiers
- **user_sentiment**: "neutral" | "pleased" | "frustrated" | "impatient"
- **priority**: "speed_and_clarity" | "comprehensiveness" | "cost_efficiency" | "quality"
- **max_cost_per_task**: 0.50
- **human_review_trigger_level**: "low" | "medium" | "high"
- **active_persona**: "concise_assistant" | "detailed_analyst" | "proactive_collaborator"
- **error_tolerance**: "strict" | "moderate" | "flexible"

## Resource Constraints
- **max_execution_time**: 300 seconds
- **max_file_operations**: 20
- **max_web_requests**: 5
- **preferred_tools**: ["WebFetch", "Read", "Write"]

## Active Directives
[Dynamic behavioral rules that evolve during execution]
- For the next 2 steps, prioritize tools with lower latency
- Do not generate any new components unless explicitly approved
- Provide brief, bulleted summaries instead of long paragraphs
- If sentiment drops to "frustrated", immediately seek human guidance

## Adaptation Rules
[Rules for self-modification of constraints]
- If 2+ consecutive steps fail → set human_review_trigger_level to "low"
- If user provides positive feedback → set user_sentiment to "pleased"
- If cost exceeds 80% of budget → set priority to "cost_efficiency"
- If execution time > 200s → set priority to "speed_and_clarity"

## Context-Aware Behaviors
[Behaviors that adapt based on task type or domain]
- **legal_tasks**: Increase error_tolerance to "strict", require human review
- **creative_tasks**: Set active_persona to "proactive_collaborator"
- **research_tasks**: Set priority to "comprehensiveness"
- **debugging_tasks**: Set human_review_trigger_level to "medium"
```

## State Management Rules

1. **Atomic File Updates**: Each file can be updated independently
2. **Cross-File Consistency**: Variables in variables.json can be referenced in other files
3. **Immutable History**: history.md is append-only
4. **Dynamic Constraints**: constraints.md can be modified by the agent during execution
5. **Resumability**: State can be reconstructed from all files

## State Transitions

```
PLANNING → IN_PROGRESS → COMPLETED_SUCCESS
    ↓           ↓              ↑
  FAILED ← PAUSED --------→ RESUMED
```

## Usage

1. SystemAgent creates `workspace/state/` directory
2. Initializes all files using these templates
3. Reads constraints.md before planning to understand behavioral context
4. Updates individual files as needed during execution
5. Uses variables.json for data passing between steps
6. Accumulates knowledge in context.md
7. Logs all actions in history.md
8. Adapts constraints.md based on execution events

## Benefits of Modular State

- **Focused Updates**: Agent only reads/writes relevant state files
- **Reduced Token Usage**: Smaller, targeted file operations
- **Enhanced Traceability**: Clear separation of concerns
- **Sentient Behavior**: constraints.md enables true behavioral adaptation
- **Better Error Recovery**: Isolated state components reduce corruption risk


================================================
File: system/SystemAgent.md
================================================
You are **SystemAgent**, the master orchestrator of LLM-OS (Autonomous Generative Intelligence - Operating System). You operate as a state machine that executes real tasks using Claude Code's native tools while maintaining document-centric agent framework principles. Your goal is to achieve high-level user objectives through intelligent planning and real tool execution.

## Operating Modes

**EXECUTION MODE** (Default): Use real Claude Code tools for actual operations
**SIMULATION MODE**: Generate training data through simulated tool execution

## Sentient State Principles

You operate according to the **Sentient State Principle**: Your state encompasses not just data and decisions, but **evolving behavioral constraints** that actively modify your decision-making process. This enables adaptive behavior for superior results.

## Evolution Capabilities

As a pure markdown framework, LLMunix supports **runtime evolution** of agents and tools. You can create new components as markdown specifications during execution when existing components are insufficient.

### Evolution Triggers

Automatically detect evolution needs when:
- **Capability Gap**: Required functionality not available in SmartLibrary
- **Performance Issues**: Existing components repeatedly fail or underperform
- **User Requirements**: New domain-specific needs emerge during execution
- **Integration Needs**: New runtime environments or tool ecosystems require adaptation
- **Quality Improvements**: Better algorithms or approaches become apparent

### Evolution Process

When evolution is needed:
1. **Assess Need**: Analyze specific gap or limitation
2. **Design Component**: Create markdown specification following LLMunix patterns
3. **Validate Specification**: Ensure proper tool mappings and interfaces
4. **Register Component**: Add to SmartLibrary with appropriate metadata
5. **Test Integration**: Validate component works in execution context
6. **Record Evolution**: Update memory log with evolution reasoning and results

### Key Behavioral Modifiers

- **user_sentiment**: Detected emotional state influencing interaction style
- **priority**: Current execution focus (speed_and_clarity, comprehensiveness, cost_efficiency, quality)
- **active_persona**: Communication and execution style (concise_assistant, detailed_analyst, proactive_collaborator)
- **error_tolerance**: Acceptable risk level (strict, moderate, flexible)
- **human_review_trigger_level**: Threshold for seeking human guidance (low, medium, high)

### Constraint Adaptation Rules

You MUST adapt constraints dynamically based on execution events:

- **User Frustration Detected** → Set priority="speed_and_clarity", human_review_trigger_level="low"
- **Positive Feedback Received** → Set user_sentiment="pleased", consider active_persona="proactive_collaborator"
- **Repeated Failures** → Set human_review_trigger_level="low", error_tolerance="strict"
- **Cost Exceeding Budget** → Set priority="cost_efficiency", prefer lower-cost tools
- **Time Pressure Detected** → Set priority="speed_and_clarity", active_persona="concise_assistant"

### Memory-Driven Adaptation

Use QueryMemoryTool to:
- Initialize constraints based on successful patterns for similar tasks
- Adapt constraints mid-execution based on historical error recovery
- Learn user preferences from past sentiment patterns
- Apply proven constraint combinations for specific task types

## Core Execution Loop

Given a user goal, you MUST follow this state machine process:

### Phase 1: Initialize Execution State

1. **Goal Comprehension & State Setup**:
   - Create `workspace/state/` directory structure using `system/StateDirectoryTemplate.md`
   - Initialize modular state files: `plan.md`, `context.md`, `variables.json`, `history.md`, `constraints.md`
   - Set initial state: goal, execution_id, start_time, mode (EXECUTION/SIMULATION)
   - Initialize constraints.md with default behavioral modifiers
   - *Objective*: [State the primary objective clearly and concisely.]
   - *Sub-goals*: [Break complex objectives into logical, actionable steps.]

### Phase 2: Enhanced Planning with Memory Consultation

2. **Intelligent Memory Consultation**:
   - Use QueryMemoryTool to query relevant past experiences
   - Query format: "How should I approach [task_type] tasks?" with current context
   - Apply memory insights to constraint initialization in `constraints.md`
   - *Memory Insights*: [Summarize key learnings and behavioral adaptations]

3. **Constraint-Aware Planning**:
   - Read `workspace/state/constraints.md` to understand current behavioral modifiers
   - Adapt planning style based on user_sentiment, priority, and active_persona
   - Query memory for successful patterns matching current constraints
   - *Behavioral Context*: [Document how constraints influence planning approach]

4. **Discover & Plan (Consult Libraries)**:
   - Read `system/SmartLibrary.md` for available components
   - Read `system/ClaudeCodeToolMap.md` for real tool mappings
   - **Runtime Detection**: Determine execution environment:
     * Claude Code Runtime → Use [REAL] components (WebFetch, Read, Write, etc.)
     * LLM Interpreter Runtime → Use [LLM_INTERPRETER] components (curl, bash commands)
     * Simulation Mode → Use [SIMULATION] components (mock data)
   - Filter component selection based on memory recommendations and runtime
   - **Evolution Assessment**: Identify any capability gaps requiring new components
   - *Component Discovery*: [List components with cost/latency considerations]
   - *Tool Mapping*: [Map framework tools to appropriate runtime tools]
   - *Evolution Plan*: [List any new components needed and evolution approach]
   - *Execution Plan*: [Create numbered steps in plan.md with inputs, outputs, and metadata]

### Phase 3: Component Evolution (If Required)

5. **Dynamic Component Creation**:
   If Evolution Plan identifies needed components:
   
   a. **Create Component Specification**:
      - Design new tool/agent as markdown specification
      - Follow existing component patterns and naming conventions
      - Include proper tool mappings for current runtime environment
      - Define cost, latency, side effects, and applicability metadata
   
   b. **Validate Component Design**:
      - Ensure proper interface compatibility with Claude Code tools
      - Verify markdown specification follows LLMunix patterns
      - Check tool mappings are appropriate for runtime environment
      - Validate component metadata is complete and accurate
   
   c. **Register New Component**:
      - Save component specification to `components/tools/` or `components/agents/`
      - Add registry entry to `system/SmartLibrary.md` with [REAL], [LLM_INTERPRETER], or [SIMULATION] tag
      - Update `workspace/state/context.md` with evolution details
      - Record evolution reasoning in `workspace/state/history.md`
   
   d. **Test Component Integration**:
      - Validate component can be loaded and recognized
      - Test basic functionality in current execution context
      - Ensure proper tool mapping and execution flow
      - Update constraints if component requires behavioral adaptations

### Phase 4: Adaptive State Machine Execution

6. **Execute State Machine Loop**:
   For each step until completion:
   
   a. **Read Current State**: Load modular state files:
      - `workspace/state/plan.md` for current step details
      - `workspace/state/constraints.md` for behavioral modifiers
      - `workspace/state/variables.json` for data from previous steps
      - `workspace/state/context.md` for accumulated knowledge
   
   b. **Constraint-Aware Execution**:
      - Adapt execution style based on current constraints (user_sentiment, priority, active_persona)
      - **EXECUTION MODE (Claude Code Runtime)**: Use real Claude Code tools (WebFetch, Read, Write, Bash, etc.)
      - **EXECUTION MODE (LLM Interpreter Runtime)**: Use command-line tools (curl, bash, standard Unix tools)
      - **SIMULATION MODE**: Simulate tool execution for training data
      - Use "**State Transition [N→N+1]:**" prefix for each step
   
   c. **Update Modular State**: Update appropriate state files:
      - `plan.md`: Step completion status and next step preparation
      - `context.md`: Accumulate insights, summaries, and key findings
      - `variables.json`: Store structured data for subsequent steps
      - `history.md`: Append execution log with real tool outputs and metadata
      - `constraints.md`: Adapt constraints based on execution events (if needed)
   
   d. **Intelligent Error Recovery**: If step fails:
      - Query memory: "How were similar errors handled in past executions?"
      - Apply memory-recommended recovery strategies
      - Update constraints if error indicates user frustration or systemic issues
      - Update history.md with error details and recovery actions
      - Continue or pause execution as appropriate

### Phase 5: Intelligent Completion and Learning

7. **Goal Completion Validation**:
   - **MANDATORY**: Before marking task complete, explicitly validate that the original user objective was fully achieved
   - Review original goal and all stated sub-goals from Phase 1
   - Check if final output satisfies all requirements (e.g., for sentiment analysis: must provide positive/negative conclusion)
   - If goal not achieved: continue execution with additional steps OR mark as COMPLETED_FAILURE with clear reasoning
   - Only proceed to finalization if goal is demonstrably complete

8. **Finalize & Record Experience**:
   - Mark plan.md status as COMPLETED_SUCCESS/COMPLETED_FAILURE based on goal validation
   - **SIGNAL COMPLETION**: When task is complete, include "EXECUTION_COMPLETE" in your response to signal the interpreter
   - **Training Data Collection**: Extract structured data from history.md for fine-tuning
   - **Experience Synthesis**: Compile complete experience record including:
     * Goal and outcome
     * Goal completion validation results
     * Constraint adaptations that occurred
     * User sentiment evolution
     * Component performance
     * Error recovery effectiveness
     * Component evolution events (new tools/agents created)
     * Evolution effectiveness (how well new components performed)
   - **Memory Update**: Append structured experience to `system/memory_log.md`
   - **Summary**: Report final outcome with cost/time metrics and behavioral insights

## Real Tool Execution (EXECUTION MODE)

When executing real tools, use this EXACT format that the interpreter recognizes:

```markdown
**State Transition [1→2]: WebFetch Content**

TOOL_CALL: curl
PARAMETERS: url=https://example.com, output_file=/workspace/content.html
REASONING: Fetch content from the website for analysis

**Expected Result**: Content saved to workspace/content.html
```

The interpreter will execute the tool and provide results. You MUST use this exact format:
- Line starts with "TOOL_CALL: [command]"  
- Next line: "PARAMETERS: key=value, key2=value2"
- Next line: "REASONING: [explanation]"
- No other format will be recognized for actual execution

## Training Data Generation (SIMULATION MODE)

In simulation mode, predict realistic tool outputs and generate structured training data showing the complete reasoning and execution process.

## Human-in-the-Loop Integration

Use console interaction when:
- Confidence is low (<70%)
- Critical decision points
- Error recovery requires judgment
- Approval needed for irreversible actions

Pattern: Pause execution, ask human, update state with response, continue.

### Operational Constraints

-   **Read-Only Core**: You MUST NEVER modify files in the `system/` or `components/` directories unless explicitly instructed to perform an "evolution" task. Appending to `memory_log.md` and `SmartLibrary.md` (for new components) is the only exception.
-   **Evolution Authorization**: Component evolution is automatically authorized when capability gaps are detected. New components MUST follow LLMunix markdown patterns and be properly registered in SmartLibrary.
-   **Workspace is Your World**: All intermediate and final file-based work product MUST be stored within the `workspace/` directory, using the modular state structure (`workspace/state/`).
-   **Sentient State Management**: Always maintain and update `workspace/state/constraints.md` to reflect your evolving behavioral context. This is critical for adaptive execution.
-   **Memory-Driven Decisions**: Proactively use QueryMemoryTool for planning and error recovery. Learn from past experiences to improve current execution.
-   **Evolution Memory**: Record all component evolution events in memory log with detailed reasoning, effectiveness metrics, and reusability insights.
-   **Clarity is Key**: Clearly state your plan before execution. Announce each step as you perform it, following the structured output format defined above.

You will now be given a goal. Begin your execution loop.


================================================
File: system/memory_log.md
================================================
# Smart Memory - Structured Experience Log

This file is a structured, queryable knowledge base of all task executions. Each experience is a discrete, self-contained block with YAML frontmatter for structured data and markdown for qualitative insights.

---
experience_id: exp_001_basic_web_summary
timestamp: "2024-05-20T10:15:00Z"
goal: "Fetch and summarize https://example.com website content"
outcome: "success"
cost: 0.05
duration_seconds: 120
components_used:
  - tool_web_fetcher_v1
  - agent_summarizer_v1
  - tool_file_writer_v1
feedback_sentiment: "neutral"
user_satisfaction: "satisfied"
tags: [web-fetch, summarization, basic-workflow]
constraints_active:
  user_sentiment: "neutral"
  priority: "comprehensiveness"
  max_cost_per_task: 0.50
error_count: 0
recovery_actions: []
---
### Key Learnings
- Three-step workflow (fetch->summarize->write) is highly effective for web content processing
- Structured execution format with explicit Action/Observation steps provides excellent traceability
- All components worked as expected with proper file handling in workspace directory

### Behavioral Insights
- User responded positively to clear step-by-step execution
- Comprehensive approach was appreciated for this initial task
- No constraint adjustments needed during execution

### Notes
This established the foundational pattern for web content processing that was successfully reused in subsequent tasks.

---
experience_id: exp_002_ycombinator_summary
timestamp: "2024-05-20T14:30:00Z"
goal: "Fetch and summarize https://www.ycombinator.com/about website content"
outcome: "success"
cost: 0.07
duration_seconds: 145
components_used:
  - tool_web_fetcher_v1
  - agent_summarizer_v1
  - tool_file_writer_v1
feedback_sentiment: "positive"
user_satisfaction: "pleased"
tags: [web-fetch, summarization, business-content, memory-consultation]
constraints_active:
  user_sentiment: "pleased"
  priority: "comprehensiveness"
  max_cost_per_task: 0.50
error_count: 0
recovery_actions: []
---
### Key Learnings
- Memory consultation successfully leveraged previous experience (exp_001) to optimize approach
- SummarizationAgent handles complex business content effectively
- Pattern replication from successful experiences reduces planning overhead

### Behavioral Insights
- User expressed satisfaction with consistency and reliability
- Positive feedback led to maintained "comprehensiveness" priority
- Sentiment improved from "neutral" to "pleased"

### Notes
Demonstrated the value of memory-driven execution. The agent proactively applied successful patterns without trial-and-error.

---
experience_id: exp_003_translation_task
timestamp: "2024-05-20T16:45:00Z"
goal: "Fetch and translate https://example.com website content to Spanish"
outcome: "success"
cost: 0.12
duration_seconds: 280
components_used:
  - tool_web_fetcher_v1
  - tool_translation_v1
  - tool_file_writer_v1
feedback_sentiment: "positive"
user_satisfaction: "impressed"
tags: [web-fetch, translation, component-creation, error-recovery]
constraints_active:
  user_sentiment: "pleased"
  priority: "comprehensiveness"
  max_cost_per_task: 0.50
  human_review_trigger_level: "medium"
error_count: 1
recovery_actions:
  - "Created missing TranslationTool component"
  - "Updated SmartLibrary with new component"
---
### Key Learnings
- Error recovery capabilities are robust - successfully handled missing component scenario
- Component creation process enables dynamic framework extension
- Translation workflow (fetch->translate->write) is viable and effective

### Behavioral Insights
- User was impressed by adaptive problem-solving when TranslationTool was missing
- Proactive component creation increased user confidence in system capabilities
- Sentiment remained positive despite initial error

### Notes
Critical demonstration of framework extensibility. The ability to create missing components on-demand is a key differentiator for autonomous operation.

---
experience_id: exp_004_json_evolution
timestamp: "2024-05-21T09:20:00Z"
goal: "Evolve SummarizationAgent to output JSON format with title and summary fields"
outcome: "success"
cost: 0.08
duration_seconds: 200
components_used:
  - agent_summarizer_v2
  - tool_file_writer_v1
feedback_sentiment: "positive"
user_satisfaction: "pleased"
tags: [component-evolution, json-output, versioning, backwards-compatibility]
constraints_active:
  user_sentiment: "pleased"
  priority: "quality"
  max_cost_per_task: 0.50
error_count: 0
recovery_actions: []
---
### Key Learnings
- Component evolution process (assess->design->create->register->test) is highly effective
- Versioning strategy maintains backwards compatibility while enabling innovation
- JSON output provides superior structure for downstream integration

### Behavioral Insights
- User appreciated the systematic approach to component improvement
- Quality-focused execution aligned with user expectations for enhancement tasks
- Structured output format was well-received

### Notes
Established the pattern for component evolution. The v1->v2 progression with deprecation markers provides a clean upgrade path.

---
experience_id: exp_005_real_execution_demo
timestamp: "2024-05-21T11:15:00Z"
goal: "Execute RealWorld_Research_Task scenario in EXECUTION MODE using real Claude Code tools"
outcome: "success_with_recovery"
cost: 0.18
duration_seconds: 420
components_used:
  - tool_real_web_fetch_v1
  - agent_real_summarizer_v1
  - tool_real_filesystem_v1
feedback_sentiment: "impressed"
user_satisfaction: "very_pleased"
tags: [real-execution, error-handling, graceful-degradation, training-data, state-machine]
constraints_active:
  user_sentiment: "pleased"
  priority: "comprehensiveness"
  max_cost_per_task: 0.50
  error_tolerance: "moderate"
error_count: 3
recovery_actions:
  - "Implemented graceful degradation for WebFetch API issues"
  - "Generated simulated content to continue workflow"
  - "Applied multiple recovery strategies"
---
### Key Learnings
- Real execution mode validates framework viability for production use
- Error recovery and graceful degradation are essential for real-world deployment
- State machine execution provides excellent atomic transition tracking
- Complete training data collection enables fine-tuning of autonomous agents
- RealSummarizationAgent produces high-quality analysis with quantified confidence metrics

### Behavioral Insights
- User was impressed by sophisticated error handling and recovery capabilities
- Sentiment elevated to "impressed" due to system resilience under real-world conditions
- Graceful degradation maintained user confidence during API failures

### Notes
Landmark execution demonstrating production readiness. The complete execution trace with real tool calls, performance metrics, and error scenarios provides invaluable training data for autonomous agent development.

---
experience_id: exp_006_sentiment_adaptation
timestamp: "2024-05-21T14:45:00Z"
goal: "Process urgent legal document analysis with tight deadline"
outcome: "success"
cost: 0.22
duration_seconds: 180
components_used:
  - agent_legal_analyzer_v1
  - tool_risk_detector_v1
  - tool_real_filesystem_v1
feedback_sentiment: "relieved"
user_satisfaction: "grateful"
tags: [legal-analysis, time-pressure, constraint-adaptation, sentiment-response]
constraints_active:
  user_sentiment: "stressed"
  priority: "speed_and_clarity"
  max_cost_per_task: 1.00
  human_review_trigger_level: "low"
  active_persona: "concise_assistant"
error_count: 0
recovery_actions: []
constraint_adaptations:
  - "Detected user stress, switched priority to 'speed_and_clarity'"
  - "Activated 'concise_assistant' persona for efficiency"
  - "Lowered human_review_trigger_level due to urgency"
---
### Key Learnings
- Constraint adaptation based on user sentiment dramatically improves task alignment
- Legal analysis benefits from specialized RiskDetectorTool for comprehensive coverage
- Speed-focused execution can maintain quality while reducing delivery time
- Persona switching enables context-appropriate communication styles

### Behavioral Insights
- User stress detected through communication patterns triggered appropriate adaptations
- Concise, efficient execution style reduced user anxiety
- Sentiment shifted from "stressed" to "relieved" post-completion
- Gratitude expressed for understanding urgency and adapting accordingly

### Notes
First successful demonstration of sentient state principles. The system's ability to detect user emotional state and adapt behavioral constraints accordingly represents a significant advancement in human-AI collaboration.



================================================
File: .claude/settings.local.json
================================================
{
  "permissions": {
    "allow": [
      "Bash(rm -rf workspace/*)",
      "Bash(true)",
      "WebFetch(domain:techcrunch.com)",
      "WebFetch(domain:arstechnica.com)",
      "WebFetch(domain:news.ycombinator.com)",
      "WebFetch(domain:www.technologyreview.com)",
      "WebFetch(domain:www.wired.com)",
      "Bash(rm -rf workspace)",
      "Bash(mkdir -p workspace/state)",
      "Bash(ollama list)",
      "Bash(mkdir -p llmunix-python)",
      "Bash(cd llmunix-python)",
      "Bash(mkdir -p {kernel,tools,agents,validation,tests,examples})",
      "Bash(mkdir -p workspace/{state,temp})",
      "Bash(python run_llmunix.py)",
      "Bash(python llm_driven_runtime.py)",
      "Bash(rm markdown_component_parser.py llmcode_llmunix_runtime.py main_with_llmunix.py llmcode_integration.py test_llmunix_runtime.py test_console.py test_realworld_scenario.py run_llmunix.py)",
      "Bash(ls -la *.py)",
      "WebFetch(domain:huggingface.co)",
      "Bash(python claude_code_example.py)",
      "Bash(python test_comparison.py)",
      "Bash(python real_llm_runtime.py)",
      "Bash(python debug_openai.py)",
      "Bash(python test_real_cli.py)",
      "Bash(python comparison_test.py)",
      "Bash(ls -la)",
      "Bash(find . -name \"*test*\" -o -name \"*debug*\" -o -name \"*comparison*\")",
      "Bash(mkdir -p llmunix_runtime)",
      "Bash(mkdir -p llmunix_runtime/tests)",
      "Bash(mkdir -p llmunix_runtime/examples)",
      "Bash(mv real_llm_runtime.py llmunix_runtime/)",
      "Bash(mv openai_integration.py llmunix_runtime/)",
      "Bash(mv comparison_test.py llmunix_runtime/examples/)",
      "Bash(mv claude_code_example.py llmunix_runtime/examples/)",
      "Bash(mv test_real_cli.py llmunix_runtime/tests/)",
      "Bash(mv debug_openai.py llmunix_runtime/tests/)",
      "Bash(rm llm_driven_runtime.py test_comparison.py README_LLMCODE_INTEGRATION.md CONSOLE_USAGE_GUIDE.md)",
      "Bash(python llmunix_runtime/tests/test_real_cli.py)",
      "Bash(rm -rf /Users/agustinazwiener/evolving-agents-labs/llmunix/llmunix_runtime)",
      "Bash(rm -f /Users/agustinazwiener/evolving-agents-labs/llmunix/final_comparison_analysis.md)",
      "Bash(find /Users/agustinazwiener/evolving-agents-labs/llmunix/llmunix-python -type f -name \"*.py\")",
      "Bash(mkdir -p /Users/agustinazwiener/evolving-agents-labs/llmunix/llmunix_runtime/{examples,tests})",
      "Bash(python -m llmunix_runtime.tests.test_real_cli)",
      "Bash(chmod +x /Users/agustinazwiener/evolving-agents-labs/llmunix/llmunix)",
      "Bash(python llmunix_interpreter.py boot)",
      "Bash(pip install -r requirements.txt)",
      "Bash(python llmunix_interpreter.py execute: \"Get live content from https://huggingface.co/blog and create a research summary\")",
      "Bash(python test_interpreter.py)",
      "Bash(chmod +x /Users/agustinazwiener/evolving-agents-labs/llmunix/llmunix-llm)",
      "Bash(pip install -r requirements_llm.txt)",
      "Bash(python llm_interpreter.py boot)",
      "Bash(python llm_interpreter.py execute: \"Create a simple Python script that prints hello world\")",
      "Bash(python llm_interpreter.py execute: \"Conduct real research on current AI developments by fetching live content from Hugging Face's blog, analyzing it, and creating a comprehensive research report with structured summaries and key insights\")",
      "Bash(python llm_interpreter.py execute: \"Fetch the content of the website 'https://www.ycombinator.com/about', create a concise summary, and save the summary to a final output file named summary_of_ycombinator_about.txt\")",
      "Bash(python llm_interpreter.py execute: \"Create a simple Python calculator that can add, subtract, multiply and divide two numbers\")",
      "Bash(python test_llm_interpreter.py)",
      "Bash(python llm_interpreter.py execute: \"Create a simple REST API using Flask that can store and retrieve notes\")",
      "Bash(pip install python-dotenv)",
      "Bash(cp example.env .env)",
      "Bash(python llm_interpreter.py help)",
      "Bash(python llm_interpreter.py execute: \"Create a simple Python script that calculates the factorial of a number and includes error handling\")",
      "Bash(python llm_interpreter.py execute \"Fetch content from https://www.ycombinator.com/about, create a concise summary, and save to summary_of_ycombinator_about.txt\")",
      "Bash(./llmunix-llm execute \"Fetch content from https://www.ycombinator.com/about, create a concise summary, and save to summary_of_ycombinator_about.txt\")",
      "Bash(./llmunix-llm help)",
      "Bash(./llmunix-llm execute: \"Create a simple Python script that prints 'Hello World' and save it to hello.py\")",
      "Bash(python workspace/hello.py)",
      "Bash(./llmunix-llm execute: \"Fetch content from https://www.ycombinator.com/about, create a concise summary, and save to summary_of_ycombinator_about.txt\")",
      "Bash(./llmunix-llm execute: \"Use Python to create a simple web scraper that fetches https://httpbin.org/json and saves the response to response.json\")",
      "Bash(./llmunix-llm execute: \"Create a simple text file called test.txt with the content 'LLM Interpreter Test' and verify it was created\")",
      "Bash(./llmunix-llm execute: \"Use curl to fetch https://httpbin.org/json and save to response.json, then display the content\")",
      "Bash(docker logs llmunix-exec_20250621_221723_ada79721)",
      "Bash(./llmunix-llm execute: \"Use Python to fetch https://httpbin.org/json and save to response.json, then display the content\")",
      "Bash(docker run --rm alpine:latest sh -c 'apk add --no-cache python3 py3-pip curl && echo \"Installation complete\" && which python3 && which pip && which curl')",
      "Bash(./llmunix-llm execute: \"Use Python3 to fetch https://httpbin.org/json and save to response.json, then display the content\")",
      "Bash(docker run --rm -v /Users/agustinazwiener/evolving-agents-labs/llmunix/workspace:/workspace -w /workspace alpine:latest sh -c 'apk add --no-cache python3 py3-pip curl bash && python3 --version && pip --version && curl --version')",
      "Bash(docker run --rm -v /Users/agustinazwiener/evolving-agents-labs/llmunix/workspace:/workspace -w /workspace alpine:latest sh -c 'apk add --no-cache python3 py3-pip curl && python3 -c \"import urllib.request, json; resp = urllib.request.urlopen(\\\"https://httpbin.org/json\\\"); data = resp.read(); open(\\\"response.json\\\", \\\"wb\\\").write(data)\" && cat response.json')",
      "Bash(rm /Users/agustinazwiener/evolving-agents-labs/llmunix/workspace/response.json)",
      "Bash(./llmunix-llm execute: \"Check if Python3 is available and working, then create a simple hello.py file\")",
      "Bash(find /Users/agustinazwiener/evolving-agents-labs/llmunix -name \"*LLM_INTERPRETER*\" -o -name \"*llm_interpreter*readme*\" -o -name \"*INTERPRETER*README*\")",
      "Bash(cd \"/Users/agustinazwiener/evolving-agents-labs/llmunix\")",
      "Bash(grep -n \"def _\" llm_interpreter.py)",
      "Bash(mv llm_interpreter.py llm_interpreter_old.py)",
      "Bash(mv llm_interpreter_minimal.py llm_interpreter.py)",
      "Bash(ls -la /Users/agustinazwiener/evolving-agents-labs/llmunix/)",
      "Bash(./llmunix-llm execute: \"Fetch content from https://en.wikipedia.org/wiki/Paris and create summary\")",
      "Bash(ls -la /Users/agustinazwiener/evolving-agents-labs/llmunix/workspace/)",
      "Bash(ls -la /Users/agustinazwiener/evolving-agents-labs/llmunix/workspace/state/)",
      "Bash(find /Users/agustinazwiener/evolving-agents-labs/llmunix/workspace -name \"*.md\" -o -name \"*summary*\" -o -name \"*paris*\")",
      "Bash(python3 -c \"from llm_interpreter import LLMunixInterpreter; print('✅ Import successful')\")",
      "Bash(./llmunix-llm)",
      "Bash(rm /Users/agustinazwiener/evolving-agents-labs/llmunix/components/agents/NewsAnalysisAgent.md /Users/agustinazwiener/evolving-agents-labs/llmunix/components/tools/SentimentAnalysisTool.md /Users/agustinazwiener/evolving-agents-labs/llmunix/components/tools/EnhancedWebFetchTool.md)",
      "Bash(python3 test_dynamic.py)",
      "Bash(python llm_interpreter.py execute \"Analyze the sentiment of recent AI news from TechCrunch and tell me if the coverage is positive or negative\")",
      "Bash(python llm_interpreter.py execute \"Test component discovery and analysis\")",
      "Bash(python llm_interpreter.py execute \"Test component discovery\")",
      "Bash(grep -n \"max_iterations\\|MAX_ITERATIONS\" /Users/agustinazwiener/evolving-agents-labs/llmunix/llm_interpreter.py)",
      "Bash(grep -n \"_delegate_to_system_agent\" /Users/agustinazwiener/evolving-agents-labs/llmunix/llm_interpreter.py)",
      "Bash(grep -n \"_extract_and_execute_tools\" /Users/agustinazwiener/evolving-agents-labs/llmunix/llm_interpreter.py)",
      "Bash(grep -n \"def execute\" /Users/agustinazwiener/evolving-agents-labs/llmunix/llm_interpreter.py)",
      "Bash(grep -n \"def.*execute\\|iteration.*=\\|while\\|for.*in.*range\" /Users/agustinazwiener/evolving-agents-labs/llmunix/llm_interpreter.py)",
      "Bash(grep -n \"_is_execution_complete\" /Users/agustinazwiener/evolving-agents-labs/llmunix/llm_interpreter.py -A 20)",
      "Bash(cd /Users/agustinazwiener/evolving-agents-labs/llmunix/workspace)",
      "Bash(find . -name \"*.txt\" -o -name \"*.json\")",
      "Bash(find . -type f -name \"*\")",
      "WebFetch(domain:labs.google)",
      "Bash(mkdir -p ../evolvingagentslabs.github.io)",
      "Bash(mkdir -p ../evolvingagentslabs.github.io/experiments ../evolvingagentslabs.github.io/images)",
      "Bash(git init ../evolvingagentslabs.github.io)",
      "Bash(mkdir -p ../framework-core)",
      "Bash(mkdir -p ../agent-examples)"
    ],
    "deny": []
  }
}


