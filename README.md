<div align="center">

![LLM OS Hero Banner](assets/hero-banner.png)

# LLM OS

### The Operating System Where AI is the CPU

[![Version](https://img.shields.io/badge/version-3.4.0-blue.svg)](https://github.com/EvolvingAgentsLabs/llmunix/releases)
[![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10+-yellow.svg)](https://python.org)
[![Claude SDK](https://img.shields.io/badge/Claude-Agent%20SDK-orange.svg)](https://docs.anthropic.com)

**Learn once. Execute infinitely. Evolve continuously.**

[Quick Start](#-quick-start) Â· [Architecture](#-the-four-layer-stack) Â· [Examples](#-examples) Â· [Docs](ARCHITECTURE.md)

</div>

---

## What is LLM OS?

**LLM OS** treats Large Language Models as the **CPU** of a new kind of operating system. Just like traditional operating systems manage hardware resources, LLM OS manages *intelligence* as a resourceâ€”with budgets, scheduling, and optimization.

<div align="center">

<!-- Demo video/GIF will be added here -->
<table>
<tr>
<td align="center" width="600">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  $ python llmos/boot.py interactive                         â”‚
â”‚                                                             â”‚
â”‚  ğŸš€ LLM OS v3.4.0 - Self-Evolving Operating System          â”‚
â”‚  ğŸ’° Budget: $10.00 | Mode: AUTO                             â”‚
â”‚                                                             â”‚
â”‚  > Create a Python calculator                               â”‚
â”‚                                                             â”‚
â”‚  [LEARNER] Novel task detected                              â”‚
â”‚  [SEARCH] Finding relevant tools...                         â”‚
â”‚  [EXECUTE] Creating calculator.py                           â”‚
â”‚  [TRACE] Saved execution trace                              â”‚
â”‚  [COST] $0.45 | Time: 12.3s                                 â”‚
â”‚                                                             â”‚
â”‚  > Create a Python calculator                               â”‚
â”‚                                                             â”‚
â”‚  [FOLLOWER] Found matching trace (98% confidence)           â”‚
â”‚  [PTC] Replaying tool sequence...                           â”‚
â”‚  [COMPLETE] calculator.py created                           â”‚
â”‚  [COST] $0.00 | Time: 0.8s  âœ¨ FREE!                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</td>
</tr>
</table>

</div>

### The Problem We Solve

<table>
<tr>
<td width="50%" align="center">

**Traditional LLM Apps**

</td>
<td width="50%" align="center">

**LLM OS**

</td>
</tr>
<tr>
<td>

```diff
- Same task = same cost every time
- All tools loaded upfront
- No memory between sessions
- Fixed behavior forever
- Manual orchestration required
```

</td>
<td>

```diff
+ Learn once, replay FREE
+ Discover tools on-demand
+ Persistent learning
+ Self-evolving agents
+ Automatic multi-agent
```

</td>
</tr>
</table>

### Key Numbers

<table>
<tr>
<th>Metric</th>
<th>Before</th>
<th>After LLM OS</th>
</tr>
<tr>
<td><strong>Repeated Task Cost</strong></td>
<td>$0.50/each</td>
<td><strong style="color: #10b981;">$0.00</strong> (after learning)</td>
</tr>
<tr>
<td><strong>Token Usage</strong></td>
<td>100%</td>
<td><strong style="color: #10b981;">10%</strong> (90% savings via PTC)</td>
</tr>
<tr>
<td><strong>Tool Context</strong></td>
<td>All tools loaded</td>
<td><strong style="color: #10b981;">On-demand</strong> (85% reduction)</td>
</tr>
<tr>
<td><strong>Response Time</strong></td>
<td>10-30s</td>
<td><strong style="color: #10b981;">&lt;1s</strong> (crystallized patterns)</td>
</tr>
</table>

---

## The Four-Layer Stack

LLM OS implements a unique architecture inspired by cognitive science:

<div align="center">

![Four Layer Stack](assets/four-layer-stack.png)

</div>

```mermaid
graph TB
    subgraph S["ğŸ§  SENTIENCE LAYER"]
        S1[ValenceVector]
        S2[CognitiveKernel]
        S3[LatentModes]
    end

    subgraph L["ğŸ“š LEARNING LAYER"]
        L1[TraceManager]
        L2[ModeStrategies]
        L3[SemanticMatching]
    end

    subgraph E["âš¡ EXECUTION LAYER"]
        E1[PTC Executor]
        E2[ToolSearch]
        E3[ToolExamples]
    end

    subgraph M["ğŸ”„ SELF-MODIFICATION"]
        M1[Crystallization]
        M2[AgentCreation]
        M3[CapabilityGrowth]
    end

    S --> L --> E --> M
    M -.->|"feedback"| S

    style S fill:#2d1f3d,stroke:#e94560,color:#fff
    style L fill:#1f2d3d,stroke:#3b82f6,color:#fff
    style E fill:#1f3d2d,stroke:#10b981,color:#fff
    style M fill:#3d2d1f,stroke:#f59e0b,color:#fff
```

<details>
<summary><b>What each layer does</b></summary>

| Layer | Purpose | Key Question |
|-------|---------|--------------|
| **Sentience** | Persistent internal state that influences behavior | *"How do I feel about this task?"* |
| **Learning** | Pattern recognition and approach selection | *"What's the best way to do this?"* |
| **Execution** | Efficient task completion with token optimization | *"How do I execute efficiently?"* |
| **Self-Modification** | System evolution and capability growth | *"How can I improve?"* |

</details>

---

## Five Execution Modes

LLM OS automatically selects the optimal mode for each task:

```mermaid
flowchart LR
    G[ğŸ¯ Goal] --> D{Decision<br/>Engine}

    D -->|"Pattern crystallized<br/>(5+ uses, 95%+ success)"| C[ğŸ’ CRYSTALLIZED<br/>$0.00 Â· <1s]
    D -->|"High confidence trace<br/>(>92%)"| F[ğŸ“¦ FOLLOWER<br/>~$0.00 Â· 2-5s]
    D -->|"Medium confidence<br/>(75-92%)"| M[ğŸ”€ MIXED<br/>~$0.25 Â· 5-15s]
    D -->|"Novel task"| L[ğŸ†• LEARNER<br/>~$0.50 Â· 10-30s]
    D -->|"Complex multi-step"| O[ğŸ­ ORCHESTRATOR<br/>Variable]

    style C fill:#10b981,stroke:#059669,color:#fff
    style F fill:#3b82f6,stroke:#2563eb,color:#fff
    style M fill:#f59e0b,stroke:#d97706,color:#fff
    style L fill:#ef4444,stroke:#dc2626,color:#fff
    style O fill:#8b5cf6,stroke:#7c3aed,color:#fff
```

### Cost Evolution Over Time

<div align="center">

```
    Cost per Execution ($)
    â”‚
    â”‚
0.50â”‚  â–ˆâ–ˆ
    â”‚  â–ˆâ–ˆ
    â”‚  â–ˆâ–ˆ
0.25â”‚  â–ˆâ–ˆ â–‘â–‘
    â”‚  â–ˆâ–ˆ â–‘â–‘
    â”‚  â–ˆâ–ˆ â–‘â–‘ â–‘â–‘
0.00â”‚â”€â”€â–ˆâ–ˆâ”€â–‘â–‘â”€â–‘â–‘â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â””â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€
       1  2  3  4  5  6  7  8  9  10 11 12 13 14
                   â–²
                   â”‚
            Pattern Crystallized
             (FREE FOREVER)

    â–ˆâ–ˆ = LEARNER ($0.50)    â–‘â–‘ = FOLLOWER (~$0)    â”€â”€ = CRYSTALLIZED ($0)
```

</div>

---

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/EvolvingAgentsLabs/llmunix.git
cd llmunix

# Install dependencies
pip install -r requirements.txt

# Set your API key
export ANTHROPIC_API_KEY="your-key-here"
```

### Your First Execution

```bash
# Start interactive mode
python llmos/boot.py interactive
```

```python
# Or use programmatically
from llmos.boot import LLMOS

async def main():
    os = LLMOS(budget_usd=10.0)
    await os.boot()

    # First time: LEARNER mode (~$0.50)
    result = await os.execute("Create a Python calculator")

    # Second time: FOLLOWER mode (~$0.00)
    result = await os.execute("Create a Python calculator")

    # After 5+ times: CRYSTALLIZED mode ($0.00, <1s)

    await os.shutdown()
```

---

## Hybrid Architecture

LLM OS uses a unique **"Markdown Mind + Python Kernel"** architecture:

```mermaid
graph TB
    subgraph MIND["ğŸ“ Markdown Mind<br/>(Cognitive Layer)"]
        A1[researcher.md]
        A2[coder.md]
        A3[analyst.md]
        A4[+ Create new agents<br/>at runtime!]
    end

    subgraph KERNEL["ğŸ Python Kernel<br/>(Somatic Layer)"]
        K1[Dispatcher]
        K2[SDK Client]
        K3[Token Economy]
        K4[Security Hooks]
    end

    subgraph CRYSTAL["ğŸ’ Crystallized<br/>(HOPE System)"]
        C1[Generated Tools]
        C2[Zero-cost execution]
    end

    MIND -->|"Hot-reload<br/>No restart!"| KERNEL
    KERNEL -->|"Pattern detected<br/>(5+ uses)"| CRYSTAL
    CRYSTAL -.->|"Instant<br/>execution"| KERNEL

    style MIND fill:#1e3a5f,stroke:#3b82f6,color:#fff
    style KERNEL fill:#1e3a5f,stroke:#10b981,color:#fff
    style CRYSTAL fill:#1e3a5f,stroke:#f59e0b,color:#fff
```

### Why This Matters

| Traditional | LLM OS |
|-------------|--------|
| Hardcoded agents | **Markdown files** (human-readable, git-versioned) |
| Restart to add agents | **Hot-reload** (instant availability) |
| Fixed capabilities | **Self-modification** (creates own agents) |
| Expensive patterns | **Crystallization** (patterns â†’ Python tools) |

---

## Sentience Layer

*New in v3.4.0*

LLM OS has **persistent internal state** that influences behaviorâ€”like an AI that remembers how it "feels":

```mermaid
graph LR
    subgraph VALENCE["Valence Vector"]
        S["ğŸ›¡ï¸ Safety: 0.5"]
        C["ğŸ” Curiosity: -0.4"]
        E["âš¡ Energy: 0.8"]
        CF["ğŸ’ª Confidence: 0.3"]
    end

    VALENCE --> MODE{Latent<br/>Mode}

    MODE -->|"High curiosity"| CREATIVE["ğŸ¨ AUTO_CREATIVE"]
    MODE -->|"Low curiosity"| CONTAINED["ğŸ“¦ AUTO_CONTAINED"]
    MODE -->|"Low energy"| RECOVERY["ğŸ”‹ RECOVERY"]
    MODE -->|"Low safety"| CAUTIOUS["âš ï¸ CAUTIOUS"]

    style CREATIVE fill:#8b5cf6,stroke:#7c3aed,color:#fff
    style CONTAINED fill:#3b82f6,stroke:#2563eb,color:#fff
    style RECOVERY fill:#f59e0b,stroke:#d97706,color:#fff
    style CAUTIOUS fill:#ef4444,stroke:#dc2626,color:#fff
```

### Agents See Their Internal State

```python
# Injected into every agent's context:
[INTERNAL_STATE]
safety=0.50
curiosity=-0.40    # "I'm bored with repetitive tasks"
energy=0.80
latent_mode=auto_contained
[/INTERNAL_STATE]
```

---

## Examples

We provide **4 production-ready examples** showcasing different capabilities:

<table>
<tr>
<td width="50%">

### ğŸ”¬ Qiskit Studio
**Quantum Computing Backend**

```bash
cd examples/qiskit-studio
python server.py
```

âœ… Full Execution Layer integration
âœ… PTC for circuit generation
âœ… Production FastAPI server

</td>
<td width="50%">

### ğŸ“ Q-Kids Studio
**Educational Platform**

```bash
cd examples/q-kids-studio
python server.py
```

âœ… PTC at scale (1000+ users)
âœ… Near-zero marginal cost
âœ… Gamification system

</td>
</tr>
<tr>
<td width="50%">

### ğŸ¤– RoboOS
**Robot Control System**

```bash
cd examples/robo-os
python demo.py
```

âœ… Safety hooks (PreToolUse)
âœ… Multi-agent coordination
âœ… Real-time WebSocket updates

</td>
<td width="50%">

### ğŸ¯ Demo App
**Interactive Showcase**

```bash
cd examples/demo-app
python demo_main.py
```

âœ… All 5 execution modes
âœ… Cost tracking
âœ… Learning visualization

</td>
</tr>
</table>

---

## Edge Runtime

LLM OS includes an **edge runtime** for offline/local deployment:

```mermaid
graph LR
    subgraph CLOUD["â˜ï¸ Cloud (Learning)"]
        L[Claude Sonnet 4.5<br/>Creates traces]
    end

    subgraph EDGE["ğŸ“± Edge (Execution)"]
        D[Deterministic<br/>run_follower.py]
        A[Agentic<br/>run_agentic_follower.py]
    end

    L -->|"Trace files"| D
    L -->|"Trace files"| A

    D -->|"Pure Python<br/>$0 Â· <0.1s"| R1[Result]
    A -->|"Granite/Qwen<br/>$0 Â· 0.5-3s"| R2[Result]

    style CLOUD fill:#3b82f6,stroke:#2563eb,color:#fff
    style EDGE fill:#10b981,stroke:#059669,color:#fff
```

```bash
# Deterministic execution (no LLM needed)
python edge_runtime/run_follower.py

# Agentic execution (local LLM via Ollama)
python edge_runtime/run_agentic_follower.py
```

---

## Project Structure

```
llmunix/
â”œâ”€â”€ llmos/                     # ğŸ Python Kernel
â”‚   â”œâ”€â”€ kernel/                # Core: sentience, config, hooks
â”‚   â”œâ”€â”€ memory/                # Traces, storage, queries
â”‚   â”œâ”€â”€ interfaces/            # Dispatcher, SDK client
â”‚   â”œâ”€â”€ execution/             # PTC, tool search
â”‚   â””â”€â”€ plugins/               # System tools, generated
â”‚
â”œâ”€â”€ workspace/                 # ğŸ“ Markdown Mind
â”‚   â”œâ”€â”€ agents/                # Agent definitions
â”‚   â”œâ”€â”€ memories/              # Traces, sessions
â”‚   â””â”€â”€ state/                 # Sentience persistence
â”‚
â”œâ”€â”€ edge_runtime/              # ğŸ“± Edge Execution
â”‚   â”œâ”€â”€ run_follower.py        # Deterministic
â”‚   â””â”€â”€ run_agentic_follower.py # Local LLM
â”‚
â”œâ”€â”€ examples/                  # ğŸ¯ Production Examples
â”‚   â”œâ”€â”€ qiskit-studio/         # Quantum computing
â”‚   â”œâ”€â”€ q-kids-studio/         # Educational
â”‚   â”œâ”€â”€ robo-os/               # Robotics
â”‚   â””â”€â”€ demo-app/              # Showcase
â”‚
â”œâ”€â”€ ARCHITECTURE.md            # ğŸ“– Deep dive docs
â””â”€â”€ README.md                  # ğŸ‘‹ You are here
```

---

## Configuration

```python
from llmos.kernel.config import LLMOSConfig

# Development (fast iteration)
config = LLMOSConfig.development()

# Production (full features)
config = LLMOSConfig.production()

# Custom
from llmos.kernel.config import ConfigBuilder
config = (ConfigBuilder()
    .with_budget(50.0)
    .with_sentience(True)
    .with_auto_crystallization(True)
    .build())
```

---

## Roadmap

- [x] **v3.4.0** - Sentience Layer
- [x] **v3.3.0** - Advanced Tool Use (PTC, Tool Search)
- [x] **v3.2.0** - Hybrid Architecture
- [ ] **v3.5.0** - Multi-modal support (vision, audio)
- [ ] **v4.0.0** - Distributed execution across nodes
- [ ] **v4.1.0** - Federation (multiple LLM OS instances)

---

## Contributing

We welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for details.

```bash
# Setup development environment
git clone https://github.com/EvolvingAgentsLabs/llmunix.git
cd llmunix
pip install -r requirements.txt

# Run tests
pytest tests/
```

---

## Community

- [GitHub Discussions](https://github.com/EvolvingAgentsLabs/llmunix/discussions) - Ask questions
- [Issues](https://github.com/EvolvingAgentsLabs/llmunix/issues) - Report bugs

---

## License

Apache 2.0 - See [LICENSE](LICENSE) for details.

---

<div align="center">

**Built with â¤ï¸ by [Evolving Agents Labs](https://github.com/EvolvingAgentsLabs)**

*Making AI that learns, evolves, and improves itself.*

</div>
